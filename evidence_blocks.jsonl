{"evidence_id": "phase1_fence_regex_removal", "block_id": "phase1_fence_regex_removal", "phase": 1, "date": "2025-10-12", "file": "src/docpipe/markdown_parser_core.py", "line_before": 3583, "pattern_removed": "text = re.sub(r\"```[^`]*```\", \"\", text, flags=re.DOTALL)", "replacement_strategy": "Token-based fence detection using walk_tokens_iter() and token.type == 'fence'", "code_snippet": "# Remove code blocks markers\ntext = re.sub(r\"```[^`]*```\", \"\", text, flags=re.DOTALL)\n---\n# Remove code blocks markers (Phase 1: token-based fence removal)\nif HAS_TOKEN_UTILS and walk_tokens_iter is not None:\n    # Token-based approach: parse and remove fence blocks\n    try:\n        temp_tokens = self.md.parse(text)\n        lines = text.split('\\n')\n        lines_to_remove = set()\n\n        for token in walk_tokens_iter(temp_tokens):\n            if token.type == \"fence\" and token.map:\n                start_line, end_line = token.map\n                # Mark lines for removal (fence markers + content)\n                for line_idx in range(start_line, end_line):\n                    lines_to_remove.add(line_idx)\n\n        # Remove marked lines\n        if lines_to_remove:\n            lines = [line for idx, line in enumerate(lines) if idx not in lines_to_remove]\n            text = '\\n'.join(lines)\n    except Exception:\n        # Fallback to regex if token parsing fails\n        text = re.sub(r\"```[^`]*```\", \"\", text, flags=re.DOTALL)\nelse:\n    # Fallback to regex if token utilities not available\n    text = re.sub(r\"```[^`]*```\", \"\", text, flags=re.DOTALL)", "test_result": "542/542 baseline tests passing", "performance_impact": "Δmedian=-15.69%, Δp95=-15.78% (improved)", "notes": "This was the only regex pattern in Phase 1 (Fences & Indented Code). Core fence/code detection was already token-based. Includes graceful fallback to regex if token utilities are unavailable.", "sha256": "58d0908b8d2300d0ed0bc4310731349f0ad5b93f3ed0ccfd5bec754d5e120f54"}
{"evidence_id": "phase2-plaintext-token-impl", "phase": 2, "file": "src/docpipe/markdown_parser_core.py", "lines": "3577-3609", "description": "Token-based plaintext extraction - ZERO regex (removed fallback)", "code_snippet": "    def _strip_markdown(self, text: str) -> str:\n        \"\"\"Remove markdown formatting from text using token-based extraction.\n\n        Phase 2: Pure token-based plaintext extraction (zero regex).\n        Extracts only text content, excluding all markdown formatting:\n        - Headers, emphasis, bold, links, code blocks, inline code\n\n        Policy: Excludes code_inline content (technical content, not prose).\n        \"\"\"\n        # Token-based plaintext extraction (Phase 2 - zero regex)\n        tokens = self.md.parse(text)\n        plaintext_parts = []\n\n        for token in walk_tokens_iter(tokens):\n            if token.type == \"text\":\n                # Pure text content - include it\n                plaintext_parts.append(token.content)\n            elif token.type in (\"softbreak\", \"hardbreak\"):\n                # Convert breaks to spaces\n                plaintext_parts.append(\" \")\n            # Skip all other token types:\n            # - \"heading_open\" (header markers)\n            # - \"strong_open\"/\"em_open\" (emphasis markers)\n            # - \"code_inline\" (technical content - excluded per policy)\n            # - \"link_open\"/\"link_close\" (link markup)\n            # - \"fence\"/\"code_block\" (code blocks)\n            # We only want the text content, not the formatting\n\n        # Join and normalize whitespace\n        plaintext = \"\".join(plaintext_parts)\n        # Collapse multiple spaces and strip\n        plaintext = \" \".join(plaintext.split())\n        return plaintext", "sha256": "612a57226ad409aa396ca1698ae2a800ce3b145c6da69614521c142db913bea6", "timestamp": "2025-10-12T07:21:00.573637Z"}
{"evidence_id": "phase3-sanitize-failclosed", "phase": 3, "date": "2025-10-12", "file": "src/docpipe/markdown_parser_core.py", "lines": "964-1035", "description": "Deprecated sanitize() text mutation; fail-closed validation via _apply_security_policy()", "patterns_removed": ["(?<!\\!)\\[([^\\]]+)\\]\\(([^)]+)\\)", "!\\[([^\\]]*)\\]\\(([^)]+)\\)"], "patterns_moved_to_phase6": ["^[a-z]:[/\\\\]"], "replacement_strategy": "Fail-closed validation: deprecate text mutation, reuse _apply_security_policy() for enforcement, return embedding_blocked flag instead of mutated text", "code_snippet": "def sanitize(self, policy=None, security_profile=None) -> dict[str, Any]:\n    \"\"\"DEPRECATED (Phase 3): Non-mutating wrapper. Use parse() results instead.\"\"\"\n    warnings.warn(\"sanitize() is deprecated: it no longer mutates text. Use parse() and inspect result['metadata'] for policy decisions.\", DeprecationWarning, stacklevel=2)\n    parsed = self.parse()\n    md = parsed.get(\"metadata\", {})\n    blocked = bool(md.get(\"embedding_blocked\") or md.get(\"quarantined\"))\n    reasons = []\n    if md.get(\"embedding_block_reason\"): reasons.append(md[\"embedding_block_reason\"])\n    reasons.extend(md.get(\"quarantine_reasons\", []) or [])\n    return {\"sanitized_text\": self.content, \"blocked\": blocked, \"reasons\": reasons}", "sha256": "b11abf41a7b84c118b7621f5d1657ecdb52f5718123a2b21a1aaf1c39789ce92", "test_result": "542/542 baseline tests passing, 10/10 Phase 3 tests passing", "performance_impact": "Δmedian=-1.9%, Δp95=-1.67% (improved - removed second parse)", "rationale": "Fail-closed approach eliminates text mutation, allows pure token-based detection. Aligns with EXECUTION_GUIDE §0 'fail closed' principle. Removes heavy second parse from sanitize(). _apply_security_policy() already handles link/image filtering via tokens.", "timestamp": "2025-10-12T14:30:00.000000Z"}
{"evidence_id": "phase4-html-pure-token", "phase": 4, "date": "2025-10-12", "file": "src/docpipe/markdown_parser_core.py", "lines": "1145-1184", "description": "Phase 4: Pure token-based HTML detection (ZERO regex)", "patterns_converted": ["<(?:div|span|script|style|iframe|object|embed|form)[\\s>]", "<(?:a|img|em|strong|b|i|u|code|kbd|sup|sub)[\\s>]", "<script[\\s>]", "\\bon(?:load|error|click|mouse|key|focus|blur|change|submit)\\s*="], "replacement_strategy": "Pure token-based approach: Always parse with html=True to get HTML tokens. Policy layer (allows_html config) controls whether HTML is stripped, but detection ALWAYS works via tokens. Zero regex, zero false positives from code blocks.", "code_snippet": "# HTML detection (Phase 4: pure token-based, zero regex)\nhtml_blocks = structure.get(\"html_blocks\", [])\nhtml_inline = structure.get(\"html_inline\", [])\n\nif html_blocks:\n    security[\"statistics\"][\"has_html_block\"] = True\nif html_inline:\n    security[\"statistics\"][\"has_html_inline\"] = True\n\n# Script detection (pure token-based)\nhtml_items = html_blocks + html_inline\nfor html_item in html_items:\n    content = html_item.get(\"content\", \"\").lower()\n    if \"<script\" in content:\n        security[\"statistics\"][\"has_script\"] = True\n        security[\"warnings\"].append({\"type\": \"script_tag\", \"line\": html_item.get(\"line\") or html_item.get(\"start_line\"), \"message\": \"Document contains <script> tags\"})\n        break\n\n# Event handler detection (pure token-based)\nevent_handlers = [\"onload\", \"onerror\", \"onclick\", \"onmouse\", \"onkey\", \"onfocus\", \"onblur\", \"onchange\", \"onsubmit\"]\nfor html_item in html_items:\n    content = html_item.get(\"content\", \"\").lower()\n    if any(handler in content for handler in event_handlers):\n        security[\"statistics\"][\"has_event_handlers\"] = True\n        security[\"warnings\"].append({\"type\": \"event_handlers\", \"line\": html_item.get(\"line\") or html_item.get(\"start_line\"), \"message\": \"Document contains HTML event handler attributes\"})\n        break", "sha256": "f92a58f23ff27284660434f108a92f4e5ce54a1f14594546f648126738f91080", "test_result": "542/542 baseline tests passing, 12/12 Phase 4 tests passing", "performance_impact": "Δmedian=-3.81%, Δp95=-3.29% (improved)", "rationale": "Parser separation: always parse with html=True (extract structure), policy layer enforces allows_html (strip if not allowed). Zero regex = zero false positives from code blocks. Aligns with project goal: pure token-based detection. Fail-closed security: HTML ALWAYS detected since parser ALWAYS generates tokens.", "tests_created": "test_phase4_html_detection.py (12 tests covering token detection, no false positives from code blocks/escaped HTML, script/event handler detection)", "timestamp": "2025-10-12T15:30:00.000000Z"}
{"evidence_id": "phase5-frontmatter-plugin", "phase": 5, "date": "2025-10-12", "file": "src/docpipe/markdown_parser_core.py", "lines": "1554-1579", "description": "Phase 5: Plugin-based frontmatter extraction (NO content mutation)", "patterns_removed": ["^---\\s*$", "^\\.\\.\\.$", "lines 1625-1640: blank line insertion loop"], "replacement_strategy": "Use mdit-py-plugins.front_matter to extract YAML frontmatter from tokens. Plugin creates 'front_matter' token with raw YAML content. No pre-processing, no content stripping, no blank line insertion. Extract frontmatter like all other features: from tokens in parse() method.", "code_snippet": "def _extract_frontmatter(self) -> dict | None:\n    \"\"\"Extract YAML frontmatter from tokens (Phase 5: plugin-based).\n    \n    The front_matter plugin creates a 'front_matter' token with YAML content.\n    We parse the YAML content from the token.\n    \n    Returns:\n        Frontmatter dict or None if not present\n    \"\"\"\n    # Find front_matter token (plugin creates this)\n    for token in self.tokens:\n        if hasattr(token, 'type') and token.type == 'front_matter':\n            # Token content is the raw YAML string\n            yaml_content = token.content\n            if yaml_content:\n                try:\n                    parsed_yaml = yaml.safe_load(yaml_content)\n                    # Return parsed YAML if it's a dict or list\n                    if isinstance(parsed_yaml, (dict, list)):\n                        return parsed_yaml\n                except yaml.YAMLError:\n                    # Invalid YAML - return None\n                    pass\n            break\n    return None", "sha256": "548be4e33892cda9a2cbc87cc6d37dad6ca46f73c77966fe5e67d5a39ccc0604", "test_result": "542/542 baseline tests passing, all CI gates passing", "performance_impact": "Δmedian=-0.95%, Δp95=-0.33% (improved)", "rationale": "Eliminates content mutation bug (blank line insertion lines 1625-1640). Aligns with architecture: 'extract structure, never mutate source'. Frontmatter extracted same way as all features: from tokens after parsing. No pre-processing in __init__(). Plugin handles YAML parsing and token generation. Content.raw and content.lines preserve source exactly.", "architectural_fix": "Removed pre-processing from __init__ (lines 471-474). Moved frontmatter to structure dict (extracted in parse() via _extract_frontmatter()). No more self.frontmatter - use structure['frontmatter'] like all other features.", "timestamp": "2025-10-12T21:00:00.000000Z"}
{"evidence_id": "phase5-tables-pure-token", "phase": 5, "date": "2025-10-12", "file": "src/docpipe/markdown_parser_core.py", "lines": "1958-2086", "description": "Phase 5: Pure token-based table extraction (ZERO regex) + raw content preservation + task list separation", "patterns_removed": ["r\"-{3,}\"", "r\"[|:\\-\\s]+\""], "methods_removed": ["_infer_table_alignment() (lines 2798-2845)", "_infer_table_alignment_line() (lines 2876-2903)"], "replacement_strategy": "Extract table alignment directly from th.attrs['style'] provided by markdown-it's built-in GFM tables plugin. No regex needed - alignment already parsed and stored in token attributes. Added raw_content field to preserve original markdown for analysis.", "features_added": ["raw_content: original markdown table text (unchanged)", "table_valid_md: boolean flag (combines !is_ragged && !align_mismatch)", "Separated task lists: structure['tasklists'] distinct from structure['lists']", "Recursion depth limits: max_depth=10 for list nesting safety"], "code_snippet": "# Extract raw table content (preserve original markdown)\nraw_content = \"\"\nif start_line is not None and end_line is not None:\n    raw_content = \"\\n\".join(self.lines[start_line:end_line])\n\ntable = {\n    \"id\": f\"table_{len(ctx)}\",\n    \"raw_content\": raw_content,  # Original markdown table (unchanged)\n    \"headers\": [],  # Parsed headers (polished)\n    \"rows\": [],     # Parsed rows (polished)\n    \"align\": None,  # Parsed alignment (polished)\n    # ...\n}\n\n# Alignment from th.attrs (markdown-it provides this)\nalign = \"left\"  # default\nif hasattr(th, 'attrs') and th.attrs:\n    style = th.attrs.get('style', '')\n    if 'text-align:center' in style:\n        align = \"center\"\n    elif 'text-align:right' in style:\n        align = \"right\"\n    elif 'text-align:left' in style:\n        align = \"left\"\naligns.append(align)\n\n# Validation flag\ntable[\"table_valid_md\"] = not is_ragged and not align_mismatch", "sha256": "9d5517cc9dc25604ef818940b38a223e35606ddea7387376c44b32b8372ca20d", "test_result": "542/542 baseline tests passing after regeneration", "performance_impact": "Δmedian=-0.95%, Δp95=-0.61% (improved)", "rationale": "markdown-it's built-in GFM tables plugin already parses separator lines (|:---|:--:|---:) and stores alignment in th.attrs. Regex was redundant - we were re-parsing what markdown-it already parsed. Pure token-based extraction eliminates regex entirely. raw_content preserves original markdown for analysis per user requirement: 'Exactly the same text that original md had' - prevents information loss from polishing.", "discovery": "User provided markdown-it source code showing alignment stored in token.attrs. Realized regex methods (_infer_table_alignment*) were redundant since markdown-it already provides this data. User requested raw content preservation to enable table analysis without losing original structure.", "lists_simplification": "Simplified _extract_list_items() checkbox detection by creating _detect_task_checkbox() helper (lines 1763-1783) using walk_tokens_iter() pattern. Removed complex nested loops, replaced with clean token-based detection of html_inline checkbox tokens from tasklists plugin.", "task_list_separation": "Created _extract_tasklists() method (lines 1768-1817) and _extract_tasklist_items() helper (lines 1836-1903). Task lists now in structure['tasklists'], separate from structure['lists']. Detection via 'contains-task-list' class added by plugin. Added recursion depth limits (max_depth=10) to prevent stack overflow.", "security_profile_fix": "Added tasklists plugin to ALLOWED_PLUGINS for all security profiles (strict, moderate, permissive) at lines 188-200. Plugin was previously only in permissive profile, causing checkbox detection to fail.", "timestamp": "2025-10-12T23:30:00.000000Z"}
{"evidence_id": "phase6-contentcontext-removal", "phase": 6, "date": "2025-10-12", "file": "src/docpipe/markdown_parser_core.py", "lines": "79, 530-533, 2833-2892", "description": "Phase 6: Removed ContentContext entirely - pure token-based prose/code classification", "patterns_removed": ["ContentContext regex heuristics for fence detection (r\"^\\s*([`~])\\1{2,}(\\s*\\S.*)?\\s*$\")", "ContentContext indented code heuristics (4-space/tab detection)", "content_context feature flag from get_available_features()"], "replacement_strategy": "Pure token-based classification: _build_mappings() now derives prose/code distinction entirely from markdown-it AST code blocks extracted via _extract_code_blocks(). No regex, no heuristics. Classification follows token boundaries exactly (token.map[0] to token.map[1] exclusive).", "code_snippet": "# Phase 6: ContentContext removed - use pure token-based classification\n# Prose/code distinction now derived entirely from AST code blocks\nself._context = None\nself.context = None\n---\ndef _build_mappings(self) -> dict[str, Any]:\n    \"\"\"Build line-to-content mappings.\n\n    Phase 6: Pure token-based classification using AST code blocks.\n    No ContentContext - classification derived entirely from markdown-it tokens.\n    \"\"\"\n    mappings = {\n        \"line_to_type\": {},\n        \"line_to_section\": {},\n        \"prose_lines\": [],\n        \"code_lines\": [],\n        \"code_blocks\": [],  # Expose code blocks with language\n    }\n\n    # Initialize all lines as prose (default assumption)\n    for i in range(len(self.lines)):\n        mappings[\"prose_lines\"].append(i)\n        mappings[\"line_to_type\"][str(i)] = \"prose\"\n\n    # Build section mappings directly to avoid circular dependency\n    sections = self._sections or self._get_cached(\"sections\", self._extract_sections)\n    for section in sections:\n        if section[\"start_line\"] is not None and section[\"end_line\"] is not None:\n            for line_num in range(section[\"start_line\"], section[\"end_line\"] + 1):\n                if 0 <= line_num < len(self.lines):  # Bounds check\n                    mappings[\"line_to_section\"][str(line_num)] = section[\"id\"]\n\n    # Cache mappings for O(1) lookups in _find_section_id\n    self._mappings_cache = mappings\n\n    # Pure token-based code block classification (Phase 6)\n    # Extract code blocks from AST and mark those lines as code\n    try:\n        code_blocks = self._get_cached(\"code_blocks\", self._extract_code_blocks)\n        for b in code_blocks:\n            s, e = b.get(\"start_line\"), b.get(\"end_line\")\n            if s is None or e is None:\n                continue\n\n            # Add to code_blocks list for mappings\n            # Note: structure uses exclusive end_line, but mappings uses inclusive for backward compat\n            mappings[\"code_blocks\"].append({\n                \"start_line\": s,\n                \"end_line\": e - 1,  # Convert exclusive to inclusive\n                \"language\": b.get(\"language\"),\n            })\n\n            # Mark these lines as code (end_line from structure is exclusive)\n            for ln in range(s, e):\n                mappings[\"line_to_type\"][str(ln)] = \"code\"\n                # Remove from prose_lines if present\n                if ln in mappings[\"prose_lines\"]:\n                    mappings[\"prose_lines\"].remove(ln)\n                # Add to code_lines if not present\n                if ln not in mappings[\"code_lines\"]:\n                    mappings[\"code_lines\"].append(ln)\n    except Exception:\n        pass\n\n    return mappings", "sha256": "ac3a0bf3a206b657f9fbf138880f842b4235b3e0b22d1316bd5798b62423772b", "test_result": "542/542 baseline tests passing (baselines regenerated with corrected mappings)", "performance_impact": "Δmedian=-2.86%, Δp95=-2.06% (improved - faster without ContentContext overhead)", "bug_fixed": "ContentContext incorrectly classified blank lines outside code blocks as 'code'. Example: blank line after fence marker was marked as code. Token-based classification correctly follows markdown-it token boundaries (token.map = [start, end_exclusive]), so blank lines outside fence tokens are correctly classified as prose.", "rationale": "ContentContext used regex heuristics that were: (1) less accurate than tokens, (2) required correction by AST anyway (old _build_mappings lines 2885-2933 explicitly corrected ContentContext mistakes), (3) added performance overhead (double-pass parsing). Pure token-based approach is simpler, faster, and more accurate. Eliminates 214-line content_context.py module entirely.", "architectural_improvement": "Removed double-pass parsing: ContentContext parse → AST parse → AST correction of ContentContext errors. Now single-pass: AST parse only. Prose/code classification derived directly from code block tokens, no intermediate heuristic layer. Aligns with zero-regex architecture goal.", "documentation_updated": "CLAUDE.md updated to reflect ContentContext removal, module structure updated, usage examples removed.", "timestamp": "2025-10-12T23:00:00.000000Z"}
