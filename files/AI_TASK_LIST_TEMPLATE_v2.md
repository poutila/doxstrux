# AI_TASK_LIST_TEMPLATE.md

**Version**: 2.2
**Objective**: Produce an AI task list that (1) does not drift and (2) stays close to reality.

---

## ğŸ¯ DERIVED GOVERNANCE (INPUTS)

This template enforces rules derived from these governance documents:

| Document | What It Contributes |
|----------|---------------------|
| `CLEAN_TABLE_PRINCIPLE.md` | No placeholders, no TODOs, no deferrals, no silent errors |
| `NO_WEAK_TESTS.md` | Semantic assertions, fixture-driven, fail-before/pass-after |
| `UV_AS_PACKAGE_MANAGER.md` | uv-only execution, no `.venv/bin/python` |
| `CHAIN_OF_THOUGHT_GOLDEN.md` | Evidence anchors, decision chains, stop rules |
| `CODE_QUALITY.md` | KISS/YAGNI/SOLID gates, real consumers required |

**If you use different governance docs, update this table.**

---

## âš¡ THE FOUR DRIFT KILLERS

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. SINGLE PLACEHOLDER SYNTAX: [[PH:NAME]]                  â”‚
â”‚     Makes "no placeholders" enforceable and non-noisy       â”‚
â”‚                                                             â”‚
â”‚  2. PHASE 0 BASELINE REALITY                                â”‚
â”‚     Forces capture of actual repo state before planning     â”‚
â”‚                                                             â”‚
â”‚  3. CANONICAL FILE LISTS PER TASK                           â”‚
â”‚     Prevents "Files:" headers diverging from verification   â”‚
â”‚                                                             â”‚
â”‚  4. UV-ONLY EXECUTION                                       â”‚
â”‚     Eliminates "works locally but not in CI" command drift  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš ï¸ NON-NEGOTIABLE INVARIANTS

1. **Clean Table is a delivery gate** â€” Do not ship or mark "complete" unless fully correct, verified, stable. No deferrals.
2. **No silent errors** â€” Errors raise unconditionally. No "strict mode" flags.
3. **No weak tests** â€” Tests must assert meaningful behavior (semantics), not existence/import/smoke/line-count.
4. **uv is the only runner** â€” All commands are `uv run ...`. Any `.venv/bin/python` in committed code is drift.
5. **Evidence anchors required** â€” If you claim "X exists/passes/is wired", attach command output or file evidence.
6. **No synthetic reality** â€” Do not invent outputs, file lists, test results, or tool versions.

---

## âš ï¸ PLACEHOLDER FORMAT (MACHINE-DETECTABLE)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  THIS TEMPLATE USES ONLY ONE PLACEHOLDER FORMAT:            â”‚
â”‚                                                             â”‚
â”‚  [[PH:PLACEHOLDER_NAME]]                                    â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN FORMATS (will cause false positives in linting): â”‚
â”‚  âŒ [PLACEHOLDER]      - collides with markdown links       â”‚
â”‚  âŒ <PLACEHOLDER>      - collides with HTML/XML             â”‚
â”‚  âŒ YYYY-MM-DD         - ambiguous date placeholder         â”‚
â”‚  âŒ TBD, TODO, FIXME   - ambiguous markers                  â”‚
â”‚                                                             â”‚
â”‚  PRE-FLIGHT CHECK (must return zero matches):               â”‚
â”‚  grep -RInE '\[\[PH:[A-Z0-9_]+\]\]' PROJECT_TASKS.md && \   â”‚
â”‚    { echo "Placeholders remain"; exit 1; } || true          â”‚
â”‚                                                             â”‚
â”‚  NO EXCEPTIONS in executable project task lists.            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## How This Template Works

This template has **sixteen** enforcement mechanisms:

1. **STOP Checkpoints** â€” AI must pause and verify before continuing
2. **Phase Gates** â€” Cannot advance until all criteria pass
3. **Clean Table Rule** â€” No debt carries forward
4. **Test Strength Rules** â€” "It runs" tests are explicitly forbidden
5. **File Manifest Verification** â€” Every listed file must exist
6. **Source of Truth Hierarchy** â€” Code wins over docs
7. **Strictness Gates** â€” Loose must pass before strict
8. **Unimplemented Feature Protocol** â€” Don't test what doesn't exist
9. **Discovery-First Protocol** â€” Discover structure before defining models
10. **Code Quality Gates (SOLID/KISS/YAGNI)** â€” No speculative code
11. **No Silent Errors** â€” All exceptions raise unconditionally
12. **Reflection Loop** â€” Mandatory self-assessment before proceeding
13. **Task ID Uniqueness** â€” No duplicate IDs, all tracked in Appendix
14. **Global Clean Table Enforcement** â€” Check entire repo, not selective files
15. **Aspirational vs Actual Alignment** â€” Defined patterns must be used in examples
16. **Scope Decisions Table** â€” Explicit in/out of scope declarations

**The AI assistant should copy this template, fill in project-specific details, and follow it sequentially.**

### ğŸ“¦ MINIMALISM NOTE

This template is comprehensive. For simpler projects, you can strip down to just:

1. Non-negotiable invariants (6 rules)
2. The four drift killers
3. Baseline snapshot + task structure + drift ledger

The detailed sections below provide depth for complex projects. **Smaller templates drift less** â€” remove what you don't need, but keep the four drift killers.

---

# [[PH:PROJECT_NAME]] - Detailed Task List

**Project**: [[PH:ONE_LINE_DESCRIPTION]]
**Created**: [[PH:DATE_YYYY_MM_DD]]
**Status**: Phase 0 - NOT STARTED

---

## ğŸ¯ MANDATORY BASELINE SNAPSHOT (POPULATE FIRST)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BEFORE ANY WORK: CAPTURE BASELINE REALITY                  â”‚
â”‚                                                             â”‚
â”‚  This section MUST be populated with EXECUTED command       â”‚
â”‚  output before proceeding. Do not fill from memory.         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Environment

| Field | Value |
|-------|-------|
| **Repo** | [[PH:REPO_NAME]] |
| **Branch** | [[PH:GIT_BRANCH]] |
| **Commit** | [[PH:GIT_COMMIT_HASH]] |
| **OS** | [[PH:OS_VERSION]] |
| **Runtime** | [[PH:RUNTIME_VERSION]] |
| **Package Manager** | [[PH:PACKAGE_MANAGER_VERSION]] |

### Evidence (paste exact output)

```bash
$ git rev-parse --abbrev-ref HEAD
[[PH:PASTE_BRANCH_OUTPUT]]

$ git rev-parse HEAD
[[PH:PASTE_COMMIT_OUTPUT]]

$ uv --version
[[PH:PASTE_UV_VERSION]]

$ uv run python --version
[[PH:PASTE_PYTHON_VERSION]]
```

### Baseline Test Status

```bash
$ uv run pytest -q
[[PH:PASTE_TEST_OUTPUT]]
```

**If tests fail**: Log in Drift Ledger (Section 9), do not proceed until resolved or explicitly scoped out.

---

## Quick Status Dashboard

| Phase | Name | Status | Tests | Clean Table |
|-------|------|--------|-------|-------------|
| 0 | Setup & Infrastructure | â³ NOT STARTED | -/- | - |
| 1 | [[PH:PHASE_1_NAME]] | ğŸ“‹ PLANNED | -/- | - |
| 2 | [[PH:PHASE_2_NAME]] | ğŸ“‹ PLANNED | -/- | - |

**Status Key**: âœ… COMPLETE | â³ IN PROGRESS | ğŸ“‹ PLANNED | âŒ BLOCKED

---

## Success Criteria (Project-Level)

The project is DONE when ALL of these are true:

- [ ] [Primary objective achieved]
- [ ] [All tests pass: `[command]`]
- [ ] [Performance requirement met]
- [ ] [No regressions introduced]

---

## â›” PHASE GATE RULES

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE N+1 CANNOT START UNTIL:                              â”‚
â”‚                                                             â”‚
â”‚  1. All Phase N tasks have âœ… status                        â”‚
â”‚  2. Phase N tests pass: [test_command] â†’ PASS               â”‚
â”‚  3. Phase N Clean Table verified                            â”‚
â”‚  4. Phase unlock artifact exists: .phase-N.complete.json    â”‚
â”‚  5. File manifest verified: all listed files exist          â”‚
â”‚  6. Strictness gates passed (if applicable)                 â”‚
â”‚                                                             â”‚
â”‚  If ANY criterion fails â†’ STOP. Fix or rollback.            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“œ SOURCE OF TRUTH HIERARCHY

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  When spec/docs and code disagree, resolve in this order:   â”‚
â”‚                                                             â”‚
â”‚  1. RUNNING CODE OUTPUT (highest authority)                 â”‚
â”‚     â””â”€â”€ What the system actually produces                   â”‚
â”‚                                                             â”‚
â”‚  2. GENERATED SAMPLES / SNAPSHOTS                           â”‚
â”‚     â””â”€â”€ Captured from real runs, not hand-written           â”‚
â”‚                                                             â”‚
â”‚  3. IMPLEMENTATION CODE                                     â”‚
â”‚     â””â”€â”€ The source files that produce (1)                   â”‚
â”‚                                                             â”‚
â”‚  4. THIS TASK LIST                                          â”‚
â”‚     â””â”€â”€ Execution plan, derived from above                  â”‚
â”‚                                                             â”‚
â”‚  5. DESIGN SPECS / ARCHITECTURE DOCS (lowest authority)     â”‚
â”‚     â””â”€â”€ Aspirational; historical once code exists           â”‚
â”‚                                                             â”‚
â”‚  CONFLICT RULE:                                             â”‚
â”‚  If higher-authority source differs from lower â†’            â”‚
â”‚  UPDATE THE LOWER SOURCE, not the higher.                   â”‚
â”‚                                                             â”‚
â”‚  NEVER change working code to match a design doc.           â”‚
â”‚  Instead, update the design doc to match reality.           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Drift Repair Entry Format

When a mismatch between sources is discovered, record the repair:

```
## Drift Repair Log

| Date | Higher Source | Lower Source | Mismatch | Resolution |
|------|---------------|--------------|----------|------------|
| YYYY-MM-DD | parser.py:45 | SPEC.md:3.2 | Parser returns List, spec says Dict | Updated SPEC.md to match code |
| YYYY-MM-DD | output.json | SCHEMA.md | Field "foo" present in output, missing from schema | Added "foo" to SCHEMA.md |
```

**Minimal entry format** (if table is too heavy):
```
DRIFT REPAIR: [date]
- FOUND: [higher source] differs from [lower source]
- MISMATCH: [what differed]
- RESOLUTION: [what was changed]
```

**Why log repairs**: Without a log, you can't distinguish "we fixed this" from "we never noticed". Logs enable audit and pattern detection.

---

## ğŸ”’ STRICTNESS GATE PROTOCOL

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STRICTNESS GATES: Prevent premature tightening             â”‚
â”‚                                                             â”‚
â”‚  Many projects have "loose â†’ strict" transitions:           â”‚
â”‚  - Validation: permissive â†’ strict                          â”‚
â”‚  - Types: Any â†’ specific                                    â”‚
â”‚  - Checks: warnings â†’ errors                                â”‚
â”‚  - Constraints: optional â†’ required                         â”‚
â”‚                                                             â”‚
â”‚  RULE: Cannot tighten until loose version passes:           â”‚
â”‚                                                             â”‚
â”‚  1. Implement with loose/permissive settings                â”‚
â”‚  2. Run against N representative real-world inputs          â”‚
â”‚  3. ALL must pass without error                             â”‚
â”‚  4. Only then tighten constraints                           â”‚
â”‚  5. Re-run same inputs; failures indicate model bug         â”‚
â”‚                                                             â”‚
â”‚  If tightening causes failures â†’ FIX THE MODEL/CODE         â”‚
â”‚  Do NOT synthesize fake data to satisfy strict checks       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Strictness vs Error Handling (IMPORTANT DISTINCTION)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  THESE ARE ORTHOGONAL CONCEPTS - DO NOT CONFUSE THEM        â”‚
â”‚                                                             â”‚
â”‚  STRICTNESS GATE = Validation constraint progression        â”‚
â”‚  - extra="allow" â†’ extra="forbid"                           â”‚
â”‚  - Optional fields â†’ required fields                        â”‚
â”‚  - Loose regex â†’ tight regex                                â”‚
â”‚  - "Accept anything" â†’ "Accept only valid"                  â”‚
â”‚                                                             â”‚
â”‚  NO SILENT ERRORS = Error handling behavior                 â”‚
â”‚  - Exceptions always raise (unconditional)                  â”‚
â”‚  - No strict mode toggle for error handling                 â”‚
â”‚  - No swallowed exceptions at any strictness level          â”‚
â”‚                                                             â”‚
â”‚  KEY INSIGHT:                                               â”‚
â”‚  Errors raise at EVERY strictness level.                    â”‚
â”‚  Strictness controls WHAT triggers an error,                â”‚
â”‚  not WHETHER errors are raised.                             â”‚
â”‚                                                             â”‚
â”‚  EXAMPLE:                                                   â”‚
â”‚  - Loose: Unknown field â†’ allowed (no error)                â”‚
â”‚  - Strict: Unknown field â†’ ValidationError (raised!)        â”‚
â”‚  - Both levels: Malformed data â†’ always raises              â”‚
â”‚                                                             â”‚
â”‚  Add this clarifying sentence to your strictness docs:      â”‚
â”‚  "Strictness refers to validation constraints, not error-   â”‚
â”‚   handling behavior; errors always raise."                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Strictness Gate Template

```bash
# Before tightening [CONSTRAINT_NAME]:
# 1. Run loose version against real inputs
[LOOSE_VALIDATION_COMMAND]
# Expected: PASS on Nâ‰¥[MINIMUM] inputs

# 2. Only after (1) passes, tighten
[TIGHTEN_COMMAND]

# 3. Re-run same inputs
[STRICT_VALIDATION_COMMAND]
# Expected: PASS on same inputs

# If (3) fails but (1) passed â†’ Model/code is wrong, not inputs
```

---

## ğŸš« UNIMPLEMENTED FEATURE PROTOCOL

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DO NOT TEST FEATURES THAT DON'T EXIST YET                  â”‚
â”‚                                                             â”‚
â”‚  Design docs often describe future features.                â”‚
â”‚  Task lists may reference planned capabilities.             â”‚
â”‚                                                             â”‚
â”‚  BEFORE writing a test for ANY feature:                     â”‚
â”‚                                                             â”‚
â”‚  1. VERIFY it exists in running code                        â”‚
â”‚     â””â”€â”€ grep, import, or execute to confirm                 â”‚
â”‚                                                             â”‚
â”‚  2. VERIFY it produces the expected output                  â”‚
â”‚     â””â”€â”€ Run it, capture output, inspect                     â”‚
â”‚                                                             â”‚
â”‚  3. Only then write assertions about it                     â”‚
â”‚                                                             â”‚
â”‚  FOR RESERVED/PLANNED FEATURES:                             â”‚
â”‚  - Define with safe defaults (Optional, None, empty)        â”‚
â”‚  - DO NOT assert specific values                            â”‚
â”‚  - DO NOT synthesize fake data to satisfy tests             â”‚
â”‚  - Mark clearly as "reserved" or "not yet implemented"      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Feature Verification Checklist

Before testing `[FEATURE]`:

- [ ] Feature exists in code: `grep -rn "[FEATURE]" src/`
- [ ] Feature is callable/accessible (not just defined)
- [ ] Feature produces output matching test expectations
- [ ] Feature is NOT marked "reserved", "planned", "TODO"

```bash
# Verification command template
grep -rn "[FEATURE_NAME]" src/ && echo "âœ… Found" || echo "âŒ NOT FOUND - do not test"
```

---

## ğŸ” CODE-FIRST VERIFICATION

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VERIFY BEFORE ASSUMING                                     â”‚
â”‚                                                             â”‚
â”‚  Design docs may reference:                                 â”‚
â”‚  - Class names that don't exist                             â”‚
â”‚  - Method signatures that differ                            â”‚
â”‚  - Constants with wrong values                              â”‚
â”‚  - APIs that were renamed or removed                        â”‚
â”‚                                                             â”‚
â”‚  BEFORE writing code/tests that depend on a symbol:         â”‚
â”‚                                                             â”‚
â”‚  1. grep for the exact symbol name                          â”‚
â”‚  2. Verify signature/parameters match expectations          â”‚
â”‚  3. If mismatch: UPDATE TASK LIST, not code                 â”‚
â”‚                                                             â”‚
â”‚  This prevents:                                             â”‚
â”‚  - Tests that fail due to typos in design docs              â”‚
â”‚  - Renaming stable code to match outdated specs             â”‚
â”‚  - Importing modules that don't exist                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Symbol Verification Template

```bash
# Before using [SYMBOL] from design doc:

# 1. Verify class/function/type exists
grep -rn "class [CLASS_NAME]" src/           # Python
grep -rn "function [FUNC_NAME]" src/         # JavaScript
grep -rn "struct [STRUCT_NAME]" src/         # Rust/Go/C
grep -rn "interface [IFACE_NAME]" src/       # TypeScript/Java

# 2. Verify signature (if applicable)
grep -A5 "[FUNCTION_NAME]" src/

# 3. Verify constants/values
grep -rn "[CONSTANT_NAME]" src/

# If ANY verification fails:
# â†’ Update task list to match actual code
# â†’ DO NOT rename code to match task list
```

---

## ğŸ§ª TDD Protocol

Every task follows this sequence:

```
1. WRITE TEST FIRST (or identify existing test)
   â””â”€â”€ Test must fail initially (red)
   â””â”€â”€ Test must assert SPECIFIC BEHAVIOR (see Test Strength Rules)

2. IMPLEMENT minimum code to pass
   â””â”€â”€ Test must pass (green)

3. VERIFY no regressions
   â””â”€â”€ Run: [full_test_command]
   â””â”€â”€ Expected: N/N PASS

4. CLEAN TABLE CHECK
   â””â”€â”€ No TODOs, no placeholders, no warnings

5. FILE MANIFEST CHECK
   â””â”€â”€ Every file in "Files:" header exists
```

**Test Commands Reference**:
```bash
# Fast iteration (after each small change)
[FAST_TEST_COMMAND]

# Full validation (before commits)
[FULL_TEST_COMMAND]

# Performance check (before phase completion)
[PERF_TEST_COMMAND]
```

---

## â›” TEST STRENGTH RULES (MANDATORY)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FORBIDDEN TEST PATTERNS - DO NOT USE:                      â”‚
â”‚                                                             â”‚
â”‚  âŒ assert result.returncode == 0  (only proves it runs)    â”‚
â”‚  âŒ assert result.returncode in (0, 1)  (useless)           â”‚
â”‚  âŒ assert "something" in output   (too vague)              â”‚
â”‚  âŒ Tests with no assertions                                â”‚
â”‚  âŒ Tests that pass with empty/stub implementation          â”‚
â”‚                                                             â”‚
â”‚  âŒ EXIT CODE TOLERANCE:                                    â”‚
â”‚     assert result.returncode in (0, 1)  # Accepts BOTH!     â”‚
â”‚     # Tool could be unimplemented, just sys.exit(0)         â”‚
â”‚                                                             â”‚
â”‚  âŒ "PRINTS SOMETHING PLAUSIBLE":                           â”‚
â”‚     assert "exported" in result.stdout.lower()              â”‚
â”‚     assert "Discovered" in output                           â”‚
â”‚     # Stub that prints "exported" passes!                   â”‚
â”‚                                                             â”‚
â”‚  âŒ IMPORT-ONLY tests:                                      â”‚
â”‚     test_module(): import mymodule  # proves nothing        â”‚
â”‚                                                             â”‚
â”‚  âŒ EXISTENCE-ONLY tests:                                   â”‚
â”‚     assert file.exists()  # no content verification         â”‚
â”‚     assert length(output) > 0  # just "not empty"           â”‚
â”‚                                                             â”‚
â”‚  âŒ SMOKE tests (just runs without crash):                  â”‚
â”‚     result = run_tool()  # no assertion on behavior         â”‚
â”‚     assert result != null  # trivially true                 â”‚
â”‚                                                             â”‚
â”‚  âŒ LINE-COUNT tests:                                       â”‚
â”‚     assert length(lines) > 10  # arbitrary threshold        â”‚
â”‚                                                             â”‚
â”‚  âŒ CI/WORKFLOW tests that only check strings:              â”‚
â”‚     assert "validate_tool.py" in workflow_yaml              â”‚
â”‚     # Doesn't verify correct wiring, just presence          â”‚
â”‚                                                             â”‚
â”‚  REQUIRED TEST PATTERNS - MUST USE:                         â”‚
â”‚                                                             â”‚
â”‚  âœ… Assert specific output values or structures             â”‚
â”‚  âœ… Assert count/quantity of results (== expected, not >0)  â”‚
â”‚  âœ… Assert file contents, not just file existence           â”‚
â”‚  âœ… Assert behavior differences between valid/invalid input â”‚
â”‚  âœ… Tests that FAIL with stub implementation                â”‚
â”‚  âœ… Fixture-driven deterministic outcomes                   â”‚
â”‚  âœ… Each error code/rule has test that triggers it          â”‚
â”‚  âœ… Exit code 0 means SUCCESS, exit code 1 means FAILURE    â”‚
â”‚     (never accept both as "passing")                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Test Strength Checklist (Apply to EVERY Test)

Before accepting a test as valid:

- [ ] **Stub test**: Would this pass if implementation just returned `None` or `[]`? â†’ If YES, test is too weak
- [ ] **Behavior captured**: Does test verify the actual business logic, not just "code ran"?
- [ ] **Failure modes**: Does test distinguish success from failure cases?
- [ ] **Output verification**: Does test check actual output content, not just presence?
- [ ] **Deterministic**: Does test use fixtures for reproducible outcomes?
- [ ] **Rule coverage**: If code emits error codes/rules, does each have a triggering test?
- [ ] **Existence vs Correctness**: Does test check semantic correctness, not just "file exists"?

### Existence vs Correctness Tests

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EXISTENCE TESTS ARE NOT CORRECTNESS TESTS                  â”‚
â”‚                                                             â”‚
â”‚  âŒ EXISTENCE-ONLY (weak):                                  â”‚
â”‚     assert file("output.json").exists()                     â”‚
â”‚     assert length(content) > 0                              â”‚
â”‚     assert "exported" in stdout                             â”‚
â”‚                                                             â”‚
â”‚  These pass with a stub that writes "{}" or "exported\n"    â”‚
â”‚                                                             â”‚
â”‚  âœ… CORRECTNESS (strong):                                   â”‚
â”‚     data = parse(file("output.json"))                       â”‚
â”‚     assert data["$id"] == "expected-schema-url"             â”‚
â”‚     assert "metadata" in data["properties"]                 â”‚
â”‚     assert data["required"] contains ["content", "metadata"]â”‚
â”‚                                                             â”‚
â”‚  These fail unless the output is semantically correct       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Example - Schema Export Test:
```
# âŒ WEAK: Only checks existence
test_export_schema():
    run(["./export_schema"])
    assert file("schema.json").exists()
    assert "exported" in stdout

# âœ… STRONG: Checks semantic correctness
test_export_schema():
    run(["./export_schema"])
    schema = json.load(file("schema.json"))
    assert schema["$id"] == "https://example.com/parser-output.schema.json"
    assert "metadata" in schema["properties"]
    assert "content" in schema["properties"]
    assert schema["properties"]["metadata"]["type"] == "object"
```

---

## ğŸ­ NO-OP STUB TOOL DETECTION

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TOOLS CAN SATISFY TESTS WITHOUT DOING REAL WORK            â”‚
â”‚                                                             â”‚
â”‚  PROBLEM: Tests check tool outputs but not tool behavior    â”‚
â”‚                                                             â”‚
â”‚  Example - Validation Tool Tests:                           â”‚
â”‚  âœ“ --help exits 0 and mentions --report                     â”‚
â”‚  âœ“ --report writes JSON with {total, passed, pass_rate}     â”‚
â”‚  âœ“ --threshold 0 exits 0                                    â”‚
â”‚  âœ“ --threshold 101 exits 1                                  â”‚
â”‚                                                             â”‚
â”‚  NO-OP STUB THAT PASSES ALL TESTS:                          â”‚
â”‚  def main():                                                â”‚
â”‚      if "--help" in sys.argv:                               â”‚
â”‚          print("--report --threshold"); sys.exit(0)         â”‚
â”‚      report = {"total": 0, "passed": 0, "pass_rate": 0.0}   â”‚
â”‚      write_json(report)  # Never validated anything!        â”‚
â”‚      threshold = get_threshold()                            â”‚
â”‚      sys.exit(0 if threshold <= 0 else 1)                   â”‚
â”‚                                                             â”‚
â”‚  This stub:                                                 â”‚
â”‚  âŒ Never reads actual fixture files                        â”‚
â”‚  âŒ Never calls schema validation                           â”‚
â”‚  âŒ Never parses real data                                  â”‚
â”‚  âœ“ Passes all tests!                                        â”‚
â”‚                                                             â”‚
â”‚  REQUIRED TESTS TO PREVENT NO-OP STUBS:                     â”‚
â”‚                                                             â”‚
â”‚  âœ… Test with KNOWN-GOOD input: total > 0, passed > 0       â”‚
â”‚  âœ… Test with KNOWN-BAD input: failed > 0, specific error   â”‚
â”‚  âœ… Test that tool actually calls the validation function   â”‚
â”‚     (mock/spy if needed)                                    â”‚
â”‚  âœ… Test exit code matches actual results, not canned       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### No-Op Stub Prevention Tests

```
# For any tool that validates/processes data:

test_tool_processes_real_good_input():
    # Run against known-good fixtures directory
    result = run_tool(["--input", "fixtures/valid/"])
    report = parse_output(result)
    assert report["total"] > 0, "Must process at least 1 fixture"
    assert report["passed"] > 0, "Known-good fixtures must pass"

test_tool_detects_real_bad_input():
    # Run against known-bad fixture
    result = run_tool(["--input", "fixtures/invalid/malformed.json"])
    report = parse_output(result)
    assert report["failed"] > 0, "Must detect invalid fixture"
    assert "validation error" in report["errors"][0].lower()

test_tool_exit_code_reflects_results():
    # With threshold 100%, known-bad input must fail
    result = run_tool(["--input", "fixtures/invalid/", "--threshold", "100"])
    assert result.exitCode != 0, "Must fail when threshold not met"
```

### Fabricated Count Detection

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TESTS SHOULD VERIFY ACTUAL DATA PROCESSING, NOT JUST SHAPE â”‚
â”‚                                                             â”‚
â”‚  PROBLEM: Tool can fabricate counts without touching data:  â”‚
â”‚                                                             â”‚
â”‚  if "--test-dir" in argv:                                   â”‚
â”‚      total = 100  # Hardcoded, never read files!            â”‚
â”‚  report = {"total": total, "passed": total, "failed": 0}    â”‚
â”‚                                                             â”‚
â”‚  Tests checking "total >= 50" pass without real processing  â”‚
â”‚                                                             â”‚
â”‚  SOLUTIONS:                                                 â”‚
â”‚                                                             â”‚
â”‚  âœ… Test with EXACT count (not >=):                         â”‚
â”‚     # fixtures/exactly_3/ has exactly 3 files               â”‚
â”‚     result = run_tool(["--input", "fixtures/exactly_3/"])   â”‚
â”‚     assert report["total"] == 3  # == not >=                â”‚
â”‚                                                             â”‚
â”‚     # WHY: Can't hardcode if different dirs have different  â”‚
â”‚     # exact counts that tests verify                        â”‚
â”‚                                                             â”‚
â”‚  âœ… Test with DELIBERATE schema-invalid fixture:            â”‚
â”‚     # fixtures/invalid/schema_violation.json has field      â”‚
â”‚     # that violates schema (not just malformed JSON)        â”‚
â”‚     result = run_tool(["--input", "fixtures/invalid/"])     â”‚
â”‚     assert report["failed"] > 0, "Must detect violation"    â”‚
â”‚     assert report["pass_rate"] < 100, "Can't be 100%"       â”‚
â”‚                                                             â”‚
â”‚  âœ… Test that tool actually calls validation function:      â”‚
â”‚     # Mock/spy the validation call                          â”‚
â”‚     with mock.patch("module.validate") as m:                â”‚
â”‚         run_tool(["--input", "fixtures/"])                  â”‚
â”‚         assert m.called, "Must call validate()"             â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN:                                                 â”‚
â”‚  âŒ Tests that only check "total >= N" (fabricatable)       â”‚
â”‚  âŒ Tests that skip bad-input cases "because complex"       â”‚
â”‚  âŒ "Invalid" fixtures that are just malformed JSON         â”‚
â”‚     (parser error != validation error)                      â”‚
â”‚  âŒ Assuming tool works because output shape is correct     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Deliberate Schema-Invalid Fixture Requirement

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AT LEAST ONE FIXTURE MUST BE SCHEMA-INVALID (NOT MALFORMED)â”‚
â”‚                                                             â”‚
â”‚  DISTINCTION:                                               â”‚
â”‚                                                             â”‚
â”‚  MALFORMED (parser error):                                  â”‚
â”‚     {invalid json                                           â”‚
â”‚     â†’ Caught before validation even runs                    â”‚
â”‚                                                             â”‚
â”‚  SCHEMA-INVALID (validation error):                         â”‚
â”‚     {"unknown_field": true, "missing_required": ...}        â”‚
â”‚     â†’ Parses fine, but fails model_validate()               â”‚
â”‚                                                             â”‚
â”‚  WHY THIS MATTERS:                                          â”‚
â”‚  A stub can detect malformed JSON without calling your      â”‚
â”‚  actual validation logic. Schema-invalid files FORCE the    â”‚
â”‚  tool to actually run validation.                           â”‚
â”‚                                                             â”‚
â”‚  REQUIRED FIXTURE SET:                                      â”‚
â”‚                                                             â”‚
â”‚  fixtures/                                                  â”‚
â”‚  â”œâ”€â”€ valid/                                                 â”‚
â”‚  â”‚   â”œâ”€â”€ complete.json      # All required fields           â”‚
â”‚  â”‚   â””â”€â”€ minimal.json       # Only required fields          â”‚
â”‚  â”œâ”€â”€ invalid/                                               â”‚
â”‚  â”‚   â”œâ”€â”€ malformed.json     # Bad JSON syntax               â”‚
â”‚  â”‚   â”œâ”€â”€ extra_field.json   # Unknown field (if forbidden)  â”‚
â”‚  â”‚   â”œâ”€â”€ wrong_type.json    # Field with wrong type         â”‚
â”‚  â”‚   â””â”€â”€ missing_req.json   # Missing required field        â”‚
â”‚  â””â”€â”€ exactly_N/             # Known count for == tests      â”‚
â”‚      â””â”€â”€ (exactly N files)                                  â”‚
â”‚                                                             â”‚
â”‚  TEST REQUIREMENT:                                          â”‚
â”‚  At least one test must assert failed > 0 against a         â”‚
â”‚  schema-invalid (not just malformed) fixture.               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ STRUCTURAL VS SEMANTIC TEST ASSUMPTIONS

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TESTS CAN BREAK ON LEGITIMATE REFACTORS IF TOO STRUCTURAL  â”‚
â”‚                                                             â”‚
â”‚  PROBLEM:                                                   â”‚
â”‚  Test assumes dict access:                                  â”‚
â”‚     ir = parser.to_ir()                                     â”‚
â”‚     stats = ir.security.get("statistics", {})               â”‚
â”‚     assert stats.get("has_script") is True                  â”‚
â”‚                                                             â”‚
â”‚  If code evolves to typed model:                            â”‚
â”‚     ir.security = SecurityInfo(statistics=Stats(...))       â”‚
â”‚                                                             â”‚
â”‚  Test breaks even though semantics are preserved!           â”‚
â”‚                                                             â”‚
â”‚  SOLUTIONS:                                                 â”‚
â”‚                                                             â”‚
â”‚  Option A: Type-agnostic accessor:                          â”‚
â”‚     def get_stat(obj, key):                                 â”‚
â”‚         if hasattr(obj, key):                               â”‚
â”‚             return getattr(obj, key)                        â”‚
â”‚         return obj.get(key) if isinstance(obj, dict) else Noneâ”‚
â”‚                                                             â”‚
â”‚  Option B: Tie test to explicit type contract:              â”‚
â”‚     # This test assumes ir.security is dict                 â”‚
â”‚     # Update when IR moves to typed SecurityInfo model      â”‚
â”‚     assert isinstance(ir.security, dict), "Update test!"    â”‚
â”‚                                                             â”‚
â”‚  Option C: Test the semantic, not the structure:            â”‚
â”‚     # Don't care if dict or model, just check behavior      â”‚
â”‚     assert ir.has_dangerous_content() == True               â”‚
â”‚                                                             â”‚
â”‚  DOCUMENT the assumption:                                   â”‚
â”‚  # STRUCTURAL ASSUMPTION: ir.security is dict               â”‚
â”‚  # BREAKS IF: IR refactors to typed SecurityInfo            â”‚
â”‚  # UPDATE WHEN: Phase 2 typed models complete               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš–ï¸ POLICY VS SAFETY INVARIANT TESTS

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DISTINGUISH TESTS THAT ENCODE POLICY FROM SAFETY INVARIANTSâ”‚
â”‚                                                             â”‚
â”‚  SAFETY INVARIANT (should never change):                    â”‚
â”‚  - Script tags are detected                                 â”‚
â”‚  - Malicious content is flagged                             â”‚
â”‚  - Parser doesn't crash on malformed input                  â”‚
â”‚                                                             â”‚
â”‚  POLICY (may change with configuration/profiles):           â”‚
â”‚  - Clean doc is NOT blocked for embedding                   â”‚
â”‚  - Permissive profile allows math plugin                    â”‚
â”‚  - Specific link schemes trigger warnings                   â”‚
â”‚                                                             â”‚
â”‚  WHY IT MATTERS:                                            â”‚
â”‚  If you treat current policy as hard invariant:             â”‚
â”‚  - Tests break when policy legitimately changes             â”‚
â”‚  - Profile evolution becomes "breaking change"              â”‚
â”‚  - Heuristic improvements cause test failures               â”‚
â”‚                                                             â”‚
â”‚  REQUIRED: Label tests by category:                         â”‚
â”‚                                                             â”‚
â”‚  # SAFETY INVARIANT: Script detection must work             â”‚
â”‚  def test_script_detected():                                â”‚
â”‚      ...                                                    â”‚
â”‚                                                             â”‚
â”‚  # POLICY TEST: Current permissive profile behavior         â”‚
â”‚  # May change if: profile definitions evolve                â”‚
â”‚  def test_clean_doc_not_blocked():                          â”‚
â”‚      ...                                                    â”‚
â”‚                                                             â”‚
â”‚  BENEFIT: When policy changes, you know which tests to      â”‚
â”‚  update vs which failures indicate real bugs.               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ CONSCIOUS TRADE-OFF DOCUMENTATION

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SOME TESTS ARE INTENTIONALLY WEAK - DOCUMENT WHY           â”‚
â”‚                                                             â”‚
â”‚  Not all tests can or should be bulletproof. But:           â”‚
â”‚  - Weak tests that LOOK strong create false confidence      â”‚
â”‚  - Undocumented trade-offs become forgotten oversights      â”‚
â”‚                                                             â”‚
â”‚  PATTERN: Mark intentional weaknesses explicitly            â”‚
â”‚                                                             â”‚
â”‚  # TRADE-OFF: This test only verifies command presence,     â”‚
â”‚  # not correct wiring. Full CI testing would be brittle     â”‚
â”‚  # and duplicate what CI itself validates. Accepted risk:   â”‚
â”‚  # someone could write plausible-looking but broken CI.     â”‚
â”‚  def test_ci_workflow_commands_present():                   â”‚
â”‚      ...                                                    â”‚
â”‚                                                             â”‚
â”‚  REQUIRED FOR INTENTIONALLY WEAK TESTS:                     â”‚
â”‚                                                             â”‚
â”‚  âœ… Comment explaining WHY test is limited                  â”‚
â”‚  âœ… What risk is accepted                                   â”‚
â”‚  âœ… What would break if assumption is wrong                 â”‚
â”‚  âœ… Whether stronger test is possible but rejected          â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN:                                                 â”‚
â”‚  âŒ Weak test with no explanation (looks like oversight)    â”‚
â”‚  âŒ "This is good enough" without stating trade-off         â”‚
â”‚  âŒ Assuming readers know why test is limited               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Trade-off Documentation Template

```
# CONSCIOUS TRADE-OFF: [Test Name]
# 
# LIMITATION: [What this test doesn't verify]
# 
# REASON: [Why stronger test was rejected]
#   - [Specific reason 1]
#   - [Specific reason 2]
# 
# ACCEPTED RISK: [What could go wrong]
# 
# MITIGATION: [How the risk is handled elsewhere, if at all]
# 
# REVISIT IF: [Conditions under which to strengthen this test]
```

---

## ğŸ”— FORCED REFACTOR TESTS

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TESTS SHOULD FORCE UPDATES WHEN UNDERLYING CODE EVOLVES    â”‚
â”‚                                                             â”‚
â”‚  PROBLEM:                                                   â”‚
â”‚  Phase 1 test uses dict access:                             â”‚
â”‚     security = validated.metadata.get("security", {})       â”‚
â”‚                                                             â”‚
â”‚  Phase 2 introduces typed Metadata model, but:              â”‚
â”‚  - Test still passes (dict access still works via __getitem__)â”‚
â”‚  - No one remembers to update test to typed access          â”‚
â”‚  - Test lags behind code evolution                          â”‚
â”‚                                                             â”‚
â”‚  SOLUTION: Add "evolution assertions" that break:           â”‚
â”‚                                                             â”‚
â”‚  # Phase 1: Metadata is dict (update when typed)            â”‚
â”‚  assert isinstance(validated.metadata, dict), \             â”‚
â”‚      "PHASE 2 TODO: Update to typed Metadata access"        â”‚
â”‚                                                             â”‚
â”‚  # When Phase 2 happens, this fails and reminds you         â”‚
â”‚                                                             â”‚
â”‚  OR: Use feature flags to switch test behavior:             â”‚
â”‚                                                             â”‚
â”‚  if TYPED_MODELS_ENABLED:                                   â”‚
â”‚      assert isinstance(validated.metadata, Metadata)        â”‚
â”‚      stats = validated.metadata.security.statistics         â”‚
â”‚  else:                                                      â”‚
â”‚      assert isinstance(validated.metadata, dict)            â”‚
â”‚      stats = validated.metadata.get("security", {})...      â”‚
â”‚                                                             â”‚
â”‚  BENEFIT: Tests that intentionally break remind you to      â”‚
â”‚  update them when the underlying system evolves.            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ›¡ï¸ CI AS LAST SAFETY NET (ACKNOWLEDGE IT)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WHEN RELYING ON CI RATHER THAN LOCAL DISCIPLINE, SAY SO    â”‚
â”‚                                                             â”‚
â”‚  EXAMPLE:                                                   â”‚
â”‚  Process: Create artifact with placeholder â†’ Edit â†’ Commit  â”‚
â”‚  Risk: Forget to re-run tests after editing                 â”‚
â”‚  Mitigation: CI will catch placeholders                     â”‚
â”‚                                                             â”‚
â”‚  This is VALID, but should be EXPLICIT:                     â”‚
â”‚                                                             â”‚
â”‚  ## Safety Net Documentation                                â”‚
â”‚                                                             â”‚
â”‚  | Check                  | Local Enforcement | CI Enforcement â”‚
â”‚  |------------------------|-------------------|----------------|
â”‚  | Placeholder detection  | Honor system      | Hard gate      â”‚
â”‚  | Clean Table (Phase 0-2)| Per-task only     | Not enforced   â”‚
â”‚  | Clean Table (Phase 3+) | Per-task only     | Repo-wide      â”‚
â”‚  | Schema drift           | Not checked       | git diff gate  â”‚
â”‚                                                             â”‚
â”‚  WHY DOCUMENT THIS:                                         â”‚
â”‚  - Readers know which checks are "CI will catch it" vs      â”‚
â”‚    "you must run locally or you'll break things"            â”‚
â”‚  - Expectations are set correctly                           â”‚
â”‚  - CI failure is understood, not surprising                 â”‚
â”‚                                                             â”‚
â”‚  ANTI-PATTERN: Claiming local discipline enforces X when    â”‚
â”‚  actually only CI catches it.                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Forbidden vs Required Examples

> **Note**: Examples below use Python syntax, but the **patterns apply to any language**.
> The key concepts are: (1) don't just check exit codes, (2) verify actual output content,
> (3) test both success and failure paths distinctly.

```
# âŒ FORBIDDEN: "It runs" test
test_discover_shape():
    result = run(["./tools/discover"])
    assert result.exitCode == 0  # USELESS - stub passes this

# âœ… REQUIRED: Behavior test
test_discover_shape():
    result = run(["./tools/discover"], captureOutput=true)
    assert result.exitCode == 0
    
    # Verify actual output exists and has content
    output = file("all_keys.txt")
    assert output.exists(), "Output file must be created"
    
    lines = output.read().split("\n")
    assert length(lines) >= 10, "Expected â‰¥10 keys discovered"
    
    # Verify specific expected keys appear
    assert any("metadata" in line for line in lines), "Must discover metadata keys"
```

```
# âŒ FORBIDDEN: Exit code tolerance
test_validator():
    result = run(["./tools/validate"])
    assert result.exitCode in (0, 1)  # USELESS - any behavior passes

# âœ… REQUIRED: Distinct behavior verification
test_validator_valid_input():
    result = run(["./tools/validate", "valid.json"], captureOutput=true)
    assert result.exitCode == 0
    assert "PASS" in result.stdout
    assert "errors: 0" in result.stdout

test_validator_invalid_input():
    result = run(["./tools/validate", "invalid.json"], captureOutput=true)
    assert result.exitCode == 1  # Must fail for invalid
    assert "FAIL" in result.stdout
    assert "errors:" in result.stdout
```

---

## ğŸ­ TEST ROLE DISAMBIGUATION

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  A TEST CANNOT BE BOTH MILESTONE-DRIVER AND BRANCH-GUARD    â”‚
â”‚                                                             â”‚
â”‚  TWO DISTINCT TEST ROLES:                                   â”‚
â”‚                                                             â”‚
â”‚  MILESTONE DRIVER:                                          â”‚
â”‚  - Purpose: Drive development toward a goal                 â”‚
â”‚  - Behavior: Must run and fail until goal achieved          â”‚
â”‚  - Example: test_schema_extra_forbid() fails until models   â”‚
â”‚             are complete, then passes                       â”‚
â”‚                                                             â”‚
â”‚  MAIN-BRANCH GUARD:                                         â”‚
â”‚  - Purpose: Prevent regression on protected branch          â”‚
â”‚  - Behavior: Skips on feature branches, runs on main        â”‚
â”‚  - Example: test_full_validation() runs only on main CI     â”‚
â”‚                                                             â”‚
â”‚  THE CONFLICT:                                              â”‚
â”‚  If a test is both "must fail until X" and "skips except    â”‚
â”‚  on main", it provides no feedback during development.      â”‚
â”‚                                                             â”‚
â”‚  SOLUTION: Write two tests if you need both behaviors:      â”‚
â”‚                                                             â”‚
â”‚  test_schema_extra_forbid_milestone():                      â”‚
â”‚      # MILESTONE DRIVER - always runs, expected to fail     â”‚
â”‚      # until Phase 2.4 complete                             â”‚
â”‚      ...                                                    â”‚
â”‚                                                             â”‚
â”‚  test_schema_extra_forbid_guard():                          â”‚
â”‚      # BRANCH GUARD - only runs on main, must pass          â”‚
â”‚      if not is_main_branch(): pytest.skip("main only")      â”‚
â”‚      ...                                                    â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN:                                                 â”‚
â”‚  âŒ Same test trying to serve both purposes                 â”‚
â”‚  âŒ Ambiguous skip conditions that hide failures            â”‚
â”‚  âŒ "Expected: FAIL" in a test that sometimes skips         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§¹ Clean Table Definition

> A task is CLEAN only when ALL are true:

**Completeness**:
- âœ… No unresolved errors or warnings
- âœ… No TODOs, FIXMEs, or placeholders in changed code
- âœ… No unverified assumptions; facts are checked or removed
- âœ… No duplicated, conflicting, or workaround logic
- âœ… Tests pass (not skipped, not mocked away)
- âœ… Code is production-ready (not "good enough for now")

**No Hidden Debt**:
- âœ… No "temporary" fixes or band-aids masking root causes
- âœ… No deferred follow-ups ("adjust later", "check this later")
- âœ… No speculative comments ("you may need to", "might require")
- âœ… Solution is canonical and stable, not a workaround

**No Silent Errors**:
- âœ… All errors raise unconditionally (no swallowed exceptions)
- âœ… No "strict mode" toggles or conditional error handling
- âœ… No environment variables that disable error checking
- âœ… Execution stops on error; no "continue on failure" paths

**File & Symbol Integrity**:
- âœ… All files listed in task "Files:" header actually exist
- âœ… No literal placeholders in artifacts (e.g., `YYYY-MM-DD` must be replaced)
- âœ… No tests for unimplemented features
- âœ… No assumptions about code that weren't verified

**Code Quality (SOLID/KISS/YAGNI)**:
- âœ… **YAGNI**: All new code has immediate consumer (passed decision tree)
- âœ… **KISS**: Simpler alternative was considered and rejected with reason
- âœ… **SRP**: Each new module/class has single clear purpose
- âœ… No unused parameters, hooks, flags, or abstractions
- âœ… Generalizations have â‰¥2 real consumers (not speculative)

**If any box is unchecked â†’ task is NOT complete.**

---

## ğŸ”‡ NO SILENT ERRORS

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ALL ERRORS MUST RAISE UNCONDITIONALLY                      â”‚
â”‚                                                             â”‚
â”‚  There is no "strict mode" toggle. Errors are always fatal. â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN PATTERNS:                                        â”‚
â”‚                                                             â”‚
â”‚  âŒ Conditional raise based on flag:                        â”‚
â”‚     except Exception as e:                                  â”‚
â”‚         if strict:                                          â”‚
â”‚             raise                                           â”‚
â”‚         errors.append(e)  # swallowed!                      â”‚
â”‚         continue                                            â”‚
â”‚                                                             â”‚
â”‚  âŒ Swallowed exception with warning:                       â”‚
â”‚     except Exception as e:                                  â”‚
â”‚         print(f"Warning: {e}")                              â”‚
â”‚         continue                                            â”‚
â”‚                                                             â”‚
â”‚  âŒ Strict mode parameter:                                  â”‚
â”‚     def process(data, strict: bool = True): ...             â”‚
â”‚                                                             â”‚
â”‚  âŒ Environment variable for strict mode:                   â”‚
â”‚     if os.environ.get("STRICT_MODE"):                       â”‚
â”‚         raise SomeError(...)                                â”‚
â”‚                                                             â”‚
â”‚  REQUIRED PATTERN:                                          â”‚
â”‚                                                             â”‚
â”‚  âœ… Unconditional raise, no conditions:                     â”‚
â”‚     except Exception as e:                                  â”‚
â”‚         raise ProcessingError(context, e) from e            â”‚
â”‚                                                             â”‚
â”‚  There must be NO possibility to choose "run without        â”‚
â”‚  error handling". Errors are never optional.                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“– SPEC INTERPRETATION RULES

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NORMATIVE vs ASPIRATIONAL CONTENT                          â”‚
â”‚                                                             â”‚
â”‚  NORMATIVE (must follow exactly):                           â”‚
â”‚  - Phase gates and their criteria                           â”‚
â”‚  - TDD protocol steps                                       â”‚
â”‚  - Clean Table rules                                        â”‚
â”‚  - Test strength rules                                      â”‚
â”‚  - Source of truth hierarchy                                â”‚
â”‚  - Rollback procedures                                      â”‚
â”‚                                                             â”‚
â”‚  ASPIRATIONAL (derive from code, don't enforce):            â”‚
â”‚  - "Final state" / "target architecture" sections           â”‚
â”‚  - Example schemas or data structures                       â”‚
â”‚  - Field/API tables in design documents                     â”‚
â”‚  - Diagrams showing future state                            â”‚
â”‚                                                             â”‚
â”‚  RULE: When you see a complete schema/structure labeled     â”‚
â”‚  "final" or "target" â†’ treat as "where we want to end up"   â”‚
â”‚  NOT as "what must be implemented exactly as written"       â”‚
â”‚                                                             â”‚
â”‚  Derive actual names/structures from RUNNING CODE.          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ DISCOVERY-FIRST PROTOCOL

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DISCOVER BEFORE DEFINING                                   â”‚
â”‚                                                             â”‚
â”‚  When building models/schemas/types for existing code:      â”‚
â”‚                                                             â”‚
â”‚  1. RUN the existing code with representative inputs        â”‚
â”‚  2. CAPTURE actual outputs (JSON, logs, return values)      â”‚
â”‚  3. ANALYZE captured outputs for structure/patterns         â”‚
â”‚  4. DERIVE models from actual outputs                       â”‚
â”‚  5. VALIDATE models against captured outputs                â”‚
â”‚  6. Only then write tests that assert specific structures   â”‚
â”‚                                                             â”‚
â”‚  DO NOT:                                                    â”‚
â”‚  âŒ Define models from design docs alone                    â”‚
â”‚  âŒ Assume field names match documentation                  â”‚
â”‚  âŒ Write tests before discovery phase completes            â”‚
â”‚  âŒ Change existing code to match your models               â”‚
â”‚                                                             â”‚
â”‚  Discovery artifacts should include:                        â”‚
â”‚  - sample_outputs/ directory with real outputs              â”‚
â”‚  - all_keys.txt or similar listing all observed fields      â”‚
â”‚  - type_analysis.json showing inferred types                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Discovery Phase Template

```bash
# 1. Generate sample outputs from real code
[RUN_COMMAND] > sample_outputs/sample_001.[EXT]
[RUN_COMMAND] > sample_outputs/sample_002.[EXT]
# ... repeat for Nâ‰¥10 representative inputs

# 2. Extract all unique keys/fields (method depends on output format)
# For JSON:
cat sample_outputs/*.json | jq -r 'paths | join(".")' | sort -u > all_keys.txt
# For XML:
grep -ohE '<[^/][^>]+>' sample_outputs/*.xml | sort -u > all_keys.txt
# For structured text:
[CUSTOM_EXTRACTION_COMMAND] > all_keys.txt

# 3. Verify discovery completeness
wc -l all_keys.txt
# Expected: â‰¥[MINIMUM_KEYS] unique paths/fields

# 4. Only proceed to model definition after discovery
test $(wc -l < all_keys.txt) -ge [MINIMUM_KEYS] && echo "âœ… Discovery complete" || echo "âŒ Insufficient discovery"
```

---

## ğŸ—ï¸ CODE QUALITY PRINCIPLES

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MANDATORY DESIGN PRINCIPLES                                â”‚
â”‚                                                             â”‚
â”‚  SOLID (for maintainable design):                           â”‚
â”‚  â”œâ”€â”€ Single Responsibility: One module, one purpose         â”‚
â”‚  â”œâ”€â”€ Open/Closed: Open for extension, closed for changes    â”‚
â”‚  â”œâ”€â”€ Liskov Substitution: Subtypes must be substitutable    â”‚
â”‚  â”œâ”€â”€ Interface Segregation: Small, focused interfaces       â”‚
â”‚  â””â”€â”€ Dependency Inversion: Depend on abstractions           â”‚
â”‚                                                             â”‚
â”‚  KISS (simplicity mandate):                                 â”‚
â”‚  â””â”€â”€ Choose simplest solution that solves the problem       â”‚
â”‚      Simpler = easier to understand, maintain, debug        â”‚
â”‚                                                             â”‚
â”‚  YAGNI (feature discipline):                                â”‚
â”‚  â””â”€â”€ Implement features only when currently needed          â”‚
â”‚      No speculative code, no "might need later"             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš¦ YAGNI DECISION TREE

Before implementing ANY new feature, function, or abstraction:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Q1: Is there a CURRENT requirement for this?               â”‚
â”‚      (Not "might need" or "could be useful")                â”‚
â”‚                                                             â”‚
â”‚      NO  â†’ STOP. Do not implement. (YAGNI triggered)        â”‚
â”‚      YES â†’ Continue to Q2                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Q2: Will it be used IMMEDIATELY after it's built?          â”‚
â”‚      (By known code path, not hypothetical consumer)        â”‚
â”‚                                                             â”‚
â”‚      NO  â†’ STOP. Do not implement. (YAGNI triggered)        â”‚
â”‚      YES â†’ Continue to Q3                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Q3: Is it backed by stakeholder request or concrete data?  â”‚
â”‚      (Not speculation or "good practice")                   â”‚
â”‚                                                             â”‚
â”‚      NO  â†’ STOP. Do not implement. (YAGNI triggered)        â”‚
â”‚      YES â†’ Continue to Q4                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Q4: Can it be added LATER without massive rework?          â”‚
â”‚                                                             â”‚
â”‚      YES â†’ WAIT until actually needed. (YAGNI triggered)    â”‚
â”‚      NO  â†’ Implement now (confirmed unavoidable)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

OUTCOME: implement_now = Q1_yes AND Q2_yes AND Q3_yes AND Q4_no
```

### YAGNI Violations to Avoid

| âŒ Violation | Why It's Wrong | âœ… Instead |
|-------------|----------------|-----------|
| Unused parameters | API surface with no consumer | Remove until needed |
| Speculative flags | `enable_feature_x=False` for "someday" | Add when feature exists |
| Premature abstraction | Interface with 1 implementation | Concrete until 2+ consumers |
| Unused hooks | `pre_hook`, `post_hook` with no callers | Plain function |
| Future-proofing | "We might need this later" | Build what's needed now |

---

## âœ… CODE QUALITY CHECKLIST

Add to task completion verification:

```
BEFORE marking any task complete, verify:

â–¡ YAGNI: All new code has immediate consumer (no speculative features)
â–¡ KISS: Simpler alternative was considered
â–¡ SRP: Each new module/class has single clear purpose
â–¡ No unused parameters, hooks, or abstractions added
â–¡ Generalizations have â‰¥2 real consumers today
â–¡ Tests prove current necessity (fail-before, pass-after)
```

---

## ğŸ”„ REFLECTION LOOP (MANDATORY)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BEFORE COMPLETING ANY TASK OR PHASE:                       â”‚
â”‚                                                             â”‚
â”‚  1. PAUSE - Do not proceed to next task                     â”‚
â”‚                                                             â”‚
â”‚  2. REFLECT - Answer these questions:                       â”‚
â”‚     â–¡ Did I achieve the stated objective?                   â”‚
â”‚     â–¡ Can I provide evidence for each claim I made?         â”‚
â”‚     â–¡ Did I verify assumptions or just assume?              â”‚
â”‚     â–¡ Are there risks I identified but didn't mitigate?     â”‚
â”‚     â–¡ Is there anything I'm uncertain about?                â”‚
â”‚                                                             â”‚
â”‚  3. VALIDATE - Check against success criteria:              â”‚
â”‚     â–¡ All tests pass (verified by running them)             â”‚
â”‚     â–¡ Clean Table checklist complete                        â”‚
â”‚     â–¡ No items deferred or skipped                          â”‚
â”‚     â–¡ Every claim has evidence anchor (source + quote)      â”‚
â”‚                                                             â”‚
â”‚  4. DECIDE:                                                 â”‚
â”‚     - All checks pass â†’ Proceed to next task                â”‚
â”‚     - Any uncertainty â†’ STOP and clarify                    â”‚
â”‚     - Any failure â†’ Fix before proceeding                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Semantic Type Distinctions

When reflecting, distinguish between:

| Type | Definition | Requirement |
|------|------------|-------------|
| **FACT** | Verified true statement | Must have evidence anchor |
| **CLAIM** | Assertion being made | Must have supporting evidence |
| **ASSUMPTION** | Unverified belief | Must state + attempt to verify |
| **RISK** | Potential problem | Must have mitigation or acceptance |
| **HYPOTHESIS** | Testable proposition | Must have test + result |

```
âŒ BAD: "The function returns a list" (unstated assumption)
âœ… GOOD: "ASSUMPTION: The function returns a list
         VERIFICATION: grep -A5 'def process' src/module.py
         RESULT: Returns List[str] - CONFIRMED"
```

### Evidence Anchor Requirements

Every significant claim must have:

```
CLAIM: [What you're asserting]
SOURCE: [file:line or command output]
QUOTE: "[Exact text that supports claim]"
```

Example:
```
CLAIM: Parser handles nested structures
SOURCE: tests/test_parser.py:45
QUOTE: "def test_nested_dict(): assert parse({'a': {'b': 1}}) == expected"
```

### Evidence Anchor Scoping

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NOT ALL CLAIMS REQUIRE FULL EVIDENCE ANCHORS               â”‚
â”‚                                                             â”‚
â”‚  Full evidence anchors are MANDATORY for:                   â”‚
â”‚                                                             â”‚
â”‚  âœ… API/Contract changes                                    â”‚
â”‚     "This function now returns X instead of Y"              â”‚
â”‚  âœ… Safety/Security claims                                  â”‚
â”‚     "This input is sanitized before use"                    â”‚
â”‚  âœ… Performance claims                                      â”‚
â”‚     "This is O(n) not O(nÂ²)"                                â”‚
â”‚  âœ… Architectural decisions                                 â”‚
â”‚     "We chose X over Y because..."                          â”‚
â”‚  âœ… Bug fix assertions                                      â”‚
â”‚     "This change fixes issue #123"                          â”‚
â”‚                                                             â”‚
â”‚  Full evidence anchors are OPTIONAL for:                    â”‚
â”‚                                                             â”‚
â”‚  â—‹ Small refactors (rename, move, format)                   â”‚
â”‚  â—‹ Documentation updates                                    â”‚
â”‚  â—‹ Test additions (test itself is evidence)                 â”‚
â”‚  â—‹ Obvious facts ("Python uses .py extension")              â”‚
â”‚                                                             â”‚
â”‚  WHY SCOPE THIS:                                            â”‚
â”‚  - Unbounded requirements create "evidence theater"         â”‚
â”‚  - Teams skip requirements that feel burdensome             â”‚
â”‚  - Focus evidence effort on high-risk claims                â”‚
â”‚                                                             â”‚
â”‚  RULE: When in doubt, add evidence. But don't mandate       â”‚
â”‚  evidence anchors for trivial claims.                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Reflection Template

After each task, explicitly state:

```
REFLECTION: Task [X.Y]
â”œâ”€â”€ Objective achieved: [YES/NO + evidence anchor]
â”œâ”€â”€ Claims made:
â”‚   â”œâ”€â”€ CLAIM: [statement] â†’ SOURCE: [file:line]
â”‚   â””â”€â”€ CLAIM: [statement] â†’ SOURCE: [file:line]
â”œâ”€â”€ Assumptions:
â”‚   â”œâ”€â”€ ASSUMPTION: [statement] â†’ VERIFIED: [YES/NO + how]
â”‚   â””â”€â”€ ASSUMPTION: [statement] â†’ VERIFIED: [YES/NO + how]
â”œâ”€â”€ Risks:
â”‚   â”œâ”€â”€ RISK: [description] â†’ MITIGATION: [action taken]
â”‚   â””â”€â”€ RISK: [description] â†’ MITIGATION: [action taken]
â”œâ”€â”€ Uncertainties: [list any that remain]
â”œâ”€â”€ Confidence: [0-100%] (if <80% â†’ STOP)
â””â”€â”€ Decision: [PROCEED / STOP + reason]
```

### Trace Links (Decision â†’ Evidence â†’ Impact)

For significant decisions, create trace links:

```
TRACE LINK:
â”œâ”€â”€ DECISION: [What action was taken]
â”œâ”€â”€ EVIDENCE:
â”‚   â”œâ”€â”€ CLAIM: [Supporting assertion]
â”‚   â”‚   â””â”€â”€ SOURCE: [file:line], QUOTE: "[text]"
â”‚   â””â”€â”€ CLAIM: [Another assertion]
â”‚       â””â”€â”€ SOURCE: [file:line], QUOTE: "[text]"
â”œâ”€â”€ IMPACT: [Consequence of this decision]
â””â”€â”€ RELATED:
    â”œâ”€â”€ Assumptions: [list]
    â”œâ”€â”€ Risks: [list]
    â””â”€â”€ Mitigations: [list]
```

---

## ğŸ›‘ STOP ON AMBIGUITY

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WHEN TO STOP AND CLARIFY:                                  â”‚
â”‚                                                             â”‚
â”‚  STOP if ANY of these are true:                             â”‚
â”‚                                                             â”‚
â”‚  â–¡ Confidence in approach < 80%                             â”‚
â”‚  â–¡ Missing information required to proceed                  â”‚
â”‚  â–¡ Conflicting requirements detected                        â”‚
â”‚  â–¡ Assumption cannot be verified                            â”‚
â”‚  â–¡ Success criteria unclear or unmeasurable                 â”‚
â”‚  â–¡ Multiple valid interpretations exist                     â”‚
â”‚                                                             â”‚
â”‚  DO NOT:                                                    â”‚
â”‚  âŒ Guess and proceed                                       â”‚
â”‚  âŒ Make assumptions without stating them                   â”‚
â”‚  âŒ Pick arbitrary interpretation                           â”‚
â”‚  âŒ Defer clarification to later                            â”‚
â”‚                                                             â”‚
â”‚  INSTEAD:                                                   â”‚
â”‚  âœ… State what is blocking                                  â”‚
â”‚  âœ… State what information is needed                        â”‚
â”‚  âœ… State what will be done once clarified                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stop Block Template

When stopping for clarification, use this format:

```
â›” STOP: CLARIFICATION REQUIRED

BLOCKING: [What element is missing or ambiguous]
REASON: [Why we cannot proceed without this]
NEEDED: [Minimal information required to continue]
NEXT STEP: [What will be executed once provided]

Awaiting clarification before proceeding.
```

---

## ğŸ§  THINKING vs EXECUTION BOUNDARIES

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SEPARATE REASONING FROM ACTION                             â”‚
â”‚                                                             â”‚
â”‚  --- THINKING ---                                           â”‚
â”‚  â€¢ Assumptions being made                                   â”‚
â”‚  â€¢ Risks being considered                                   â”‚
â”‚  â€¢ Trade-offs being evaluated                               â”‚
â”‚  â€¢ Alternative approaches considered                        â”‚
â”‚  â€¢ Why chosen approach is best                              â”‚
â”‚                                                             â”‚
â”‚  --- EXECUTION ---                                          â”‚
â”‚  â€¢ Exact command/code to run                                â”‚
â”‚  â€¢ Expected output/result                                   â”‚
â”‚  â€¢ How to validate success                                  â”‚
â”‚                                                             â”‚
â”‚  This separation ensures:                                   â”‚
â”‚  â€¢ Reasoning is explicit and reviewable                     â”‚
â”‚  â€¢ Actions are concrete and verifiable                      â”‚
â”‚  â€¢ Mistakes in logic vs execution are distinguishable       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Decision Chain Template

For significant decisions, document:

```
DECISION: [What action to take]
â”œâ”€â”€ WHY: [Evidence-based reason]
â”‚   â””â”€â”€ EVIDENCE: [source:line] "[quote]"
â”œâ”€â”€ WHY NOT alternatives:
â”‚   â”œâ”€â”€ [Alternative 1]: Rejected because [reason]
â”‚   â””â”€â”€ [Alternative 2]: Rejected because [reason]
â”œâ”€â”€ IMPACT: [Expected consequence]
â”œâ”€â”€ ASSUMPTIONS: [What we're assuming is true]
â”‚   â””â”€â”€ VERIFICATION: [How verified, or "UNVERIFIED - RISK"]
â”œâ”€â”€ RISKS: [What could go wrong]
â”‚   â””â”€â”€ MITIGATION: [Action taken, or "ACCEPTED because [reason]"]
â””â”€â”€ VALIDATION: [How we'll verify this was correct]
```

Example:
```
DECISION: Use polling instead of webhooks for data sync
â”œâ”€â”€ WHY: Target API doesn't support webhooks
â”‚   â””â”€â”€ EVIDENCE: docs/api.md:45 "Events API: Coming Q3 2025"
â”œâ”€â”€ WHY NOT alternatives:
â”‚   â”œâ”€â”€ Webhooks: Rejected because not yet available
â”‚   â””â”€â”€ WebSockets: Rejected because API doesn't support
â”œâ”€â”€ IMPACT: 30-second delay in data freshness
â”œâ”€â”€ ASSUMPTIONS: API rate limits allow 2 req/min
â”‚   â””â”€â”€ VERIFICATION: docs/api.md:12 "Rate limit: 100 req/min" - VERIFIED
â”œâ”€â”€ RISKS: Rate limiting during high-traffic periods
â”‚   â””â”€â”€ MITIGATION: Exponential backoff implemented in sync.py:89
â””â”€â”€ VALIDATION: Integration test covers rate limit scenario
```

---

## ğŸ“‹ TASK COMPLETION VALIDATION PIPELINE

Before marking any task complete, execute this pipeline in order:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VALIDATION PIPELINE (Execute in order, stop on failure)    â”‚
â”‚                                                             â”‚
â”‚  1. BUILD decision chains                                   â”‚
â”‚     â””â”€â”€ Document what/why/why_not for significant choices   â”‚
â”‚                                                             â”‚
â”‚  2. ATTACH evidence anchors                                 â”‚
â”‚     â””â”€â”€ Every claim has: source + quote                     â”‚
â”‚     â””â”€â”€ Missing evidence â†’ STOP                             â”‚
â”‚                                                             â”‚
â”‚  3. CREATE trace links                                      â”‚
â”‚     â””â”€â”€ Decision â†’ Evidence â†’ Impact connected              â”‚
â”‚                                                             â”‚
â”‚  4. ASSESS confidence                                       â”‚
â”‚     â””â”€â”€ If < 80% certain on any element â†’ STOP              â”‚
â”‚                                                             â”‚
â”‚  5. SEPARATE thinking from execution                        â”‚
â”‚     â””â”€â”€ Reasoning documented, actions concrete              â”‚
â”‚                                                             â”‚
â”‚  6. REFLECT                                                 â”‚
â”‚     â””â”€â”€ Evidence + decisions satisfy success criteria?      â”‚
â”‚     â””â”€â”€ If not â†’ STOP                                       â”‚
â”‚                                                             â”‚
â”‚  7. CROSS-REFERENCE                                         â”‚
â”‚     â””â”€â”€ No conflicts between claims and results             â”‚
â”‚     â””â”€â”€ All assumptions verified or flagged                 â”‚
â”‚     â””â”€â”€ All risks mitigated or accepted with reason         â”‚
â”‚                                                             â”‚
â”‚  If ANY step fails â†’ Do not complete task. Fix or STOP.     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Quick Validation Checklist

```
â–¡ Decision chains documented (what/why/why_not/impact)
â–¡ Evidence anchors present (claim â†’ source:line â†’ quote)
â–¡ Trace links created for significant decisions
â–¡ Confidence â‰¥ 80% on all elements
â–¡ Thinking separated from execution
â–¡ Reflection complete with semantic types distinguished
â–¡ Cross-reference: no conflicts, assumptions verified, risks handled
â–¡ Clean Table checklist passes
â–¡ Tests pass (actually run, not assumed)
```

---

## ğŸ“ File Manifest Rules

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FILE MANIFEST ENFORCEMENT:                                 â”‚
â”‚                                                             â”‚
â”‚  Every task has a "Files:" header listing affected files.   â”‚
â”‚                                                             â”‚
â”‚  RULE: If a file is listed, it MUST:                        â”‚
â”‚  1. Be created/modified by a step in that task              â”‚
â”‚  2. Exist after task completion                             â”‚
â”‚  3. Be verifiable via: test -f <filename>                   â”‚
â”‚                                                             â”‚
â”‚  If a file is listed but not created â†’ TASK INCOMPLETE      â”‚
â”‚  If a file is created but not listed â†’ Add it to Files:     â”‚
â”‚                                                             â”‚
â”‚  GHOST FILES ARE FORBIDDEN                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### File Manifest Verification Command

```bash
# Run after each task to verify all listed files exist
# Replace FILE_LIST with actual files from task header
for f in FILE_LIST; do
    test -f "$f" && echo "âœ… $f" || echo "âŒ MISSING: $f"
done
```

### Canonical FILE_LIST Block

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SINGLE SOURCE FOR BOTH "Files:" HEADER AND VERIFICATION    â”‚
â”‚                                                             â”‚
â”‚  PROBLEM:                                                   â”‚
â”‚  - "Files:" header is free-text (easy to forget updates)    â”‚
â”‚  - Verification uses separate list (can drift)              â”‚
â”‚  - Ghost files sneak in despite having "a rule"             â”‚
â”‚                                                             â”‚
â”‚  SOLUTION: Define canonical FILE_LIST block per task:       â”‚
â”‚                                                             â”‚
â”‚  ```bash                                                    â”‚
â”‚  # Task 0.1 FILE_LIST (single source of truth)              â”‚
â”‚  TASK_0_1_FILES=(                                           â”‚
â”‚      "tools/discover_parser_shape.py"                       â”‚
â”‚      "tools/discovery_output/all_keys.txt"                  â”‚
â”‚      "tests/test_discover_parser_shape.py"                  â”‚
â”‚  )                                                          â”‚
â”‚  ```                                                        â”‚
â”‚                                                             â”‚
â”‚  USE FOR "Files:" header:                                   â”‚
â”‚  Files: ${TASK_0_1_FILES[@]}                                â”‚
â”‚                                                             â”‚
â”‚  USE FOR verification:                                      â”‚
â”‚  for f in "${TASK_0_1_FILES[@]}"; do                        â”‚
â”‚      test -f "$f" || { echo "MISSING: $f"; exit 1; }        â”‚
â”‚  done                                                       â”‚
â”‚                                                             â”‚
â”‚  BENEFIT: One list, used in both places. Cannot drift.      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¢ TASK ID UNIQUENESS

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TASK IDs MUST BE UNIQUE AND 1:1 WITH TASKS                 â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN:                                                 â”‚
â”‚  âŒ Reusing same ID for different tasks                     â”‚
â”‚     Task 2.2a: Structure Models                             â”‚
â”‚     Task 2.2a: Update Empty Method  â† CONFLICT!             â”‚
â”‚                                                             â”‚
â”‚  âŒ Slices/sub-tasks without tracked IDs                    â”‚
â”‚     Task 2.2 mentions "slices 2.2a-e" but Appendix          â”‚
â”‚     only tracks 2.2a                                        â”‚
â”‚                                                             â”‚
â”‚  REQUIRED:                                                  â”‚
â”‚  âœ… Every task has unique ID (X.Y or X.Y.Z)                 â”‚
â”‚  âœ… Every slice mentioned in narrative has ID               â”‚
â”‚  âœ… Appendix/tracking table covers ALL IDs                  â”‚
â”‚  âœ… ID â†’ Task is bijective (one-to-one mapping)             â”‚
â”‚                                                             â”‚
â”‚  WHY: Ambiguous IDs break file tracking, progress logs,     â”‚
â”‚  and allow "which 2.2a?" confusion during execution.        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Task ID Verification

Before finalizing task list:
```bash
# Extract all task IDs mentioned in document
grep -oE "Task [0-9]+\.[0-9]+[a-z]?" PROJECT_TASKS.md | sort | uniq -d
# Expected: NO OUTPUT (no duplicates)

# Verify Appendix covers all IDs
grep -oE "[0-9]+\.[0-9]+[a-z]?" PROJECT_TASKS.md | sort -u > /tmp/task_ids.txt
grep -oE "^[|] *[0-9]+\.[0-9]+[a-z]?" PROJECT_TASKS.md | sort -u > /tmp/tracked_ids.txt
comm -23 /tmp/task_ids.txt /tmp/tracked_ids.txt
# Expected: NO OUTPUT (all tasks tracked in Appendix)
```

---

## ğŸ” SCOPE DECISIONS TABLE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EXPLICITLY DECLARE WHAT'S IN AND OUT OF SCOPE              â”‚
â”‚                                                             â”‚
â”‚  At the top of your task list, include a scope table:       â”‚
â”‚                                                             â”‚
â”‚  | Item                    | Status      | Rationale       â”‚
â”‚  |-------------------------|-------------|-----------------|
â”‚  | Core feature X          | IN SCOPE    | Primary goal    â”‚
â”‚  | Edge case Y             | IN SCOPE    | Critical path   â”‚
â”‚  | Nice-to-have Z          | OUT OF SCOPE| Defer to v2     â”‚
â”‚  | Legacy cleanup W        | OUT OF SCOPE| Separate effort â”‚
â”‚                                                             â”‚
â”‚  WHY: Prevents scope creep and makes exclusions explicit.   â”‚
â”‚  Anyone reading knows what WON'T be done, not just what     â”‚
â”‚  will be done.                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš« PROCEDURAL DEADLOCK DETECTION

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCOPE RULES CAN CREATE UNSATISFIABLE CONTRACTS             â”‚
â”‚                                                             â”‚
â”‚  DEADLOCK EXAMPLE:                                          â”‚
â”‚  Rule 1: "Parser behavior changes are OUT OF SCOPE"         â”‚
â”‚  Rule 2: "Do NOT weaken safety tests"                       â”‚
â”‚  Situation: Safety test fails due to parser behavior        â”‚
â”‚  Result: Cannot change parser, cannot change test = STUCK   â”‚
â”‚                                                             â”‚
â”‚  DETECTION: For each "X is forbidden" rule, ask:            â”‚
â”‚  "If Y depends on X, and Y is also forbidden to change,     â”‚
â”‚   what happens when they conflict?"                         â”‚
â”‚                                                             â”‚
â”‚  REQUIRED: Every scope constraint needs an escape path:     â”‚
â”‚                                                             â”‚
â”‚  Option A: Carve out exception category                     â”‚
â”‚     "Parser changes OUT OF SCOPE, EXCEPT for safety         â”‚
â”‚      semantics when required by failing safety tests"       â”‚
â”‚                                                             â”‚
â”‚  Option B: Allow observational tests (xfail)                â”‚
â”‚     "Tests are normative, but may be marked xfail with      â”‚
â”‚      documented rationale when parser disagrees"            â”‚
â”‚                                                             â”‚
â”‚  Option C: Escalation path                                  â”‚
â”‚     "If conflict detected â†’ STOP, escalate to tech lead"    â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN:                                                 â”‚
â”‚  âŒ Two "cannot change X" rules with no escape path         â”‚
â”‚  âŒ "Tests are normative" + "Implementation is SSOT" with   â”‚
â”‚     no resolution when they conflict                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ SCOPE DELTA LEDGER

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TRACK WHAT'S EXCLUDED AND WHY (AUDITABLE)                  â”‚
â”‚                                                             â”‚
â”‚  When task list excludes items from a design spec,          â”‚
â”‚  maintain an explicit ledger (not just prose):              â”‚
â”‚                                                             â”‚
â”‚  ## Scope Delta vs Design Spec                              â”‚
â”‚                                                             â”‚
â”‚  | Excluded Item        | Why Excluded      | Consequence    | Follow-on      â”‚
â”‚  |----------------------|-------------------|----------------|----------------|
â”‚  | Plugin policy tests  | Complexity budget | Plugin behavior| TASK-LIST-v2   â”‚
â”‚  |                      |                   | not guaranteed |                â”‚
â”‚  | Semantic extraction  | Separate concern  | Deep parsing   | PARSER-v3      â”‚
â”‚  |                      |                   | not validated  |                â”‚
â”‚  | Field X validation   | Not yet used      | Field may drift| When consumed  â”‚
â”‚                                                             â”‚
â”‚  REQUIRED COLUMNS:                                          â”‚
â”‚  - Excluded Item: What was cut                              â”‚
â”‚  - Why Excluded: Rationale                                  â”‚
â”‚  - Consequence: What invariants are NOT guaranteed          â”‚
â”‚  - Follow-on: Where/when it will be addressed               â”‚
â”‚                                                             â”‚
â”‚  WHY: Prose exclusions are forgotten. Ledger is auditable.  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸŒ GLOBAL CLEAN TABLE ENFORCEMENT

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLEAN TABLE CHECKS MUST BE GLOBAL, NOT SELECTIVE           â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN:                                                 â”‚
â”‚  âŒ Ad-hoc greps on only some files:                        â”‚
â”‚     grep "TODO" file1.py file2.py  # Misses file3.py!       â”‚
â”‚                                                             â”‚
â”‚  âŒ Phase gates that only check changed files:              â”‚
â”‚     # Doesn't catch pre-existing debt                       â”‚
â”‚                                                             â”‚
â”‚  REQUIRED:                                                  â”‚
â”‚  âœ… Single global check across entire repo:                 â”‚
â”‚     grep -rn "TODO\|FIXME\|XXX" src/ tests/ tools/          â”‚
â”‚                                                             â”‚
â”‚  âœ… Phase artifacts checked for placeholders:               â”‚
â”‚     grep -E "YYYY|MM-DD|placeholder|\[.*\]" .phase-*.json   â”‚
â”‚                                                             â”‚
â”‚  âœ… Ideally: automated test that fails on any violation     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Heredoc/Template Placeholder Detection

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HEREDOCS AND TEMPLATES OFTEN CONTAIN LITERAL PLACEHOLDERS  â”‚
â”‚                                                             â”‚
â”‚  PROBLEM: Task list shows:                                  â”‚
â”‚     cat > .phase-0.complete.json << 'EOF'                   â”‚
â”‚     {                                                       â”‚
â”‚       "completion_date": "YYYY-MM-DDTHH:MM:SSZ"  â† LITERAL! â”‚
â”‚     }                                                       â”‚
â”‚     EOF                                                     â”‚
â”‚                                                             â”‚
â”‚  If executed literally, the artifact contains a placeholder â”‚
â”‚  which violates Clean Table but no check catches it.        â”‚
â”‚                                                             â”‚
â”‚  REQUIRED:                                                  â”‚
â”‚  âœ… Either: Use shell variable substitution:                â”‚
â”‚     "completion_date": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"     â”‚
â”‚                                                             â”‚
â”‚  âœ… Or: Explicit instruction to replace before commit:      â”‚
â”‚     "Replace YYYY-MM-DDTHH:MM:SSZ with actual timestamp"    â”‚
â”‚                                                             â”‚
â”‚  âœ… And: Global check for common placeholder patterns:      â”‚
â”‚     grep -rE "YYYY|XXXX|\[INSERT|\[TODO\]|placeholder"      â”‚
â”‚                                                             â”‚
â”‚  COMMON PLACEHOLDER PATTERNS TO DETECT:                     â”‚
â”‚  â€¢ YYYY-MM-DD, YYYY/MM/DD (date placeholders)               â”‚
â”‚  â€¢ [INSERT X], [TODO], [PLACEHOLDER]                        â”‚
â”‚  â€¢ YOUR_*, MY_*, EXAMPLE_* (template variables)             â”‚
â”‚  â€¢ <description>, <your-value> (XML-style placeholders)     â”‚
â”‚  â€¢ __REPLACE__, %%VARIABLE%% (template markers)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Machine-Detectable Placeholder Tokens

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  USE EXPLICIT TOKENS, NOT AMBIGUOUS PATTERNS                â”‚
â”‚                                                             â”‚
â”‚  PROBLEM with grep -E "\[.*\]":                             â”‚
â”‚  âŒ Matches normal markdown: [link text](url)               â”‚
â”‚  âŒ Matches checklists: [ ] item, [x] done                  â”‚
â”‚  âŒ Creates noise â†’ check gets ignored                      â”‚
â”‚  âŒ Misses non-bracket placeholders                         â”‚
â”‚                                                             â”‚
â”‚  SOLUTION: Adopt explicit placeholder token format:         â”‚
â”‚                                                             â”‚
â”‚  FORMAT: [[PH:PLACEHOLDER_NAME]]                            â”‚
â”‚                                                             â”‚
â”‚  EXAMPLES:                                                  â”‚
â”‚  - [[PH:PROJECT_NAME]]                                      â”‚
â”‚  - [[PH:FAST_TEST_COMMAND]]                                 â”‚
â”‚  - [[PH:YOUR_SCHEMA_URL]]                                   â”‚
â”‚                                                             â”‚
â”‚  DETECTOR (noise-free):                                     â”‚
â”‚     grep -RIn '\[\[PH:' .                                   â”‚
â”‚     # Expected: 0 results in committed code                 â”‚
â”‚                                                             â”‚
â”‚  BENEFITS:                                                  â”‚
â”‚  âœ… No false positives from normal markdown                 â”‚
â”‚  âœ… Self-documenting (name says what's needed)              â”‚
â”‚  âœ… Easy to find and replace                                â”‚
â”‚  âœ… CI check is reliable                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Global Clean Table Test Template

```
# tests/test_clean_table_global.[ext]
# This test runs on every CI build and fails if debt found

test_no_todos_in_codebase():
    result = run(["grep", "-rn", "TODO|FIXME|XXX", "src/", "tests/", "tools/"])
    assert result.exitCode == 1, "Found TODOs in codebase"  # grep returns 1 if no match

test_no_placeholders_in_artifacts():
    result = run(["grep", "-rE", "YYYY-MM-DD|placeholder|\\[INSERT", "."])
    assert result.exitCode == 1, "Found placeholders in repo"

test_no_template_literals_in_generated_files():
    # Check phase completion artifacts
    for artifact in glob(".phase-*.json"):
        content = read(artifact)
        assert "YYYY" not in content, f"Placeholder in {artifact}"
        assert "XXXX" not in content, f"Placeholder in {artifact}"
        assert "[INSERT" not in content, f"Placeholder in {artifact}"
```

---

## ğŸ¯ PHASE GATE SCOPE ALIGNMENT

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE GATES MUST ENFORCE THE SAME SCOPE AS GLOBAL RULES    â”‚
â”‚                                                             â”‚
â”‚  PROBLEM:                                                   â”‚
â”‚  Global rule: "Clean Table applies to entire repo"          â”‚
â”‚  Phase 0 gate: grep TODO file1.py file2.py  # Only 2 files! â”‚
â”‚                                                             â”‚
â”‚  RESULT:                                                    â”‚
â”‚  - TODOs can exist in src/ during Phase 0-2                 â”‚
â”‚  - Only Phase 3+ catches them (when global test added)      â”‚
â”‚  - Early phases have weaker enforcement than later phases   â”‚
â”‚                                                             â”‚
â”‚  REQUIRED:                                                  â”‚
â”‚  âœ… If rule is global, ALL phase gates enforce it globally  â”‚
â”‚  âœ… Or: Global test exists from Phase 0 (not introduced     â”‚
â”‚     later)                                                  â”‚
â”‚  âœ… Phase gates should reference the global test, not       â”‚
â”‚     duplicate logic with narrower scope                     â”‚
â”‚                                                             â”‚
â”‚  PATTERN:                                                   â”‚
â”‚  Phase 0 Gate:                                              â”‚
â”‚    - Run: [FAST_TEST_COMMAND] tests/test_clean_table.py     â”‚
â”‚    - NOT: grep TODO only-these-two-files.py                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â±ï¸ TEMPORAL DEBT WINDOW

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AVOID WORKFLOWS THAT CREATE TEMPORARY DEBT IN TRACKED FILESâ”‚
â”‚                                                             â”‚
â”‚  PROBLEM WORKFLOW:                                          â”‚
â”‚  1. Create .phase-0.complete.json with placeholder date     â”‚
â”‚  2. Run tests â†’ pass (placeholder not yet committed)        â”‚
â”‚  3. Edit file to replace placeholder with real date         â”‚
â”‚  4. Forget to re-run tests                                  â”‚
â”‚  5. Commit via GUI â†’ placeholder could slip through         â”‚
â”‚                                                             â”‚
â”‚  The "debt window" is step 1â†’3 where invalid content exists â”‚
â”‚  in a tracked file but hasn't been caught yet.              â”‚
â”‚                                                             â”‚
â”‚  SOLUTIONS:                                                 â”‚
â”‚                                                             â”‚
â”‚  Option A: Never create placeholders                        â”‚
â”‚     Use shell substitution: $(date -u +%Y-%m-%dT%H:%M:%SZ)  â”‚
â”‚     File is correct from creation                           â”‚
â”‚                                                             â”‚
â”‚  Option B: Pre-commit hook                                  â”‚
â”‚     Hook runs placeholder check before every commit         â”‚
â”‚     Cannot accidentally commit debt                         â”‚
â”‚                                                             â”‚
â”‚  Option C: CI as safety net (current approach)              â”‚
â”‚     Global test catches placeholders                        â”‚
â”‚     But requires discipline to run tests after edits        â”‚
â”‚                                                             â”‚
â”‚  DOCUMENT which option you're using and its trade-offs      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ–¥ï¸ Environment Portability Rules

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FORBIDDEN PATTERNS (examples from various languages):      â”‚
â”‚                                                             â”‚
â”‚  âŒ Hard-coded paths:                                       â”‚
â”‚     .venv/bin/python, /usr/bin/node, C:\Ruby\bin\ruby      â”‚
â”‚  âŒ System-specific paths:                                  â”‚
â”‚     /usr/local/bin/*, /opt/homebrew/bin/*                  â”‚
â”‚  âŒ Version-pinned executables in paths:                    â”‚
â”‚     python3.11, node18, ruby3.2                            â”‚
â”‚                                                             â”‚
â”‚  REQUIRED PATTERNS:                                         â”‚
â”‚                                                             â”‚
â”‚  âœ… Use runtime's self-reference where available:           â”‚
â”‚     Python: sys.executable                                  â”‚
â”‚     Node: process.execPath                                  â”‚
â”‚     Ruby: RbConfig.ruby                                     â”‚
â”‚  âœ… Use package manager invocation:                         â”‚
â”‚     python -m module, npx command, bundle exec              â”‚
â”‚  âœ… Use shell lookup:                                       â”‚
â”‚     $(which python), $(which node), $(which ruby)          â”‚
â”‚                                                             â”‚
â”‚  For tool dependencies (rg, jq, etc.):                      â”‚
â”‚  - Document in Prerequisites section                        â”‚
â”‚  - Add existence check in verification commands             â”‚
â”‚  - Provide installation instructions                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš SHELL COMMAND CORRECTNESS

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SHELL SNIPPETS ARE COPY-PASTE TARGETS - MUST BE CORRECT    â”‚
â”‚                                                             â”‚
â”‚  COMMON COPY-PASTE TRAPS:                                   â”‚
â”‚                                                             â”‚
â”‚  âŒ WRONG - assigns literal string, not command output:     â”‚
â”‚     TESTS_PASSED=[YOUR_TEST_COUNT_COMMAND]                  â”‚
â”‚     VAR=[some command]                                      â”‚
â”‚                                                             â”‚
â”‚  âœ… CORRECT - command substitution:                         â”‚
â”‚     TESTS_PASSED="$(your_test_count_command)"               â”‚
â”‚     VAR="$(some command)"                                   â”‚
â”‚                                                             â”‚
â”‚  âŒ WRONG - unquoted, breaks on spaces:                     â”‚
â”‚     for f in $FILE_LIST; do                                 â”‚
â”‚                                                             â”‚
â”‚  âœ… CORRECT - quoted array:                                 â”‚
â”‚     for f in "${FILE_LIST[@]}"; do                          â”‚
â”‚                                                             â”‚
â”‚  âŒ WRONG - no error handling:                              â”‚
â”‚     RESULT=$(command)                                       â”‚
â”‚                                                             â”‚
â”‚  âœ… CORRECT - fail fast on empty/error:                     â”‚
â”‚     RESULT="$(command)" || { echo "Failed"; exit 1; }       â”‚
â”‚     [[ -n "$RESULT" ]] || { echo "Empty"; exit 1; }         â”‚
â”‚                                                             â”‚
â”‚  RULE: Every shell snippet in docs must be syntactically    â”‚
â”‚  correct and safe to copy-paste directly.                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” GREP PORTABILITY

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GREP BEHAVIOR DIFFERS BETWEEN GNU AND BSD                  â”‚
â”‚                                                             â”‚
â”‚  PROBLEM:                                                   â”‚
â”‚  grep -r "TODO\|FIXME" src/   # Works on GNU, fails on BSD  â”‚
â”‚                                                             â”‚
â”‚  DIFFERENCES:                                               â”‚
â”‚  â€¢ -r vs -R (symlink handling)                              â”‚
â”‚  â€¢ \| requires -E on BSD (extended regex)                   â”‚
â”‚  â€¢ Exit codes can vary                                      â”‚
â”‚  â€¢ Default regex dialect differs                            â”‚
â”‚                                                             â”‚
â”‚  PORTABLE PATTERN (works on both):                          â”‚
â”‚     grep -RInE '(TODO|FIXME|XXX)' src/ tests/ tools/        â”‚
â”‚                                                             â”‚
â”‚  FLAGS EXPLAINED:                                           â”‚
â”‚  -R  Recursive (follows symlinks consistently)              â”‚
â”‚  -I  Skip binary files                                      â”‚
â”‚  -n  Show line numbers                                      â”‚
â”‚  -E  Extended regex (required for | without escaping)       â”‚
â”‚                                                             â”‚
â”‚  DOCUMENT YOUR REQUIREMENT:                                 â”‚
â”‚  If you use GNU-specific features, add to Prerequisites:    â”‚
â”‚     "GNU grep required (BSD grep may not work)"             â”‚
â”‚                                                             â”‚
â”‚  OR use cross-platform tools:                               â”‚
â”‚     ripgrep: rg (consistent across platforms)               â”‚
â”‚     Python: pathlib + re module                             â”‚
â”‚     Node: glob + regex                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ ASPIRATIONAL VS ACTUAL ALIGNMENT

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IF YOU DEFINE A PATTERN, USE IT EVERYWHERE                 â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN:                                                 â”‚
â”‚  âŒ Define helper but don't use it in examples:             â”‚
â”‚     "Use PYTHON env var for portability"                    â”‚
â”‚     ...later in same doc...                                 â”‚
â”‚     subprocess.run([".venv/bin/python", ...])  # Ignores!   â”‚
â”‚                                                             â”‚
â”‚  âŒ Document skip pattern but test doesn't use it:          â”‚
â”‚     "Skip gracefully if rg not installed"                   â”‚
â”‚     ...test code...                                         â”‚
â”‚     result = run(["rg", ...])  # Hard fails if missing!     â”‚
â”‚                                                             â”‚
â”‚  REQUIRED:                                                  â”‚
â”‚  âœ… Every pattern/helper defined MUST appear in all         â”‚
â”‚     relevant examples in the same document                  â”‚
â”‚  âœ… If you show "the right way", never show "the wrong way" â”‚
â”‚     in executable code blocks                               â”‚
â”‚  âœ… Audit: search for pattern violations in your own doc    â”‚
â”‚                                                             â”‚
â”‚  WHY: Readers copy-paste examples. If examples contradict   â”‚
â”‚  your guidelines, guidelines are effectively dead.          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Aspirational vs Actual Audit

Before finalizing task list:
```bash
# If you defined PYTHON helper, verify all subprocess calls use it
grep -n "PYTHON\s*=" PROJECT_TASKS.md  # Find definition
grep -n "\.venv/bin/python\|/usr/bin/python" PROJECT_TASKS.md
# Expected: No hard-coded paths after helper is defined

# If you defined skip pattern for tool X, verify tests use it
grep -n "pytest.skip.*X not" PROJECT_TASKS.md  # Find pattern
grep -n "subprocess.*\[\"X\"" PROJECT_TASKS.md  # Find usages
# Expected: All usages either have skip guard or are in "wrong" examples
```

---

## âš ï¸ PRE-RUN DEPENDENCIES IN TESTS

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TESTS THAT REQUIRE MANUAL PRE-RUN ARE FRAGILE              â”‚
â”‚                                                             â”‚
â”‚  PATTERN:                                                   â”‚
â”‚  test_output_file_valid():                                  â”‚
â”‚      if not file("output.json").exists():                   â”‚
â”‚          pytest.skip("Run tool first")  # FRAGILE!          â”‚
â”‚      ...                                                    â”‚
â”‚                                                             â”‚
â”‚  PROBLEMS:                                                  â”‚
â”‚  - On clean checkout, tests skip silently                   â”‚
â”‚  - CI might pass with 0 tests run                           â”‚
â”‚  - Pre-run step can be forgotten                            â”‚
â”‚                                                             â”‚
â”‚  SOLUTIONS:                                                 â”‚
â”‚                                                             â”‚
â”‚  Option A: Gate ensures pre-run before tests                â”‚
â”‚     Phase gate runs tool THEN tests (documented in order)   â”‚
â”‚                                                             â”‚
â”‚  Option B: Test runs the tool itself (integration test)     â”‚
â”‚     test runs tool â†’ checks output â†’ cleans up              â”‚
â”‚                                                             â”‚
â”‚  Option C: Fixture generates required files                 â”‚
â”‚     @pytest.fixture creates file, test uses it              â”‚
â”‚                                                             â”‚
â”‚  REQUIRED: Document which option you're using and why       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ VERSION CHANGELOG GOVERNANCE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHEMA/API CHANGES MUST UPDATE CHANGELOG                   â”‚
â”‚                                                             â”‚
â”‚  If your project has versioned schemas, models, or APIs:    â”‚
â”‚                                                             â”‚
â”‚  REQUIRED:                                                  â”‚
â”‚  âœ… CHANGELOG file exists (e.g., SCHEMA_CHANGELOG.md)       â”‚
â”‚  âœ… Every schema change adds changelog entry                â”‚
â”‚  âœ… Version numbers follow semantic versioning              â”‚
â”‚  âœ… CI can enforce: "if schema changed, changelog changed"  â”‚
â”‚                                                             â”‚
â”‚  CHANGELOG ENTRY FORMAT:                                    â”‚
â”‚  ## [version] - YYYY-MM-DD                                  â”‚
â”‚  ### Added / Changed / Deprecated / Removed / Fixed         â”‚
â”‚  - Description of change                                    â”‚
â”‚  - Migration notes if breaking                              â”‚
â”‚                                                             â”‚
â”‚  ENFORCEMENT (optional CI check):                           â”‚
â”‚  if git diff --name-only | grep "schema\|models"; then      â”‚
â”‚    git diff --name-only | grep "CHANGELOG" || exit 1        â”‚
â”‚  fi                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ PACKAGE MANAGER CONSISTENCY

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IF YOU DECLARE A PACKAGE MANAGER, USE IT EVERYWHERE        â”‚
â”‚                                                             â”‚
â”‚  PROBLEM:                                                   â”‚
â”‚  Project says: "We use uv for package management"           â”‚
â”‚  But examples show: .venv/bin/python tools/script.py        â”‚
â”‚                                                             â”‚
â”‚  RESULT: Partial integration                                â”‚
â”‚  - Tool used for: setup (uv add, uv sync)                   â”‚
â”‚  - Tool NOT used for: running tests, tools, commands        â”‚
â”‚  - Developer workflow ignores the declared tool             â”‚
â”‚                                                             â”‚
â”‚  REQUIRED:                                                  â”‚
â”‚  âœ… If package manager declared, ALL commands use it:       â”‚
â”‚     uv run pytest                                           â”‚
â”‚     uv run python tools/script.py                           â”‚
â”‚     NOT: .venv/bin/python tools/script.py                   â”‚
â”‚                                                             â”‚
â”‚  âœ… "Never call X directly" rule documented:                â”‚
â”‚     "Never call .venv/bin/python directly.                  â”‚
â”‚      Always use uv run python ..."                          â”‚
â”‚                                                             â”‚
â”‚  âœ… Examples in docs match the declared tool                â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN:                                                 â”‚
â”‚  âŒ Tool for setup, raw paths for execution                 â”‚
â”‚  âŒ CI uses tool, local dev uses raw paths                  â”‚
â”‚  âŒ "We use X" but examples show direct venv access         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Dependency Single Source of Truth

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DEPENDENCIES HAVE TWO SSOT FILES - KEEP THEM IN SYNC       â”‚
â”‚                                                             â”‚
â”‚  DECLARED DEPENDENCIES (what you want):                     â”‚
â”‚  - Python: pyproject.toml                                   â”‚
â”‚  - Node: package.json                                       â”‚
â”‚  - Rust: Cargo.toml                                         â”‚
â”‚  - Ruby: Gemfile                                            â”‚
â”‚                                                             â”‚
â”‚  LOCKED DEPENDENCIES (exact versions installed):            â”‚
â”‚  - Python (uv): uv.lock                                     â”‚
â”‚  - Python (poetry): poetry.lock                             â”‚
â”‚  - Node: package-lock.json / yarn.lock / pnpm-lock.yaml     â”‚
â”‚  - Rust: Cargo.lock                                         â”‚
â”‚  - Ruby: Gemfile.lock                                       â”‚
â”‚                                                             â”‚
â”‚  RULES:                                                     â”‚
â”‚  âœ… NEVER edit lockfile manually - let tool manage it       â”‚
â”‚  âœ… ALWAYS commit manifest AND lockfile together            â”‚
â”‚  âœ… After dep change: update manifest â†’ lock â†’ sync         â”‚
â”‚  âœ… CI installs from lockfile (reproducible builds)         â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN:                                                 â”‚
â”‚  âŒ Editing lockfile by hand                                â”‚
â”‚  âŒ Committing manifest without updated lockfile            â”‚
â”‚  âŒ CI that doesn't use lockfile                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Environment Ownership

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ONLY THE DECLARED TOOL CREATES/MANAGES THE ENVIRONMENT     â”‚
â”‚                                                             â”‚
â”‚  If project uses uv:                                        â”‚
â”‚  âœ… .venv/ created by: uv sync                              â”‚
â”‚  âŒ .venv/ created by: python -m venv, virtualenv, etc.     â”‚
â”‚                                                             â”‚
â”‚  RULES:                                                     â”‚
â”‚  - Do NOT manually create virtualenvs for the project       â”‚
â”‚  - Do NOT install packages via pip when using uv/poetry     â”‚
â”‚  - Do NOT edit .venv/ contents by hand                      â”‚
â”‚  - If .venv/ is corrupted: delete it, run tool sync         â”‚
â”‚                                                             â”‚
â”‚  EVEN WHEN ACTIVATED:                                       â”‚
â”‚  Even inside an activated virtualenv, prefer tool commands: â”‚
â”‚     uv run pytest           # YES - guarantees correct env  â”‚
â”‚     pytest                  # NO - might use wrong deps     â”‚
â”‚                                                             â”‚
â”‚  Direct invocation allowed ONLY for throwaway experiments   â”‚
â”‚  - never in committed scripts, tests, or docs               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Drift Labeling

When documenting package manager rules, explicitly label violations as **drift**:

```markdown
> **Hard rule:** Never call `.venv/bin/python` directly.
> Any `.venv/bin/python` usage in this repo is **drift** and should be removed.
```

This makes violations unambiguous and searchable:
```bash
# Find drift in the codebase
grep -rn "\.venv/bin/python" --include="*.py" --include="*.md" --include="*.yml"
# Expected: 0 results (or only in "don't do this" examples)
```

---

## ğŸšª OVERRIDE ESCAPE HATCH SCOPING

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ENV OVERRIDES THAT BYPASS MANDATORY TOOLS NEED SCOPE       â”‚
â”‚                                                             â”‚
â”‚  PROBLEM:                                                   â”‚
â”‚  Rule: "uv is mandatory for all Python commands"            â”‚
â”‚  But also: UV_RUN = os.environ.get("DOXSTRUX_UV_RUN", ...)  â”‚
â”‚                                                             â”‚
â”‚  Developer sets: export DOXSTRUX_UV_RUN="python"            â”‚
â”‚  Result: uv bypassed entirely, but tests still pass         â”‚
â”‚                                                             â”‚
â”‚  This is a DIRECT ESCAPE HATCH from your "mandatory" rule   â”‚
â”‚                                                             â”‚
â”‚  REQUIRED: Scope the override explicitly:                   â”‚
â”‚                                                             â”‚
â”‚  âœ… CI-ONLY override:                                       â”‚
â”‚     "DOXSTRUX_UV_RUN override is ONLY for CI images where   â”‚
â”‚      uv cannot be installed. Local use is drift."           â”‚
â”‚                                                             â”‚
â”‚  âœ… THROWAWAY-ONLY override:                                â”‚
â”‚     "Using DOXSTRUX_UV_RUN without 'uv run' is ONLY         â”‚
â”‚      acceptable in throwaway local experiments; never in    â”‚
â”‚      CI or committed scripts/tests."                        â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN:                                                 â”‚
â”‚  âŒ Override exists but no documentation of when it's valid â”‚
â”‚  âŒ "Mandatory" rule + undocumented bypass                   â”‚
â”‚  âŒ Override allows production/CI deviation silently        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š PHASED ENFORCEMENT HONESTY

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IF A RULE ISN'T ENFORCED UNTIL PHASE N, SAY SO EXPLICITLY  â”‚
â”‚                                                             â”‚
â”‚  PROBLEM:                                                   â”‚
â”‚  Narrative: "Clean Table = no debt carries forward"         â”‚
â”‚  Mechanics: Phase 0-2 only check specific files             â”‚
â”‚             Phase 3+ has repo-wide test                     â”‚
â”‚                                                             â”‚
â”‚  Result: TODOs can slip into src/ during Phase 0-2          â”‚
â”‚          Narrative claims universal, mechanics are phased   â”‚
â”‚                                                             â”‚
â”‚  REQUIRED: Document the gap honestly:                       â”‚
â”‚                                                             â”‚
â”‚  Option A: Make it truly universal                          â”‚
â”‚     Add repo-wide check to EVERY phase gate                 â”‚
â”‚                                                             â”‚
â”‚  Option B: Document the phased enforcement                  â”‚
â”‚     "Clean Table is strictly enforced from Phase 3 onward.  â”‚
â”‚      Earlier phases may accumulate limited debt in src/."   â”‚
â”‚                                                             â”‚
â”‚  TEMPLATE:                                                  â”‚
â”‚  ## Enforcement Timeline                                    â”‚
â”‚  | Rule           | Phase 0-2      | Phase 3+        |     â”‚
â”‚  |----------------|----------------|-----------------|     â”‚
â”‚  | Clean Table    | Per-task files | Repo-wide       |     â”‚
â”‚  | Schema strict  | Not enforced   | extra="forbid"  |     â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN:                                                 â”‚
â”‚  âŒ Universal-sounding rule with phased-only enforcement    â”‚
â”‚  âŒ "Clean Table" narrative but grep only checks 2 files    â”‚
â”‚  âŒ Readers assume universal when mechanics are limited     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Package Manager Examples (Language-Agnostic)

| Language | Declared Tool | Wrong | Right |
|----------|---------------|-------|-------|
| Python (uv) | uv | `.venv/bin/python script.py` | `uv run python script.py` |
| Python (poetry) | poetry | `.venv/bin/pytest` | `poetry run pytest` |
| Node (npm) | npm | `node script.js` | `npm run script` |
| Node (pnpm) | pnpm | `./node_modules/.bin/cmd` | `pnpm exec cmd` |
| Ruby | bundler | `ruby script.rb` | `bundle exec ruby script.rb` |
| Rust | cargo | `./target/debug/tool` | `cargo run --bin tool` |

---

## ğŸ”„ CI/LOCAL WORKFLOW IDENTITY

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LOCAL COMMANDS SHOULD MATCH CI COMMANDS EXACTLY            â”‚
â”‚                                                             â”‚
â”‚  PROBLEM:                                                   â”‚
â”‚  CI workflow:     uv run pytest tests/                      â”‚
â”‚  Local docs:      .venv/bin/python -m pytest tests/         â”‚
â”‚                                                             â”‚
â”‚  RESULT:                                                    â”‚
â”‚  - "Works on my machine" but fails in CI (or vice versa)    â”‚
â”‚  - Debugging CI requires translating commands               â”‚
â”‚  - Two mental models for same action                        â”‚
â”‚                                                             â”‚
â”‚  REQUIRED:                                                  â”‚
â”‚  âœ… Same command works locally AND in CI                    â”‚
â”‚  âœ… Documentation shows the CI-identical command            â”‚
â”‚  âœ… Phase gates use CI-identical commands                   â”‚
â”‚  âœ… Test snippets use CI-identical invocation               â”‚
â”‚                                                             â”‚
â”‚  PATTERN:                                                   â”‚
â”‚  CI:    uv run pytest tests/                                â”‚
â”‚  Local: uv run pytest tests/   # IDENTICAL                  â”‚
â”‚  Docs:  "Run tests: uv run pytest tests/"                   â”‚
â”‚  Gate:  uv run pytest tests/ && echo "PASS"                 â”‚
â”‚                                                             â”‚
â”‚  BENEFITS:                                                  â”‚
â”‚  - Copy-paste from CI to local (and vice versa)             â”‚
â”‚  - No translation needed when debugging                     â”‚
â”‚  - Single source of truth for "how to run X"                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CI/Local Alignment Checklist

Before finalizing task list:
```bash
# Extract all commands from CI workflow
grep -E "run:|uv |pytest|python" .github/workflows/*.yml

# Extract all commands from task list/docs  
grep -E "\.venv/|python |pytest" PROJECT_TASKS.md

# These should use the same invocation pattern
# If CI uses "uv run pytest", docs should NOT use ".venv/bin/pytest"
```

---

## ğŸ”€ SKIP PATTERN CONSISTENCY

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IF YOU DEFINE A SKIP PATTERN, USE IT EVERYWHERE            â”‚
â”‚                                                             â”‚
â”‚  PROBLEM:                                                   â”‚
â”‚  Guidelines say:                                            â”‚
â”‚     "Skip gracefully if rg not installed"                   â”‚
â”‚     if not shutil.which("rg"):                              â”‚
â”‚         pytest.skip("ripgrep not installed")                â”‚
â”‚                                                             â”‚
â”‚  But actual test does:                                      â”‚
â”‚     result = subprocess.run(["rg", ...])                    â”‚
â”‚     assert result.returncode in (0, 1)  # Hard fails if 127!â”‚
â”‚                                                             â”‚
â”‚  RESULT: Half-committed. Either:                            â”‚
â”‚  - Tool is REQUIRED (hard fail, no skip pattern needed)     â”‚
â”‚  - Tool is OPTIONAL (skip pattern used consistently)        â”‚
â”‚                                                             â”‚
â”‚  REQUIRED: Make explicit decision and apply consistently:   â”‚
â”‚                                                             â”‚
â”‚  Option A: REQUIRED dependency                              â”‚
â”‚  - Document in Prerequisites                                â”‚
â”‚  - CI installs it                                           â”‚
â”‚  - Tests hard-fail if missing (no skip)                     â”‚
â”‚  - Remove any skip suggestions from guidelines              â”‚
â”‚                                                             â”‚
â”‚  Option B: OPTIONAL dependency                              â”‚
â”‚  - Document in Prerequisites with install instructions      â”‚
â”‚  - ALL tests that use it have skip guard                    â”‚
â”‚  - Guidelines and tests are consistent                      â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN:                                                 â”‚
â”‚  âŒ Skip pattern in guidelines, hard-fail in tests          â”‚
â”‚  âŒ Some tests skip, other tests hard-fail for same tool    â”‚
â”‚  âŒ "Should skip if missing" without actual skip code       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Required vs Optional Dependency Documentation

When documenting dependencies, be explicit:

```
## Prerequisites

### REQUIRED (CI and local must have)
- Python 3.10+
- uv package manager

### REQUIRED FOR CI, OPTIONAL LOCAL (tests skip if missing)
- ripgrep (rg): Tests skip locally, but CI must have it
  - Local: `brew install ripgrep` or tests will skip
  - CI: Pre-installed on GitHub runners

### FULLY OPTIONAL (tests skip if missing)
- graphviz: Only needed for diagram generation
```

**Anti-pattern**: Saying "REQUIRED" but tests skip when missing. This creates:
- False sense of enforcement locally
- Different behavior between CI and dev machines
- Confusion about actual contract

---

## ğŸ“„ CROSS-DOCUMENT SSOT

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WHEN SPEC AND TASK LIST COEXIST, PREVENT DRIFT             â”‚
â”‚                                                             â”‚
â”‚  COMMON SITUATION:                                          â”‚
â”‚  - SPEC.md: Design document with rich detail                â”‚
â”‚  - TASK_LIST.md: Executable plan with subset of spec        â”‚
â”‚                                                             â”‚
â”‚  RISK:                                                      â”‚
â”‚  - Spec describes tests X, Y, Z                             â”‚
â”‚  - Task list only implements X, Y (Z out of scope)          â”‚
â”‚  - Future reader of spec expects Z to exist                 â”‚
â”‚  - Soft double-SSOT: two documents, different "truth"       â”‚
â”‚                                                             â”‚
â”‚  REQUIRED:                                                  â”‚
â”‚                                                             â”‚
â”‚  âœ… Mark spec as HISTORICAL once task list exists:          â”‚
â”‚     "For active work, see TASK_LIST.md. This document       â”‚
â”‚      is design history only."                               â”‚
â”‚                                                             â”‚
â”‚  âœ… Or: Task list references spec with explicit scope:      â”‚
â”‚     "Implements sections 1-3 of SPEC.md. Sections 4-5       â”‚
â”‚      are OUT OF SCOPE for this refactor."                   â”‚
â”‚                                                             â”‚
â”‚  âœ… If spec is still active, sync changes both ways:        â”‚
â”‚     Task list change â†’ update spec (or vice versa)          â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN:                                                 â”‚
â”‚  âŒ Two documents with overlapping scope, no cross-ref      â”‚
â”‚  âŒ Spec promises tests that task list doesn't implement    â”‚
â”‚  âŒ "The spec says X but we're doing Y" without updating    â”‚
â”‚     either document                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Cross-Document Reference Pattern

At top of task list:
```markdown
## Document Relationship

| Document | Status | Role |
|----------|--------|------|
| SPEC.md | HISTORICAL | Design rationale, not executable |
| TASK_LIST.md | ACTIVE | Executable plan, SSOT for this work |
| output_models.py | ACTIVE | SSOT for schema once created |

For discrepancies: TASK_LIST.md wins over SPEC.md.
After completion: output_models.py wins over both.
```

At top of spec (once task list exists):
```markdown
## âš ï¸ HISTORICAL DOCUMENT

This document is design history. For active implementation:
- See: TASK_LIST.md (executable plan)
- See: output_models.py (schema SSOT once created)

Do NOT use this document as source of truth for current work.
```

---

## Prerequisites (Verify Before Starting)

**ALL must pass before Phase 0 begins:**

- [ ] **Environment ready**: [verification command]
- [ ] **Dependencies installed**: [verification command]
- [ ] **Tests run successfully**: [verification command]
- [ ] **No blocking issues**: [verification command]
- [ ] **Required tools available**: [list tools with `which` checks]

**Quick Check**:
```bash
# Single command to verify all prerequisites
[[PH:PREREQ_CHECK_COMMAND]]

# Tool availability (customize per project/language)
# Python:  which python && which pytest || echo "Missing"
# Node:    which node && which npm || echo "Missing"
# Rust:    which cargo || echo "Missing"
# Go:      which go || echo "Missing"
```

---

# Phase 0: Setup & Infrastructure

**Goal**: Establish testing infrastructure and baseline
**Time Estimate**: [[PH:TIME_ESTIMATE]]
**Status**: â³ NOT STARTED

---

## Task 0.1: [[PH:TASK_NAME]]

**Objective**: [[PH:ONE_SENTENCE_OBJECTIVE]]

### Canonical File List (SINGLE SOURCE)

```bash
# This array is used for BOTH the file manifest AND verification
TASK_0_1_FILES=(
    "[[PH:FILE_1]]"
    "[[PH:FILE_2]]"
)
```

**Files**: `${TASK_0_1_FILES[@]}`

### TDD Step 1: Write Test First

```python
# tests/test_[[PH:FEATURE_NAME]].py
# NOTE: All examples use `uv run`. Adapt runner for your stack.

def test_[[PH:FEATURE_NAME]]():
    """[[PH:WHAT_THIS_TEST_VERIFIES]]"""
    # Arrange
    [[PH:SETUP_CODE]]
    
    # Act
    result = [[PH:FUNCTION_CALL]]
    
    # Assert - MUST be specific, not "it runs"
    assert result == [[PH:EXPECTED_SPECIFIC_VALUE]]
    assert len(result.items) >= [[PH:MINIMUM_EXPECTED]]
    # Add assertions that would FAIL with stub implementation
```

```bash
# Verify test fails (RED)
uv run pytest tests/test_[[PH:FEATURE_NAME]].py -v
# Expected: FAIL (test exists but implementation missing)
```

### â›” Test Strength Self-Check

Before proceeding to implementation:
- [ ] Would this test pass with empty/null return? â†’ If yes, strengthen it
- [ ] Does test verify specific output content? â†’ Must be yes
- [ ] Does test have minimum count/value assertions? â†’ Must be yes

### TDD Step 2: Implement

```python
# [[PH:IMPLEMENTATION_FILE]]

def [[PH:FUNCTION_NAME]]():
    """[[PH:DOCSTRING]]"""
    [[PH:IMPLEMENTATION]]
```

### TDD Step 3: Verify (GREEN)

```bash
# Run the specific test
uv run pytest tests/test_[[PH:FEATURE_NAME]].py -v
# Expected: PASS

# Run full suite (no regressions)
uv run pytest -q
# Expected: N/N PASS
```

### â›” STOP: Clean Table Check

Before marking this task complete, verify:

**No Weak Tests Gate** (all must be YES):
- [ ] Would a stub/no-op implementation FAIL this test? â†’ Must be YES
- [ ] Does test assert semantics (structure/content), not just presence? â†’ Must be YES
- [ ] Is there at least one negative case for critical behavior? â†’ Must be YES
- [ ] Is the test NOT any of: import-only, smoke, existence-only, line-count, exit-code-only? â†’ Must be YES

**Clean Table Gate**:
- [ ] Test passes (not skipped)
- [ ] Full test suite passes: `uv run pytest -q` â†’ N/N PASS
- [ ] No TODOs or placeholders in new code
- [ ] No new warnings introduced
- [ ] Code is documented
- [ ] **All files in "Files:" header exist**: `test -f [each_file]`

**Status**: â³ NOT STARTED

---

## Task 0.2: [Next Task]

[Same structure as Task 0.1]

---

## â›” STOP: Phase 0 Gate

**Before starting Phase 1, ALL must be true:**

```bash
# 1. All tests pass
[FULL_TEST_COMMAND]
# Expected: N/N PASS

# 2. No regressions
[REGRESSION_CHECK]
# Expected: 0 failures

# 3. Clean Table verified
grep -r "TODO\|FIXME\|XXX" src/
# Expected: No results (or only pre-existing)

# 4. File manifest verified
for f in [ALL_PHASE_0_FILES]; do test -f "$f" || echo "MISSING: $f"; done
# Expected: No output (all files exist)
```

### Phase 0 Completion Checklist

- [ ] All Task 0.x items have âœ… status
- [ ] Full test suite passes
- [ ] **All tests are strong** (not "it runs" tests)
- [ ] No new TODOs introduced
- [ ] Infrastructure documented
- [ ] **All listed files exist**
- [ ] Git checkpoint created

### Create Phase Unlock Artifact

```bash
# Only run this after ALL above criteria pass
# NOTE: Replace placeholders with ACTUAL values before running

PHASE_NUM=0
PHASE_NAME="Setup & Infrastructure"
# Get test count (adapt command to your test framework)
# Python/pytest: pytest --collect-only -q 2>/dev/null | tail -1 | grep -oP '\d+(?= test)'
# Node/jest:     npm test -- --listTests 2>/dev/null | wc -l
# Go:            go test -list '.*' ./... 2>/dev/null | wc -l
TESTS_PASSED=[YOUR_TEST_COUNT_COMMAND]
GIT_COMMIT=$(git rev-parse --short HEAD)
COMPLETION_DATE=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

cat > .phase-${PHASE_NUM}.complete.json << EOF
{
  "schema_version": "1.0",
  "phase": ${PHASE_NUM},
  "phase_name": "${PHASE_NAME}",
  "tests_passed": ${TESTS_PASSED:-0},
  "tests_total": ${TESTS_PASSED:-0},
  "clean_table": true,
  "git_commit": "${GIT_COMMIT}",
  "completion_date": "${COMPLETION_DATE}"
}
EOF

# Verify no placeholders remain
grep -E "YYYY|placeholder|\[.*\]" .phase-${PHASE_NUM}.complete.json && \
    echo "âŒ ERROR: Placeholders found in artifact" && exit 1

git add .phase-${PHASE_NUM}.complete.json
git commit -m "Phase ${PHASE_NUM} complete: ${PHASE_NAME}"
git tag phase-${PHASE_NUM}-complete
```

**Phase 0 Status**: â³ NOT STARTED

---

# Phase 1: [Phase Name]

**Goal**: [One sentence]
**Time Estimate**: [X-Y hours]
**Prerequisite**: `.phase-0.complete.json` must exist
**Status**: ğŸ“‹ PLANNED

---

## â›” STOP: Verify Phase 0 Complete

```bash
# This MUST pass before starting Phase 1
test -f .phase-0.complete.json && echo "âœ… Phase 0 complete" || echo "âŒ BLOCKED"
```

---

## Task 1.1: [Task Name]

**Objective**: [One sentence]
**Files**: `[files]`

### TDD Step 1: Write Test First

[Test code with verification command]

**â›” Test Strength Self-Check** (mandatory):
- [ ] Test fails with stub implementation
- [ ] Test verifies specific output values
- [ ] Test has boundary/count assertions

### TDD Step 2: Implement

[Implementation guidance]

### TDD Step 3: Verify

```bash
[TEST_COMMAND]
# Expected: PASS
```

### â›” STOP: Clean Table Check

- [ ] Test passes
- [ ] **Test is strong**
- [ ] Full suite passes
- [ ] No new debt
- [ ] **All "Files:" exist**

**Status**: ğŸ“‹ PLANNED

---

## Task 1.2: [Next Task]

[Same structure]

---

## â›” STOP: Phase 1 Gate

[Same structure as Phase 0 Gate, including file manifest verification]

---

# Appendix A: Rollback Procedures

## A.1: Single Test Failure

```bash
# 1. Identify failing test
[TEST_COMMAND] 2>&1 | head -50

# 2. If fixable in <15 min, fix it
# 3. If not fixable, revert last change
git diff HEAD~1 --stat
git checkout HEAD~1 -- [affected_file]

# 4. Verify tests pass again
[FULL_TEST_COMMAND]
```

## A.2: Multiple Test Failures (Full Revert)

```bash
# 1. Identify last known good state
git log --oneline -10

# 2. Hard reset to good commit
git reset --hard [GOOD_COMMIT]

# 3. Verify
[FULL_TEST_COMMAND]

# 4. Document what went wrong
echo "[DATE]: Reverted due to [REASON]" >> ISSUES.md
```

## A.3: Phase Gate Failure

```bash
# DO NOT proceed to next phase
# 1. Identify which criterion failed
# 2. Fix the specific issue
# 3. Re-run phase gate verification
# 4. Only proceed when ALL criteria pass
```

## A.4: Weak Test Discovered

```bash
# If a test is discovered to be an "it runs" test:
# 1. DO NOT proceed with current task
# 2. Strengthen the test first
# 3. Verify strengthened test fails with stub
# 4. Only then continue implementation
```

---

# Appendix B: AI Assistant Instructions

## Drift Prevention Rules

1. **Before each response**, re-read the current task's objective
2. **After completing code**, immediately run verification commands
3. **If test fails**, fix it before moving on (no "we'll fix it later")
4. **If blocked**, document why and suggest rollback
5. **Never skip** Clean Table checks
6. **Never write** "it runs" tests (see Test Strength Rules)
7. **Never list** files that won't be created
8. **Never test** features that don't exist yet
9. **Never assume** code matches design docsâ€”verify first
10. **Never tighten** constraints until loose version passes

## Authority Resolution

When design doc says X but code does Y:
1. **Verify** code behavior is intentional (not a bug)
2. **If intentional** â†’ Update design doc and task list to match code
3. **If bug** â†’ Fix code, but through proper TDD cycle
4. **Never** rename/restructure working code just to match docs

## Verification Frequency

| Action | Verify Immediately |
|--------|-------------------|
| Create new file | File exists, syntax valid |
| Modify function | Related tests pass |
| Write test | **Test fails with stub** |
| Reference symbol from spec | **Symbol exists in code** |
| Complete task | Full test suite + file manifest |
| Complete phase | Phase gate checklist |
| Tighten constraints | **Loose version passes first** |

## When to Stop and Escalate

- Test suite drops below 100% pass rate
- Performance degrades beyond threshold
- Clean Table criteria cannot be satisfied
- Phase gate blocked for >2 attempts
- **Test discovered to be "it runs" pattern**
- **File listed but not created**
- **Spec and code disagree on symbol names**
- **Tightening constraints breaks previously passing tests**
- **Feature referenced in spec doesn't exist in code**

## Prohibited Actions

- âŒ Starting Phase N+1 without Phase N artifact
- âŒ Marking task complete with failing tests
- âŒ Leaving TODOs in "completed" code
- âŒ Skipping Clean Table verification
- âŒ Proceeding after rollback without re-verification
- âŒ **Writing tests that only check exit code**
- âŒ **Writing tests that pass with stub implementation**
- âŒ **Listing files in "Files:" that aren't created**
- âŒ **Leaving literal placeholders in artifacts**
- âŒ **Using hard-coded interpreter paths** (use runtime self-reference)
- âŒ **Testing features marked "reserved" or "planned"**
- âŒ **Assuming symbol names without grep verification**
- âŒ **Changing working code to match design docs**
- âŒ **Tightening constraints before loose version passes**
- âŒ **Defining models without discovery phase**
- âŒ **Treating "final state" diagrams as implementation specs**

**YAGNI Violations (Code Quality)**:
- âŒ **Adding features without current requirement** (YAGNI Q1)
- âŒ **Building for hypothetical future consumers** (YAGNI Q2)
- âŒ **Speculative flags, hooks, or parameters** (unused API surface)
- âŒ **Premature abstractions** (interface with only 1 implementation)
- âŒ **"Might need later" code** (if addable later, defer)
- âŒ **Generalizations without â‰¥2 real consumers today**
- âŒ **Complex solution when simpler one works** (KISS violation)

**Silent Error Violations**:
- âŒ **Swallowed exceptions** (catch and continue without raise)
- âŒ **Conditional error handling** (if strict: raise)
- âŒ **Strict mode parameters** (strict: bool = True)
- âŒ **Environment variables for strictness** (STRICT_MODE env var)
- âŒ **Warning instead of error** (print warning and continue)
- âŒ **Error collection without raise** (errors.append() then continue)

**Hidden Debt Violations**:
- âŒ **"Temporary" fixes** that mask root causes
- âŒ **"Adjust later" comments** without immediate resolution
- âŒ **"You may need to" speculative comments**
- âŒ **Workaround logic** instead of proper fix
- âŒ **Deferred follow-ups** in completed code

**Reflection Violations**:
- âŒ **Skipping reflection** before task completion
- âŒ **Proceeding with uncertainty** (confidence < 80%)
- âŒ **Unstated assumptions** (assuming without documenting)
- âŒ **Guessing when ambiguous** (should STOP and clarify)
- âŒ **Mixing thinking and execution** (must separate reasoning from action)
- âŒ **Claims without evidence anchors** (no source:line + quote)
- âŒ **Assumptions treated as facts** (must distinguish semantic types)
- âŒ **Undocumented decisions** (must have decision chain)
- âŒ **Missing trace links** (decision â†’ evidence â†’ impact)
- âŒ **Unverified assumptions proceeding** (must verify or flag as risk)

**Governance Violations**:
- âŒ **Duplicate task IDs** (same ID for different tasks)
- âŒ **Untracked tasks in Appendix** (all IDs must appear in file tracking)
- âŒ **Selective Clean Table checks** (must be global, entire repo)
- âŒ **Aspirational helpers not used** (defined pattern ignored in examples)
- âŒ **Existence-only tests** (must check content correctness, not just presence)
- âŒ **Unhandled pre-run dependencies** (tests that skip without explicit strategy)
- âŒ **Missing scope decisions** (must declare what's in/out of scope)
- âŒ **Exit code tolerance** (`returncode in (0, 1)` accepts both success and failure)
- âŒ **Plausible output tests** (`"exported" in stdout` doesn't verify behavior)
- âŒ **Heredoc placeholders** (literal `YYYY-MM-DD` committed to artifacts)
- âŒ **Inconsistent skip patterns** (guidelines say skip, tests hard-fail)
- âŒ **CI string-only tests** (checking YAML contains string, not correct wiring)
- âŒ **Schema changes without changelog** (versioned artifacts need history)
- âŒ **No-op stub tools** (tool passes tests but never processes real data)
- âŒ **Phase gate scope mismatch** (global rules enforced locally in early phases)
- âŒ **Temporal debt window** (create placeholder â†’ edit â†’ forget to re-test)
- âŒ **Undocumented weak tests** (intentionally limited tests without TRADE-OFF comment)
- âŒ **Cross-document drift** (spec and task list overlap without cross-reference)
- âŒ **"Required" but skips** (dependency called REQUIRED but tests skip when missing)
- âŒ **Package manager partial use** ("We use X" but examples show direct venv access)
- âŒ **CI/Local command mismatch** (CI uses tool runner, local uses raw paths)
- âŒ **Manual lockfile edits** (editing uv.lock/package-lock.json by hand)
- âŒ **Orphaned manifest commits** (manifest committed without updated lockfile)
- âŒ **Rogue environment creation** (pip/virtualenv when project uses uv/poetry)
- âŒ **Activated shell shortcuts** (direct python call instead of tool run)
- âŒ **Ambiguous placeholder patterns** (`[.*]` matches normal markdown)
- âŒ **Shell syntax errors** (`VAR=[cmd]` instead of `VAR="$(cmd)"`)
- âŒ **Platform-specific grep** (GNU-only patterns without documentation)
- âŒ **Procedural deadlocks** (conflicting scope rules with no escape path)
- âŒ **Untracked scope deltas** (exclusions not in ledger with consequences)
- âŒ **Dual-purpose tests** (same test as milestone-driver AND branch-guard)
- âŒ **Evidence theater** (unbounded evidence requirements on trivial claims)
- âŒ **Unlogged drift repairs** (SSOT mismatches fixed but not recorded)
- âŒ **Greater-than count tests** (`total >= N` instead of `total == N`)
- âŒ **Malformed-only invalid fixtures** (bad JSON doesn't test schema validation)
- âŒ **Undocumented override scope** (env bypass exists but valid use not documented)
- âŒ **Universal-sounding phased rules** (sounds global, only enforced from Phase N)
- âŒ **Structural type assumptions** (dict access that breaks on typed refactor)
- âŒ **Policy-as-invariant tests** (current behavior encoded as permanent contract)
- âŒ **Non-breaking evolution tests** (tests don't force updates when code evolves)
- âŒ **False local enforcement claims** (claiming discipline when relying on CI)
- âŒ **Bracket placeholder format** (`[X]` instead of `[[PH:X]]`)
- âŒ **Missing baseline snapshot** (no captured git/runtime state before work)
- âŒ **Dual file lists** (header list differs from verification list)
- âŒ **Baseline from memory** (snapshot not from executed commands)
- âŒ **Test strength advisory-only** (no explicit gate checklist)
- âŒ **No derived governance** (rules without source documents)

## Discovery-First Checklist

Before defining models/schemas/types for existing code:

- [ ] Ran existing code with â‰¥10 representative inputs
- [ ] Captured actual outputs to sample_outputs/
- [ ] Extracted all unique keys/fields
- [ ] Derived model structure from actual outputs
- [ ] Validated model against captured outputs
- [ ] Only then: wrote tests asserting specific structures

## Strictness Transition Checklist

Before tightening any constraint (validation, types, checks):

- [ ] Loose/permissive version implemented
- [ ] Loose version passes on â‰¥N real-world inputs
- [ ] Tightened constraint applied
- [ ] Same inputs still pass (if not â†’ fix model, not inputs)
- [ ] No fake data synthesized to satisfy strict checks

## YAGNI Checklist (Before Adding ANY New Code)

For every new function, class, parameter, or abstraction:

- [ ] **Q1**: Current requirement exists (ticket/issue ID)?
- [ ] **Q2**: Will be used immediately by known consumer?
- [ ] **Q3**: Backed by stakeholder request or data (not speculation)?
- [ ] **Q4**: Cannot be added later without massive rework?
- [ ] **KISS**: Simpler alternative considered and rejected?
- [ ] **SRP**: Single clear purpose for this code?
- [ ] No unused parameters, hooks, or flags added?
- [ ] Generalizations have â‰¥2 real consumers today?

**If any Q1-Q3 is NO â†’ Do not implement**
**If Q4 is YES â†’ Defer until actually needed**

---

# Appendix C: File Change Tracking

Track all file changes for audit trail:

| Phase.Task | File | Action | Verified |
|------------|------|--------|----------|
| 0.1 | `src/module.[ext]` | CREATE | â³ |
| 0.1 | `tests/test_module.[ext]` | CREATE | â³ |
| 1.1 | `src/module.[ext]` | UPDATE | ğŸ“‹ |

**File Manifest Audit**:
```bash
# After each phase, verify all tracked files exist
cat << 'EOF' | while read line; do
    file=$(echo "$line" | awk '{print $3}')
    test -f "$file" && echo "âœ… $file" || echo "âŒ $file"
done
[PASTE_TABLE_HERE]
EOF
```

---

# Appendix D: Progress Log

## Session Log

```
[YYYY-MM-DD HH:MM] Started Phase 0
[YYYY-MM-DD HH:MM] Task 0.1 complete - tests: 5/5 PASS (strong tests verified)
[YYYY-MM-DD HH:MM] Task 0.2 complete - tests: 8/8 PASS
[YYYY-MM-DD HH:MM] Phase 0 Gate: âœ… ALL PASS
[YYYY-MM-DD HH:MM] File manifest: âœ… ALL FILES EXIST
[YYYY-MM-DD HH:MM] Created .phase-0.complete.json (no placeholders)
```

## Time Tracking

| Phase | Task | Estimated | Actual | Notes |
|-------|------|-----------|--------|-------|
| 0 | 0.1 | 1h | - | - |
| 0 | 0.2 | 2h | - | - |

---

# Appendix E: Test Strength Audit

Run this audit after writing tests to catch weak patterns:

```bash
# Find potential "it runs" tests (adapt patterns to your language)
# Look for exit code checks without content verification
grep -rn "exitCode == 0\|returncode == 0\|exit_code == 0" tests/
grep -rn "exitCode in\|returncode in" tests/

# Find assertions that only check presence, not content
grep -rn "assert.*exists\|expect.*exist\|toBeDefined" tests/

# Find empty or near-empty test functions
# Python:
grep -l "def test_" tests/ | xargs grep -A10 "def test_" | grep -B5 "^$"
# JavaScript:
grep -l "it(" tests/ | xargs grep -A10 "it(" | grep -B5 "^$"

# Manual review checklist:
# - Does each test have specific value assertions?
# - Would a stub implementation pass this test?
# - Are both success and failure paths tested?
```

If any weak patterns found, strengthen before proceeding.

---

## 9. Drift + Issues Ledger (REQUIRED)

Maintain as an append-only table. Every SSOT mismatch must be logged here.

| Date | Higher SSOT | Lower SSOT | Mismatch | Resolution | Evidence |
|------|-------------|------------|----------|------------|----------|
| [[PH:DATE]] | tests | task list | Example: test expects X, task says Y | Updated task list | `uv run pytest -v` output |
| [[PH:DATE]] | code | spec | Example: function returns list, spec says dict | Updated spec | `grep` output |

**Rules**:
- Append only (do not edit/delete old entries)
- Higher SSOT always wins
- Resolution must include what was changed
- Evidence must be concrete (command output, file path)

---

## 10. Phase Unlock Artifacts

For each completed phase N, create `.phase-N.complete.json`:

```json
{
  "phase": 0,
  "completed_at": "2025-12-14T12:00:00Z",
  "commit": "abc123def456...",
  "test_command": "uv run pytest -q",
  "test_result": "42 passed in 3.21s",
  "clean_table": "PASS"
}
```

**Timestamp must be real** (use `date -u +%Y-%m-%dT%H:%M:%SZ`), not placeholder.

---

# Template Instantiation Checklist

Before using this template, replace all `[[PH:...]]` placeholders:

**Project-level**:
- [ ] `[[PH:PROJECT_NAME]]` â†’ Your project name
- [ ] `[[PH:ONE_LINE_DESCRIPTION]]` â†’ Brief description
- [ ] `[[PH:DATE_YYYY_MM_DD]]` â†’ Today's date

**Baseline snapshot** (from executed commands):
- [ ] `[[PH:REPO_NAME]]` â†’ Repository name
- [ ] `[[PH:GIT_BRANCH]]` â†’ Current branch
- [ ] `[[PH:GIT_COMMIT_HASH]]` â†’ Current commit
- [ ] `[[PH:PASTE_*]]` â†’ Actual command outputs

**Test commands** (hard-code for your stack):
- Python: `uv run pytest -q` / `uv run pytest -v`
- Node: `npm test -- --bail` / `npm test`
- Rust: `cargo test --quiet` / `cargo test`
- Go: `go test ./... -short` / `go test ./... -v`

**Final verification** (must return zero matches):
```bash
grep -RInE '\[\[PH:[A-Z0-9_]+\]\]' PROJECT_TASKS.md && \
  { echo "ERROR: Placeholders remain"; exit 1; } || \
  echo "OK: No placeholders"
```

---

**End of Template**
