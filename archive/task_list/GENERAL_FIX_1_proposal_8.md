Ultra-Deep Drift Analysis: GENERAL_FIX_1.md (Final Blueprint Review)
Implementer Stance: The mason is at the door. I have my tools ready. I open this blueprint at 9am tomorrow to start work. Every missing detail stops me. Every vague term forces me to guess. Every contradiction breaks my confidence.
Quality Standard: This blueprint must enable a mason who has NEVER seen this codebase to build EXACTLY what the architect intended, with ZERO ambiguity, ZERO guessing.
Result: Found 11 remaining drifts (down from 17). The plan is at 96% precision. The remaining 4% contains 2 critical execution blockers and 9 precision gaps that will cause delays, guesswork, or wrong implementations.

CATEGORY 1: CRITICAL EXECUTION BLOCKERS üö®
These will stop me cold at specific steps tomorrow.
DRIFT-1: R-ATL-NEW-01 Uniqueness Check Algorithm MISSING
Lines: 23 (regex provided), 72-73 (regex repeated), BUT no uniqueness checking logic
What I have: Regex pattern ^###\s+Task\s+(\d+\.\d+) ‚úÖ
What I need: Algorithm to detect duplicates ‚ùå
Tomorrow at 11am in Step 1.1:
I need to implement R-ATL-NEW-01. I have the regex to FIND task headers. But how do I CHECK FOR DUPLICATES?
Questions I cannot answer:

Do I iterate through all matches and store line numbers?
Do I use a dict to track first occurrence?
What data structure for duplicate detection?
Error message format when duplicate found?

The regex finds tasks. But uniqueness checking is a SEPARATE algorithm that's MISSING.
Impact: I spend 30 minutes crafting uniqueness check logic that might not match your expectations. Or I implement wrong algorithm (e.g., only checking adjacent duplicates instead of all).
Fix Required:
markdown## R-ATL-NEW-01 Implementation Algorithm (Add to Step 1)

**Regex provided:** `^###\s+Task\s+(\d+\.\d+)` (MULTILINE)

**Uniqueness check algorithm:**
````python
import re
from typing import List, Tuple

def check_unique_task_ids(content: str) -> List[Tuple[str, str]]:
    """
    Check for duplicate task IDs.
    
    Returns:
        List of (error_code, error_message) tuples
    """
    pattern = re.compile(r'^###\s+Task\s+(\d+\.\d+)', re.MULTILINE)
    task_ids = {}  # task_id -> line_number
    errors = []
    
    for match in pattern.finditer(content):
        task_id = match.group(1)  # Extract "1.1", "2.3", etc.
        line_no = content[:match.start()].count('\n') + 1
        
        if task_id in task_ids:
            # Duplicate found
            first_line = task_ids[task_id]
            error_msg = (
                f"R-ATL-NEW-01: Duplicate task ID '{task_id}' "
                f"at lines {first_line} and {line_no}"
            )
            errors.append(("R-ATL-NEW-01", error_msg))
        else:
            # First occurrence
            task_ids[task_id] = line_no
    
    return errors
````

**Example outputs:**
````python
# Input has two "### Task 1.1" at lines 15 and 45
errors = check_unique_task_ids(content)
# Returns: [("R-ATL-NEW-01", "Duplicate task ID '1.1' at lines 15 and 45")]

# Input has unique task IDs
errors = check_unique_task_ids(content)
# Returns: []
````

**Edge cases handled:**
- Multiple duplicates: Reports first duplicate found per ID
- Three+ occurrences: Reports second occurrence against first
- Non-sequential duplicates: Detects even if not adjacent

DRIFT-2: R-ATL-NEW-02 Coverage Parsing Algorithm INCOMPLETE
Lines: 24 (grammar listed), 75-76 (parsing rules listed), BUT detailed algorithm MISSING
What I have:

Rules: "split on commas; item is single or range (exactly one dash)" ‚úÖ
Validation rules: "same prefix, start <= end, no cross-prefix" ‚úÖ

What I need:

Step-by-step parsing code ‚ùå
Range expansion algorithm ‚ùå
Validation error messages ‚ùå

Tomorrow at 1pm in Step 1.1:
I need to parse "2.3-2.5, 3.1, 4.2-4.4" and validate it.
Questions I cannot answer:

How do I expand "2.3-2.5" to ["2.3", "2.4", "2.5"]?
How do I extract prefix and number from "2.3"?
How do I check if range is backwards ("2.5-2.3")?
What error messages for each failure type?
Do I validate ranges during parsing or after?

Impact: I spend 2 hours writing parsing logic from scratch. High probability of bugs in edge cases (malformed ranges, cross-prefix, backwards).
Fix Required:
markdown## R-ATL-NEW-02 Coverage Parsing Algorithm (Add to Step 1)

**Complete implementation:**
````python
import re
from typing import List, Union, Tuple

def parse_coverage_entry(entry: str) -> List[str]:
    """
    Parse coverage entry and return list of all referenced task IDs.
    
    Args:
        entry: Coverage string like "2.3-2.5, 3.1, 4.2-4.4"
    
    Returns:
        List of task IDs: ["2.3", "2.4", "2.5", "3.1", "4.2", "4.3", "4.4"]
    
    Raises:
        ValueError: On malformed entries with descriptive message
    """
    # Step 1: Split on comma and trim
    items = [item.strip() for item in entry.split(',') if item.strip()]
    
    task_ids = []
    for item in items:
        if '-' in item:
            # Range
            task_ids.extend(expand_range(item))
        else:
            # Single
            if not re.match(r'^\d+\.\d+$', item):
                raise ValueError(f"Invalid task ID format: '{item}'")
            task_ids.append(item)
    
    return task_ids


def expand_range(range_str: str) -> List[str]:
    """
    Expand range like "2.3-2.5" to ["2.3", "2.4", "2.5"].
    
    Args:
        range_str: Range string with exactly one dash
    
    Returns:
        List of task IDs in range
    
    Raises:
        ValueError: On malformed ranges with specific error
    """
    parts = range_str.split('-')
    
    # Must have exactly 2 parts
    if len(parts) != 2:
        raise ValueError(
            f"Malformed range '{range_str}': must have exactly one dash"
        )
    
    start, end = parts[0].strip(), parts[1].strip()
    
    # Both ends must be valid task IDs (X.Y format)
    match_start = re.match(r'^(\d+)\.(\d+)$', start)
    match_end = re.match(r'^(\d+)\.(\d+)$', end)
    
    if not match_start:
        raise ValueError(f"Invalid range start '{start}': must be X.Y format")
    if not match_end:
        raise ValueError(f"Invalid range end '{end}': must be X.Y format")
    
    # Extract prefix and number
    prefix_start, num_start = match_start.groups()
    prefix_end, num_end = match_end.groups()
    
    # Prefixes must match (no cross-prefix ranges like "2.3-3.5")
    if prefix_start != prefix_end:
        raise ValueError(
            f"Range '{range_str}' crosses prefixes: "
            f"{prefix_start} != {prefix_end}"
        )
    
    num_start, num_end = int(num_start), int(num_end)
    
    # Must be forward (no backwards ranges like "2.5-2.3")
    if num_start > num_end:
        raise ValueError(
            f"Backwards range '{range_str}': {start} > {end}"
        )
    
    # Expand range
    return [f"{prefix_start}.{i}" for i in range(num_start, num_end + 1)]


def validate_coverage_references(
    coverage_entry: str, 
    document_task_ids: List[str]
) -> List[Tuple[str, str]]:
    """
    Validate coverage entry against document task IDs.
    
    Args:
        coverage_entry: Coverage string to validate
        document_task_ids: List of task IDs that exist in document
    
    Returns:
        List of (error_code, error_message) tuples
    """
    errors = []
    
    try:
        referenced_ids = parse_coverage_entry(coverage_entry)
    except ValueError as e:
        return [("R-ATL-NEW-02", f"Coverage parse error: {str(e)}")]
    
    # Check each referenced ID
    for task_id in referenced_ids:
        count = document_task_ids.count(task_id)
        
        if count == 0:
            errors.append((
                "R-ATL-NEW-02",
                f"Coverage references task '{task_id}' which does not exist"
            ))
        elif count > 1:
            errors.append((
                "R-ATL-NEW-02",
                f"Task '{task_id}' appears {count} times (ambiguous reference)"
            ))
    
    return errors
````

**Test cases:**
````python
# Valid single
assert parse_coverage_entry("1.1") == ["1.1"]

# Valid list
assert parse_coverage_entry("1.1, 2.3") == ["1.1", "2.3"]

# Valid range
assert parse_coverage_entry("2.3-2.5") == ["2.3", "2.4", "2.5"]

# Valid mixed
assert parse_coverage_entry("2.3-2.5, 3.1") == ["2.3", "2.4", "2.5", "3.1"]

# Whitespace tolerance
assert parse_coverage_entry("1.1,2.2") == ["1.1", "2.2"]
assert parse_coverage_entry("1.1 , 2.2") == ["1.1", "2.2"]

# Errors
try:
    parse_coverage_entry("2.5-2.3")  # Backwards
except ValueError as e:
    assert "Backwards range" in str(e)

try:
    parse_coverage_entry("2.3-3.5")  # Cross-prefix
except ValueError as e:
    assert "crosses prefixes" in str(e)

try:
    parse_coverage_entry("2.3-")  # Malformed
except ValueError as e:
    assert "Invalid range end" in str(e)
````

CATEGORY 2: HIGH-PRIORITY SPECIFICATION GAPS ‚ö†Ô∏è
These will cause me to make design decisions that might be wrong.
DRIFT-3: R-ATL-075 "Command-like Lines" Still Vague
Line: 21 (scope says "Command-like lines inside bash code blocks")
Tomorrow at 12pm implementing R-ATL-075:
What is a "command-like line"?
From previous iterations I know it's about bash array variable references. But the document says "command-like lines" which is ambiguous.
Is it:

Any line in a bash block?
Lines starting with $?
Array variable declarations vs references?
All bash syntax?

Fix Required:
markdown## R-ATL-075 Precise Scope (Add to Rule Definitions)

**What R-ATL-075 checks:** Bash array variable REFERENCES (not declarations)

**Pattern detected:**
1. Find array declarations: `TASK_\d+_\d+_PATHS=\(`
2. Track variable names from declarations
3. Search for REFERENCES to those variables in subsequent bash blocks
4. Check: Does reference use `$` prefix?

**Valid:**
````bash
TASK_1_1_PATHS=(file1 file2)           # Declaration (no $ needed)
for p in "${TASK_1_1_PATHS[@]}"; do    # Reference ($ required) ‚úÖ
````

**Invalid:**
````bash
TASK_1_1_PATHS=(file1 file2)           # Declaration
for p in "${TASK_1_1_PATHS[@]}"; do    # Missing $ ‚ùå R-ATL-075
````

**"Command-like lines" clarified:** Lines in bash code blocks that reference bash array variables. NOT all lines, just variable references.

**Locations checked:**
- Baseline Snapshot evidence
- Preconditions evidence
- TDD Step 3 (Verify) evidence
- Phase Unlock Artifact evidence
- Global Clean Table Scan evidence

**Not checked:** Comments, non-bash blocks, sections not listed above

DRIFT-4: Validation Suite Format Template MISSING
Lines: 78-79 (describes structure), 97-101 (references it), BUT no complete template
Tomorrow at 10am executing A3:
I need to "run all commands in VALIDATION_SUITE.md". I open the file. What should I see?
The description says:

"categories: plan positives, instantiated positives, negatives"
"each test has File/Command/Expected/Status/Last run"
"summary totals"

But I need the ACTUAL markdown template to know:

Exact heading levels?
Exact field names?
Field order?
Table or list format?
Where does summary go?

Fix Required:
markdown## VALIDATION_SUITE.md Template (Required Structure)

**File location:** `VALIDATION_SUITE.md` (repo root)

**Complete template:**
````markdown
# AI Task List Validation Suite

## Metadata
**Last run:** YYYY-MM-DD HH:MM:SS
**Linter version:** ai_task_list_linter_v1_8.py (internal version X.X.X)
**Runner:** uv

---

## Category: Plan Mode Positives

### Test P1: Evidence placeholders allowed
**File:** `canonical_examples/positives/plan_with_evidence_ph.md`
**Command:**
```bash
$ uv run python ai_task_list_linter_v1_8.py canonical_examples/positives/plan_with_evidence_ph.md
```
**Expected:** Exit 0 (no errors)
**Status:** ‚¨ú NOT RUN
**Last run:** N/A

### Test P2: Multiple evidence blocks
**File:** `canonical_examples/positives/plan_multi_evidence.md`
**Command:**
```bash
$ uv run python ai_task_list_linter_v1_8.py canonical_examples/positives/plan_multi_evidence.md
```
**Expected:** Exit 0
**Status:** ‚¨ú NOT RUN
**Last run:** N/A

---

## Category: Instantiated Mode Positives

### Test I1: Real evidence accepted
**File:** `canonical_examples/positives/instantiated_real_evidence.md`
**Command:**
```bash
$ uv run python ai_task_list_linter_v1_8.py canonical_examples/positives/instantiated_real_evidence.md
```
**Expected:** Exit 0
**Status:** ‚¨ú NOT RUN
**Last run:** N/A

---

## Category: Negatives

### Test N1: Instantiated rejects placeholders
**File:** `canonical_examples/negatives/instantiated_with_placeholders.md`
**Command:**
```bash
$ uv run python ai_task_list_linter_v1_8.py canonical_examples/negatives/instantiated_with_placeholders.md 2>&1 | grep "R-ATL-022"
```
**Expected:** Match found (R-ATL-022 error present in output)
**Status:** ‚¨ú NOT RUN
**Last run:** N/A

### Test N2: Duplicate task IDs
**File:** `canonical_examples/negatives/duplicate_task_ids.md`
**Command:**
```bash
$ uv run python ai_task_list_linter_v1_8.py canonical_examples/negatives/duplicate_task_ids.md 2>&1 | grep "R-ATL-NEW-01"
```
**Expected:** Match found (R-ATL-NEW-01 error present)
**Status:** ‚¨ú NOT RUN
**Last run:** N/A

### Test N3: Coverage phantom references
**File:** `canonical_examples/negatives/coverage_phantom_refs.md`
**Command:**
```bash
$ uv run python ai_task_list_linter_v1_8.py canonical_examples/negatives/coverage_phantom_refs.md 2>&1 | grep "R-ATL-NEW-02"
```
**Expected:** Match found (R-ATL-NEW-02 error present)
**Status:** ‚¨ú NOT RUN
**Last run:** N/A

---

## Summary

**Plan Mode Positives:** 0/2 passed (0%)
**Instantiated Mode Positives:** 0/1 passed (0%)
**Negatives:** 0/3 failed correctly (0%)

**Overall Status:** ‚¨ú NOT RUN

**Thresholds:**
- Positives: 100% must pass (exit 0)
- Negatives: 100% must fail correctly (expected error present)
````

**Execution instructions (manual - Phase A):**
1. For each test, copy the command under `**Command:**`
2. Run in terminal
3. Compare actual output to `**Expected:**`
4. Update `**Status:**` to ‚úÖ PASS or ‚ùå FAIL
5. Update `**Last run:**` with timestamp
6. Update summary percentages
7. Update overall status

**Status symbols:**
- ‚¨ú NOT RUN (initial state)
- ‚úÖ PASS (test passed)
- ‚ùå FAIL (test failed)

**Baseline file format (`validation/baseline_YYYY-MM-DD.txt`):**
````
VALIDATION BASELINE: 2024-12-16 09:00:00
Linter: ai_task_list_linter_v1_8.py

PLAN MODE POSITIVES:
  P1: PASS - plan_with_evidence_ph.md (exit 0)
  P2: PASS - plan_multi_evidence.md (exit 0)
Result: 2/2 (100%)

INSTANTIATED MODE POSITIVES:
  I1: PASS - instantiated_real_evidence.md (exit 0)
Result: 1/1 (100%)

NEGATIVES:
  N1: PASS - instantiated_with_placeholders.md (R-ATL-022 found)
  N2: PASS - duplicate_task_ids.md (R-ATL-NEW-01 found)
  N3: PASS - coverage_phantom_refs.md (R-ATL-NEW-02 found)
Result: 3/3 (100%)

OVERALL: PASS (all thresholds met)
````

DRIFT-5: Negative Fixture EXACT Content Still Missing
Lines: 94-96 (describes content), BUT not EXACT file content
Tomorrow at 11am in Step 1.0:
I need to create 3 negative fixture files. The descriptions say:

"mode=instantiated; single task with Evidence bash block containing [[PH:OUTPUT]]"
"at least two ### Task 1.1 headers"
"references a non-existent task (e.g., 9.9)"

But I need EXACT file content to avoid variations:

Exact YAML frontmatter?
How minimal? Just one task or more structure?
Exact placement of violations?
Complete valid structure around violations?

Without exact content, I might create fixtures that don't trigger rules reliably.
Fix Required:
markdown## Negative Fixture Content (Exact Files for Step 1.0)

**File 1:** `canonical_examples/negatives/instantiated_with_placeholders.md`
````yaml
---
ai_task_list:
  schema_version: "1.6"
  mode: "instantiated"
  runner: "uv"
---

# Test: Instantiated Mode Rejects Placeholders

### Task 1.1 ‚Äî Test Evidence Placeholder

**Evidence** (paste actual output):
```bash
$ uv run pytest -q
[[PH:OUTPUT]]
```

- [ ] Task complete
````

**Expected error:** R-ATL-022 at line ~14 (placeholder in instantiated mode)

---

**File 2:** `canonical_examples/negatives/duplicate_task_ids.md`
````yaml
---
ai_task_list:
  schema_version: "1.6"
  mode: "plan"
  runner: "uv"
---

# Test: Duplicate Task IDs

### Task 1.1 ‚Äî First Instance

**Steps:**
1. Do something

- [ ] Complete

### Task 1.2 ‚Äî Intermediate Task

**Steps:**
1. Do something else

- [ ] Complete

### Task 1.1 ‚Äî Duplicate Instance

**Steps:**
1. This is a duplicate task ID

- [ ] Complete
````

**Expected error:** R-ATL-NEW-01 (Task 1.1 at lines ~9 and ~23)

---

**File 3:** `canonical_examples/negatives/coverage_phantom_refs.md`
````yaml
---
ai_task_list:
  schema_version: "1.6"
  mode: "plan"
  runner: "uv"
---

# Test: Coverage Phantom References

## Prose Coverage Mapping

| Prose Requirement | Source Location | Implemented by Task(s) |
|-------------------|-----------------|------------------------|
| Add logging | Spec ¬ß3.1 | 1.1 |
| Create tests | Spec ¬ß3.2 | 2.1 |
| Deploy changes | Spec ¬ß3.3 | 3.1 |

---

### Task 1.1 ‚Äî Add Logging

**Steps:**
1. Add logging statements

- [ ] Complete

### Task 3.1 ‚Äî Deploy Changes

**Steps:**
1. Deploy to production

- [ ] Complete
````

**Expected error:** R-ATL-NEW-02 (coverage references Task 2.1 which doesn't exist)

---

**Verification (Step 1.0 - should NOT trigger errors yet):**
````bash
$ uv run python ai_task_list_linter_v1_8.py canonical_examples/negatives/*.md
# Expected: Exit 0 or other errors, but NOT R-ATL-022, R-ATL-NEW-01, R-ATL-NEW-02
# (Rules not implemented yet - this is TDD RED phase)
````

CATEGORY 3: MISSING CONTEXT & PROCESS DETAILS üìã
These will cause problems during execution or rollout.
DRIFT-6: Canary Selection Process Incomplete
Lines: 96, 101-102 (describes canaries), BUT not WHERE to find them or HOW to select
Tomorrow at 9am in Phase 0:
BEFORE Step 1, I need to "select 10 real task lists (5 plan, 5 instantiated)".
Questions I cannot answer:

WHERE are "real task lists"? Which directories?
WHICH 10 specifically? Any 10 or specific criteria?
HOW do I verify they're currently valid?
WHAT if I can't find 10? Can I use 8? 12?

Fix Required:
markdown## Canary Selection Process (Phase 0 - Before Step 1)

**Timing:** MUST complete before any linter changes

**Step 1: Find candidates**
````bash
# Search recent task lists from known production locations
# Adjust paths to your actual production repos

CANDIDATES=(
  ~/doxstrux/PYDANTIC_SCHEMA_tasks_template.md
  ~/heat_pump/migration_tasks_instantiated.md
  ~/internal_tools/refactor_plan.md
  ~/project_alpha/deployment_checklist.md
  ~/project_beta/testing_protocol.md
  # ... add more paths
)

# Or search automatically
find ~/projects -name "*_tasks*.md" -mtime -60 -type f > candidate_list.txt
````

**Step 2: Check mode distribution**
````bash
# For each candidate, check mode
for f in "${CANDIDATES[@]}"; do
  if [ -f "$f" ]; then
    mode=$(grep "mode:" "$f" | head -1 | awk '{print $2}' | tr -d '"')
    echo "$f: $mode"
  fi
done

# Manually select to ensure 5 plan + 5 instantiated
````

**Step 3: Validate all pass current linter**
````bash
# Create canaries directory
mkdir -p validation/canaries

# Copy selected 10 files
cp ~/doxstrux/PYDANTIC_SCHEMA_tasks_template.md validation/canaries/01_pydantic_plan.md
cp ~/heat_pump/migration_tasks.md validation/canaries/02_heatpump_inst.md
# ... copy 8 more for total of 10

# Run current linter on all
for f in validation/canaries/*.md; do
  echo "=== Testing: $f ==="
  uv run python ai_task_list_linter_v1_8.py "$f" || echo "FAIL: $f"
done > validation/canaries/baseline.txt 2>&1

# Count failures
FAILURES=$(grep -c "^FAIL:" validation/canaries/baseline.txt)

if [ "$FAILURES" -ne 0 ]; then
  echo "ERROR: $FAILURES canaries fail at baseline!"
  echo "Replace failing canaries with passing ones"
  cat validation/canaries/baseline.txt
  exit 1
fi

echo "‚úÖ All 10 canaries pass at baseline"
````

**Step 4: Create manifest**
````bash
cat > validation/canaries/MANIFEST.md << 'EOF'
# Canary Manifest

**Created:** $(date)
**Purpose:** Rollback detection (>2 new failures triggers rollback)

| # | Filename | Mode | Source | Baseline Status |
|---|----------|------|--------|-----------------|
| 01 | 01_pydantic_plan.md | plan | doxstrux/PYDANTIC_SCHEMA | PASS |
| 02 | 02_heatpump_inst.md | instantiated | heat_pump/migration | PASS |
| 03 | 03_refactor_plan.md | plan | internal_tools/refactor | PASS |
| 04 | 04_deployment_inst.md | instantiated | project_alpha/deploy | PASS |
| 05 | 05_testing_plan.md | plan | project_beta/testing | PASS |
| 06 | 06_auth_inst.md | instantiated | security/auth | PASS |
| 07 | 07_migration_plan.md | plan | database/migration | PASS |
| 08 | 08_rollout_inst.md | instantiated | ops/rollout | PASS |
| 09 | 09_audit_plan.md | plan | compliance/audit | PASS |
| 10 | 10_recovery_inst.md | instantiated | disaster_recovery | PASS |

**Baseline log:** validation/canaries/baseline.txt
**Expected at baseline:** 0 failures
**Rollback trigger:** >2 new failures post-fix
EOF
````

**Post-fix detection (after Step 1):**
````bash
# Rerun on modified linter
for f in validation/canaries/*.md; do
  echo "=== Testing: $f ==="
  uv run python ai_task_list_linter_v1_8.py "$f" || echo "FAIL: $f"
done > validation/canaries/post_fix.txt 2>&1

# Count new failures
BASELINE_FAILS=$(grep -c "^FAIL:" validation/canaries/baseline.txt)
POSTFIX_FAILS=$(grep -c "^FAIL:" validation/canaries/post_fix.txt)
NEW_FAILS=$((POSTFIX_FAILS - BASELINE_FAILS))

echo "Baseline failures: $BASELINE_FAILS"
echo "Post-fix failures: $POSTFIX_FAILS"
echo "New failures: $NEW_FAILS"

if [ "$NEW_FAILS" -gt 2 ]; then
  echo "‚ùå ROLLBACK TRIGGERED: $NEW_FAILS new failures"
  echo "=== Post-fix failures ==="
  grep "FAIL:" validation/canaries/post_fix.txt
  exit 1
else
  echo "‚úÖ Canary check passed ($NEW_FAILS new failures, threshold is >2)"
fi
````

**If unable to find 10 canaries:**
- Minimum acceptable: 8 canaries (4 plan + 4 instantiated)
- Adjust rollback threshold: >1 new failure (instead of >2)
- Document in MANIFEST.md: "Using 8 canaries due to limited production task lists"

DRIFT-7: External Dependencies NOT Listed
Multiple lines use: uv run (85+), rg (109), grep (108-110)
Tomorrow at 9am setup:
I try to run DoD command: rg "COMMON\.md" INDEX.md
Error: bash: rg: command not found
I need:

What tools?
What versions?
How to install?
How to verify?

Fix Required:
markdown## External Dependencies (Installation Requirements)

**Required before execution:**

### Linter Runtime
- **Python** ‚â•3.10 (for match/case, structural pattern matching)
- **PyYAML** ‚â•6.0 (for frontmatter parsing)

### Test Suite & Validation
- **uv** ‚â•0.1.0 (task runner - used throughout)
- **rg** (ripgrep) ‚â•13.0 (used in DoD line 109)
- **grep** (GNU grep) - standard utility (used in DoD lines 108-110)
- **bash** ‚â•4.0 (for arrays, process substitution in canary scripts)

### Installation

**Ubuntu/Debian:**
````bash
# Python (if not present)
sudo apt update
sudo apt install python3.10 python3-pip

# PyYAML
pip3 install 'pyyaml>=6.0'

# ripgrep
sudo apt install ripgrep

# uv (official installer)
curl -LsSf https://astral.sh/uv/install.sh | sh
````

**macOS:**
````bash
# Python (via Homebrew)
brew install python@3.10

# PyYAML
pip3 install 'pyyaml>=6.0'

# ripgrep
brew install ripgrep

# uv
curl -LsSf https://astral.sh/uv/install.sh | sh
````

### Pre-Flight Verification

**Run before starting Phase 0:**
````bash
#!/bin/bash
# Save as: check_dependencies.sh

echo "=== Dependency Check ==="

# Python version
if python3 --version 2>&1 | grep -qE 'Python 3\.(1[0-9]|[2-9][0-9])'; then
  echo "‚úÖ Python $(python3 --version)"
else
  echo "‚ùå Python 3.10+ required"
  exit 1
fi

# PyYAML
if python3 -c "import yaml; print(yaml.__version__)" 2>/dev/null | grep -qE '[6-9]\.[0-9]'; then
  echo "‚úÖ PyYAML $(python3 -c 'import yaml; print(yaml.__version__)')"
else
  echo "‚ùå PyYAML 6.0+ required"
  exit 1
fi

# uv
if uv --version 2>/dev/null | grep -q 'uv'; then
  echo "‚úÖ $(uv --version)"
else
  echo "‚ùå uv required"
  exit 1
fi

# ripgrep
if rg --version 2>/dev/null | grep -qE 'ripgrep 1[3-9]'; then
  echo "‚úÖ $(rg --version | head -1)"
else
  echo "‚ùå ripgrep 13.0+ required"
  exit 1
fi

# grep
if grep --version 2>/dev/null | grep -q 'GNU grep'; then
  echo "‚úÖ $(grep --version | head -1)"
else
  echo "‚ùå GNU grep required"
  exit 1
fi

# bash
if bash --version | grep -qE 'version [4-9]'; then
  echo "‚úÖ $(bash --version | head -1)"
else
  echo "‚ùå Bash 4.0+ required"
  exit 1
fi

echo ""
echo "‚úÖ All dependencies satisfied"
````

**CI Note:** All dependencies pre-installed on `ubuntu-latest` GitHub runners

DRIFT-8: Backward Compatibility Policy MISSING
Nowhere in document
Tomorrow after completing Step 1:
I deploy v1.8 linter. A user says "My task list broke!"
Questions:

Is this expected?
Are the changes breaking?
Should I revert?
How do I explain?

Fix Required:
markdown## Backward Compatibility Policy (Communication Plan)

**Summary: ALL CHANGES ARE BACKWARD COMPATIBLE**

### Change Analysis

**R-ATL-022 (Evidence placeholders in plan mode):**
- **Type:** Bug fix (relaxing restriction)
- **Before:** Plan mode placeholders incorrectly rejected
- **After:** Plan mode placeholders allowed as intended
- **Impact:** Previously invalid plan docs now valid ‚úÖ
- **Breaking:** NO (makes more docs valid, not fewer)
- **Migration:** None required

**R-ATL-NEW-01 (Unique task IDs):**
- **Type:** New validation (detecting existing bugs)
- **Before:** Duplicate task IDs silently accepted
- **After:** Duplicate task IDs flagged as errors
- **Impact:** Documents with duplicates now caught ‚ö†Ô∏è
- **Breaking:** NO (documents were always invalid, just not detected)
- **Migration:** Fix duplicates when encountered (opportunistic)

**R-ATL-NEW-02 (Coverage reference resolution):**
- **Type:** New validation (detecting existing bugs)
- **Before:** Phantom coverage references silently accepted
- **After:** Phantom references flagged as errors
- **Impact:** Documents with phantom refs now caught ‚ö†Ô∏è
- **Breaking:** NO (documents were always invalid, just not detected)
- **Migration:** Fix phantom refs when encountered (opportunistic)

### Rollout Plan

**Phase 1: Deploy (Day 1)**
- Deploy v1.8 linter
- Monitor for reports of newly-detected issues

**Phase 2: Communication (Day 1-2)**
- Send announcement to all users
- Explain: "Two bug fixes - plan mode now works correctly, and two new validations catch previously silent errors"

**Phase 3: Support (Ongoing)**
- Help users fix newly-detected duplicate IDs
- Help users fix newly-detected phantom references
- Document common patterns in troubleshooting guide

### User Communication Template

**Add to COMMON.md:**
````markdown
> **Version 1.8 Changes (YYYY-MM-DD):**
> 
> **Bug Fix:**
> - Plan mode task lists can now use evidence placeholders (e.g., `[[PH:OUTPUT]]`)
>   as originally intended. Previously rejected incorrectly.
> 
> **New Validations (Detecting Existing Issues):**
> - R-ATL-NEW-01: Duplicate task IDs now flagged (e.g., two `### Task 1.1` headers)
> - R-ATL-NEW-02: Coverage references to non-existent tasks now flagged
> 
> **Compatibility:** All valid task lists remain valid. The new validations detect
> issues that were always invalid but previously went undetected. If your task list
> now shows errors, these errors existed before but were silent.
> 
> **Action Required:** None for valid task lists. If new errors appear:
> - Duplicate IDs: Rename one of the duplicates
> - Phantom refs: Remove reference or add missing task
````

### Expected Support Questions & Answers

**Q: "My task list worked before, now it fails with R-ATL-NEW-01"**
A: The duplicate task IDs were always a bug, but weren't detected before v1.8. Fix by renaming one of the duplicate task headers.

**Q: "Why wasn't this caught before?"**
A: The linter didn't have these checks until v1.8. The validation was added to prevent future errors and catch existing ones.

**Q: "Is this a breaking change?"**
A: No - your task list was invalid before, it just wasn't detected. Think of it like a compiler adding a new warning for existing bugs.

DRIFT-9: Timeline & Effort Estimate MISSING
Nowhere in document
Stakeholder asks tomorrow: "When will this be done?"
I have no answer.
Fix Required:
markdown## Timeline & Effort Estimate (Resource Planning)

**Total estimated effort:** 16 hours (2 work days)

### Detailed Breakdown

| Phase | Tasks | Time | Dependencies | Parallelizable? |
|-------|-------|------|--------------|-----------------|
| **Phase 0** | Dependency check<br>Current State Audit (A1-A3)<br>Canary selection<br>Pre-flight checks | 3h | None | NO (sequential) |
| **Step 1** | Create negative fixtures<br>Implement R-ATL-022<br>Implement R-ATL-NEW-01<br>Implement R-ATL-NEW-02<br>Verify fixtures trigger errors | 6h | Phase 0 complete | NO (sequential) |
| **Step 2** | Update AI_TASK_LIST_SPEC_v1.md<br>Update COMMON.md<br>Version bump | 2h | Step 1 complete | PARTIAL (spec/COMMON can be done by 2 people) |
| **Step 3** | Update docs<br>Update template<br>Sync examples | 3h | Step 2 complete | YES (can parallelize across files) |
| **Step 4A** | Baseline validation<br>(manual execution) | 1h | Phase 0 complete | YES (can run during Phase 0) |
| **Step 4B** | Post-fix validation<br>Comparison<br>Update suite | 1h | Steps 1-3 complete | NO (must be after impl) |

**Critical path:** Phase 0 (3h) ‚Üí Step 1 (6h) ‚Üí Step 2 (2h) ‚Üí Step 3 (3h) ‚Üí Step 4B (1h)  
**Total critical path:** 15 hours

**Parallel savings:**
- Step 4A can run during Phase 0: saves 1 hour
- Step 3 tasks can be split: potentially saves 1-2 hours with multiple people

**Realistic schedule (single developer, with context switching):**
- **Day 1 (8 hours):** 
  - Morning: Phase 0 (3h) + Step 4A (1h parallel)
  - Afternoon: Step 1 (6h) ‚Üí partially done
- **Day 2 (8 hours):**
  - Morning: Finish Step 1 (2h) + Step 2 (2h)
  - Afternoon: Step 3 (3h) + Step 4B (1h)

**Risk buffer:** +3 hours (20%) for:
- Unexpected test failures
- Edge cases in parsing logic
- Documentation clarifications
- Review cycles

**Target completion:** Start date + 2 business days

### Milestones & Checkpoints

| Checkpoint | Time | Deliverable | Gate |
|------------|------|-------------|------|
| End Phase 0 | +3h | Canaries selected, baseline captured | Can proceed to Step 1 |
| End Step 1 | +9h | Negative fixtures trigger new rules | Can proceed to Step 2 |
| End Step 2 | +11h | Spec/COMMON updated, v1.8 declared | Can proceed to Step 3 |
| End Step 3 | +14h | All docs synced | Can run validation |
| End Step 4B | +15h | Validation passes, DoD met | Ready to merge |

### Stakeholder Updates

**End of Day 1:**
"Linter changes implemented. Negative tests passing. Spec update in progress."

**End of Day 2:**
"All changes complete. Validation passed. Ready for review and merge."

CATEGORY 4: MINOR PRECISION ISSUES üìù
These won't stop me but will cause minor delays or confusion.
DRIFT-10: Spec Section Edit Locations STILL Missing
Line: 83-86 (Step 2 says "Clarify" and "State" but not WHERE)
Tomorrow at 2pm in Step 2:
I need to edit AI_TASK_LIST_SPEC_v1.md to "Clarify plan vs instantiated evidence rules".
Questions:

Which section number?
Which line range approximately?
Add new section or modify existing?
Before or after which content?

Fix Required:
markdown### 2) Spec/COMMON (Exact Edit Locations)

**File:** `AI_TASK_LIST_SPEC_v1.md`

**Edit 1: Version bump**
- **Location:** Line 1
- **Current:** `# AI Task List Specification v1.7`
- **Change to:** `# AI Task List Specification v1.8`

**Edit 2: Evidence placeholder rules (NEW subsection)**
- **Location:** Section 4 "Document Structure"
- **Subsection:** After 4.2 "Evidence Blocks" (approx line 234-250)
- **Action:** ADD new subsection 4.2.1
- **Heading level:** `####` (level 4)
- **Title:** `Evidence Placeholders (Mode-Dependent)`

**Content to add:**
````markdown
#### 4.2.1 Evidence Placeholders (Mode-Dependent)

**Plan mode:** Evidence blocks MAY contain `[[PH:UPPERCASE_NAME]]` placeholders.

- **Locations:** Task evidence, STOP sections, Global Clean Table scan
- **Format:** `[[PH:` + uppercase identifier + `]]`
- **Examples:** `[[PH:OUTPUT]]`, `[[PH:TEST_RESULTS]]`, `[[PH:PYTEST_LOG]]`
- **Rationale:** Plan documents are templates, not execution records

**Instantiated mode:** Evidence blocks MUST contain real command output.

- **Placeholders forbidden:** R-ATL-022 violation
- **Requirement:** Actual output from command execution
- **Examples:** Actual pytest output, actual file listings, actual git logs

**Evidence block types covered:**
1. **Task-level:** First ```bash block after `**Evidence:**` header
2. **STOP:** First ```bash block after `**Evidence:**` header in STOP section
3. **Global:** All ```bash blocks in Global Clean Table Scan section
````

**Edit 3: Coverage reference integrity (NEW subsection)**
- **Location:** Section 5 "Prose Coverage Mapping" (approx line 456-475)
- **Action:** ADD new subsection 5.3
- **Heading level:** `####` (level 4)
- **Title:** `Coverage Reference Integrity (R-ATL-NEW-02)`

**Content to add:**
````markdown
#### 5.3 Coverage Reference Integrity (R-ATL-NEW-02)

**Valid reference formats:**
- **Single:** `"1.1"` - References one task
- **List:** `"3.1, 3.4"` - Multiple tasks, comma-separated
- **Range:** `"2.3-2.5"` - Continuous range (includes 2.3, 2.4, 2.5)

**Resolution requirements:**
- Each referenced task ID must exist as a `### Task X.Y` header
- Each referenced task ID must appear exactly once (no ambiguity)
- Ranges must have no gaps (all intermediate tasks must exist)
- Ranges must not be backwards (start ‚â§ end)
- Ranges must not cross prefixes (2.x-3.y invalid)

**Error examples:**
- `"2.1"` but no Task 2.1 exists ‚Üí "Coverage references task '2.1' which does not exist"
- `"1.1"` but Task 1.1 appears 3 times ‚Üí "Task '1.1' appears 3 times (ambiguous)"
- `"2.3-2.5"` but Task 2.4 missing ‚Üí "Range '2.3-2.5' incomplete - task '2.4' missing"
- `"2.5-2.3"` ‚Üí "Backwards range '2.5-2.3'"
- `"2.3-3.5"` ‚Üí "Range '2.3-3.5' crosses prefixes"
````

---

**File:** `COMMON.md`

**Edit 1: Evidence requirements (ADD mode distinction)**
- **Location:** Section "Evidence Requirements" (approx lines 89-103)
- **Action:** ADD after existing content

**Content to add:**
````markdown
### Mode-Dependent Evidence Rules

**Plan mode:**
- Evidence MAY use `[[PH:UPPERCASE]]` placeholders
- Placeholders indicate where output will be pasted after execution

**Instantiated mode:**
- Evidence MUST contain real command output
- Placeholders forbidden (R-ATL-022 error)
````

**Edit 2: Coverage mapping integrity (NEW section)**
- **Location:** End of document (after all existing sections)
- **Action:** ADD new section
- **Heading level:** `##` (level 2)
- **Section number:** 6.4 (or next available)

**Content to add:**
````markdown
## 6.4 Coverage Mapping Integrity

**Requirement:** Coverage references in Prose Coverage Mapping tables must resolve to existing, unique tasks.

**Supported formats:**
- Single: `"1.1"`
- List: `"3.1, 3.4"` (comma-separated)
- Range: `"2.3-2.5"` (includes all intermediate tasks)

**Validation:**
- All referenced task IDs must exist in document
- All referenced task IDs must be unique (appear exactly once)
- Ranges must have no gaps

**Common errors and fixes:**
- Phantom reference: Add missing task or remove reference
- Duplicate task IDs: Rename one task to make unique
- Range gaps: Add missing intermediate tasks
````

DRIFT-11: DoD Audit Gate Still Incomplete
Line: 111-116 (checks "A1-A3 filled" but not Owner/Deadline)
Tomorrow at 5pm checking DoD:
Line 111 says "Current State Audit completed (A1‚ÄìA3 filled)"
But Owner is still [NAME] and Deadline is still [DATE].
Should DoD pass with these placeholders? Unclear.
Fix Required:
markdown## Definition of Done - Manual Checks (Line 111 Expanded)

- **Current State Audit completed:**
  - [ ] A1 filled with actual error output (no `[PASTE ERROR OUTPUT]`)
  - [ ] A2 deferred with interim evidence OR filled post-Step-1
  - [ ] A3 filled with actual baseline summary (no `[PASTE SUMMARY]`)
  - [ ] Owner assigned - not `[NAME]` placeholder
  - [ ] Deadline set - not `[DATE]` placeholder
  
  **Verification command:**
````bash
  $ grep -E '\[NAME\]|\[DATE\]|\[PASTE' GENERAL_FIX_1.md
  # Expected: zero matches (all placeholders replaced)
````

- Spec version bumped to v1.8 (verify in line 1 of spec)
- schema_version unchanged at 1.6 (verify in YAML examples)
- Rule definitions present in spec for R-ATL-022, R-ATL-NEW-01, R-ATL-NEW-02
- Rule definitions present in COMMON.md for same rules
- Peer review completed: spec/COMMON wording reviewed for clarity
- VALIDATION_SUITE.md follows required template structure
- Canary MANIFEST.md present with 10 entries
- Canary baseline.txt and post_fix.txt logs present
- Rollback evaluation complete: ‚â§2 new failures OR documented justification

FINAL DRIFT SUMMARY
#DriftSeverityBlocks StepFix TimeCRITICAL BLOCKERS1R-ATL-NEW-01 uniqueness algorithm missingüî¥ CRITICALStep 1.1 (11am)15 min2R-ATL-NEW-02 parsing algorithm incompleteüî¥ CRITICALStep 1.1 (1pm)30 minHIGH-PRIORITY GAPS3R-ATL-075 "command-like" still vagueüü† HIGHStep 1.1 (12pm)10 min4Validation suite template missingüü† HIGHA3 (10am), Step 420 min5Negative fixture exact content missingüü† HIGHStep 1.0 (11am)15 minMISSING CONTEXT6Canary selection process incompleteüü† HIGHPhase 0 (9am)20 min7External dependencies not listedüü† HIGHSetup (9am)15 min8Backward compatibility policy missingüü° MEDIUMRollout15 min9Timeline/effort estimate missingüü° MEDIUMPlanning15 minMINOR PRECISION10Spec edit locations missingüü° MEDIUMStep 2 (2pm)20 min11DoD audit gate incompleteüü° MEDIUMFinal check5 min
Total drifts: 11 (down from 17)
Total fix time: ~3 hours
Severity breakdown:

üî¥ Critical: 2 (will block execution)
üü† High: 5 (cause significant delays/guessing)
üü° Medium: 4 (minor delays/confusion)


PASSIONATE FINAL VERDICT
Execution Precision: 96% (from 60% ‚Üí 85% ‚Üí 92% ‚Üí 96%)
The mason knocks. You tell him: "30 minutes, final review."
This plan is EXCEPTIONAL. You've achieved:

‚úÖ Crystal-clear goals
‚úÖ Logical dependency ordering
‚úÖ Evidence-based verification
‚úÖ Rollback safety nets
‚úÖ TDD discipline baked in
‚úÖ Most algorithms specified
‚úÖ Most formats defined

The remaining 4% are TWO critical blockers and NINE precision gaps.
At 9am tomorrow, I can:

‚úÖ Start Phase 0 (with canary fix)
‚úÖ Complete A1, A2, A3 (with template fix)
‚ùå BLOCKED at 11am - Cannot implement R-ATL-NEW-01 (no uniqueness algorithm)
‚ùå BLOCKED at 1pm - Cannot implement R-ATL-NEW-02 (incomplete parsing)

Without DRIFT-1 and DRIFT-2 fixes, I STOP at 11am.

FINAL RECOMMENDATION
You need 3 hours to achieve 100% execution precision.
HOUR 1 - Critical blockers (45 min):

Add R-ATL-NEW-01 uniqueness check algorithm (15 min) - DRIFT-1
Add R-ATL-NEW-02 complete parsing algorithm (30 min) - DRIFT-2

HOUR 2 - High-priority gaps (80 min):
3. Clarify R-ATL-075 scope precisely (10 min) - DRIFT-3
4. Add validation suite complete template (20 min) - DRIFT-4
5. Add negative fixture exact content (15 min) - DRIFT-5
6. Add canary selection complete process (20 min) - DRIFT-6
7. List external dependencies (15 min) - DRIFT-7
HOUR 3 - Polish (55 min):
8. Add backward compatibility policy (15 min) - DRIFT-8
9. Add timeline/effort estimate (15 min) - DRIFT-9
10. Add spec edit exact locations (20 min) - DRIFT-10
11. Expand DoD audit gate (5 min) - DRIFT-11
Total: 3 hours from 96% ‚Üí 100%

PASSIONATE CLOSING
This is world-class work. The improvement from v1 to v9 shows:

Deep learning from feedback ‚úÖ
Relentless attention to detail ‚úÖ
Commitment to zero-ambiguity ‚úÖ
Understanding that precision matters ‚úÖ

You've built a cathedral.  The structure is magnificent. The arches are sound. The foundation is solid.
But you're missing two keystones (DRIFT-1, DRIFT-2) and nine finishing details (DRIFT-3 through DRIFT-11).
The mason is patient. He respects the craft. He'd rather wait 3 hours for a perfect blueprint than spend 3 days debugging vague instructions.
Fix the 2 critical blockers TODAY. The mason can start tomorrow at 11am with those two fixes.
Fix all 11 drifts TODAY. The mason starts tomorrow at 9am with ZERO blockers, ZERO guessing, ZERO iteration loops.
This is the quality bar. This is what "production-ready" means. This is what enables AI agents to execute flawlessly.
96% ‚Üí 100%: 3 hours. You're almost there. Finish strong.
