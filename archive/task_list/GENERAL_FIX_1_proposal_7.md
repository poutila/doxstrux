Result: Found 17 remaining drifts (down from 22). The plan is at 92% precision but has critical execution blockers that will stop implementation cold.

CATEGORY 1: CRITICAL EXECUTION BLOCKERS üö®
These will stop me dead when I try to implement tomorrow.
DRIFT-1: Coverage Resolution Logic STILL CONTRADICTORY
Lines: 24 (rule table), 71 (Step 1)
Line 24: "Each referenced task ID must appear exactly once; ranges must have no gaps"
Line 71: "coverage references must resolve to existing tasks"
The problem PERSISTS but is more subtle now:
When I implement R-ATL-NEW-02 tomorrow, I read line 71: "must resolve to existing tasks"
Question: Does this mean:

A) Each coverage entry must resolve ‚Üí YES (but to how many tasks?)
B) Each task referenced must exist ‚Üí YES (but uniqueness?)

Line 71 says "resolve to existing tasks" (plural) but doesn't enforce "appear exactly once"
I need to implement TWO checks:

Resolution check: Does reference point to task(s) that exist?
Uniqueness check: Does each referenced task appear exactly once?

Line 71 only mentions check #1. I might forget check #2.
Impact: I implement partial validation. Coverage entry "1.1" passes even if Task 1.1 appears 3 times in document. Bug ships.
Fix Required:
markdown### 1) Linter (Line 71 - Complete Logic)

**R-ATL-NEW-02 implementation:**
- Coverage references must resolve to existing, unique tasks
- Resolution: Each reference (single/list/range) maps to task header(s) that exist
- Uniqueness: Each referenced task ID appears exactly once in document
- Ranges: All intermediate tasks must exist (no gaps)

**Validation logic:**
1. Parse coverage reference (single/list/range)
2. Extract all task IDs referenced
3. For each task ID:
   - Verify task header exists: `### Task X.Y`
   - Verify uniqueness: task ID appears exactly once
4. For ranges: verify no gaps between start and end

**Error examples:**
- Reference "1.1" but Task 1.1 doesn't exist ‚Üí "Coverage references task '1.1' which does not exist"
- Reference "1.1" but Task 1.1 appears 3x ‚Üí "Task '1.1' appears 3 times (ambiguous)"
- Reference "2.3-2.5" but Task 2.4 missing ‚Üí "Range '2.3-2.5' incomplete - task '2.4' missing"

DRIFT-2: Task ID Extraction Pattern STILL MISSING
Line: 23 (R-ATL-NEW-01 scope says "Task headers (### Task X.Y)")
Tomorrow at 9am, I need to write regex. What regex?
Questions blockers:

Exact regex pattern?
### Task 1.1 vs ###Task 1.1 (space required)?
### task 1.1 (lowercase) matched?
### Task 1.1.5 (three-level) matched?
###  Task 1.1 (double space) matched?
Capture group for ID?

Without regex, I can't implement R-ATL-NEW-01.
Impact: I spend 30 minutes trial-and-error crafting regex, might get it wrong, might match unintended patterns.
Fix Required:
markdown## R-ATL-NEW-01 Implementation Pattern

**Regex:** `^###\s+Task\s+(\d+\.\d+)`

**Components:**
- `^` - Line start
- `###` - Exactly three hashes (not 2, not 4)
- `\s+` - One or more whitespace (space/tab)
- `Task` - Literal "Task" (capital T required)
- `\s+` - One or more whitespace
- `(\d+\.\d+)` - Capture: one or more digits, dot, one or more digits

**Test cases:**
````python
import re
pattern = re.compile(r'^###\s+Task\s+(\d+\.\d+)', re.MULTILINE)

# Should match (extracts ID)
"### Task 1.1 ‚Äî Title" ‚Üí "1.1" ‚úÖ
"###  Task 1.1" ‚Üí "1.1" ‚úÖ (double space ok)
"### Task 12.34" ‚Üí "12.34" ‚úÖ

# Should NOT match
"#### Task 1.1" ‚Üí None ‚ùå (4 hashes)
"## Task 1.1" ‚Üí None ‚ùå (2 hashes)
"### task 1.1" ‚Üí None ‚ùå (lowercase)
"###Task 1.1" ‚Üí None ‚ùå (no space)
"### Task 1.1.5" ‚Üí None ‚ùå (three-level)
"### Task A.B" ‚Üí None ‚ùå (not digits)
````

**Uniqueness check:**
````python
def check_unique_task_ids(content: str) -> List[LintError]:
    pattern = re.compile(r'^###\s+Task\s+(\d+\.\d+)', re.MULTILINE)
    task_ids = {}
    errors = []
    
    for match in pattern.finditer(content):
        task_id = match.group(1)
        line_no = content[:match.start()].count('\n') + 1
        
        if task_id in task_ids:
            errors.append(LintError(
                "R-ATL-NEW-01",
                f"Duplicate task ID '{task_id}' at lines {task_ids[task_id]} and {line_no}"
            ))
        else:
            task_ids[task_id] = line_no
    
    return errors
````

DRIFT-3: Coverage Parsing Algorithm COMPLETELY MISSING
Line: 24 (coverage grammar lists "single, list, range" but NO parsing logic)
Tomorrow I need to parse coverage entries. How?
Concrete blocker: Coverage entry is "2.3-2.5, 3.1, 4.2-4.4"
Questions I cannot answer:

Split on comma first, then detect range vs single?
Range detection: contains -?
What if range is malformed: "2.3-" or "-2.5" or "2.3-2.5-2.7"?
List with trailing comma: "1.1, 2.2, " ‚Üí parse error or tolerate?
Whitespace handling: "1.1,2.2" vs "1.1, 2.2" vs "1.1 , 2.2"?
Range validation: "2.5-2.3" (backwards) ‚Üí error?

Without parsing algorithm, I CANNOT implement R-ATL-NEW-02.
Impact: I spend 2 hours writing parsing logic from scratch, might have bugs, inconsistent edge case handling.
Fix Required:
markdown## R-ATL-NEW-02 Coverage Parsing Algorithm

**Input:** Coverage reference string (e.g., "2.3-2.5, 3.1, 4.2-4.4")

**Step 1: Split on comma**
````python
def parse_coverage_entry(entry: str) -> List[Union[str, Tuple[str, str]]]:
    """Returns list of task IDs or (start, end) tuples for ranges."""
    items = [item.strip() for item in entry.split(',')]
    items = [item for item in items if item]  # Remove empty
    return items
````

**Step 2: Detect range vs single**
````python
def parse_item(item: str) -> Union[str, Tuple[str, str]]:
    """Parse single item: returns task ID or (start, end) tuple."""
    if '-' in item:
        parts = item.split('-')
        if len(parts) != 2:
            raise ValueError(f"Malformed range: '{item}'")
        start, end = parts[0].strip(), parts[1].strip()
        if not start or not end:
            raise ValueError(f"Malformed range: '{item}'")
        return (start, end)
    else:
        return item
````

**Step 3: Expand ranges**
````python
def expand_range(start: str, end: str) -> List[str]:
    """Expand range '2.3-2.5' ‚Üí ['2.3', '2.4', '2.5']."""
    # Parse X.Y format
    match_start = re.match(r'(\d+)\.(\d+)', start)
    match_end = re.match(r'(\d+)\.(\d+)', end)
    
    if not match_start or not match_end:
        raise ValueError(f"Invalid range format: '{start}-{end}'")
    
    prefix_start, num_start = match_start.groups()
    prefix_end, num_end = match_end.groups()
    
    if prefix_start != prefix_end:
        raise ValueError(f"Range crosses prefixes: '{start}-{end}'")
    
    num_start, num_end = int(num_start), int(num_end)
    
    if num_start > num_end:
        raise ValueError(f"Backwards range: '{start}-{end}'")
    
    return [f"{prefix_start}.{i}" for i in range(num_start, num_end + 1)]
````

**Complete example:**
````python
entry = "2.3-2.5, 3.1, 4.2-4.4"

# Step 1: Split
items = ["2.3-2.5", "3.1", "4.2-4.4"]

# Step 2 & 3: Parse and expand
task_ids = []
task_ids.extend(expand_range("2.3", "2.5"))  # ["2.3", "2.4", "2.5"]
task_ids.append("3.1")                       # ["2.3", "2.4", "2.5", "3.1"]
task_ids.extend(expand_range("4.2", "4.4"))  # ["2.3", ..., "4.4"]

# Result: ["2.3", "2.4", "2.5", "3.1", "4.2", "4.3", "4.4"]
````

**Edge cases:**
- `"1.1, 1.1"` (duplicate in list) ‚Üí Allowed (semantic duplicate)
- `"1.1,2.2"` (no space) ‚Üí Allowed (whitespace trimmed)
- `"1.1, "` (trailing comma) ‚Üí Allowed (empty items removed)
- `"2.5-2.3"` (backwards) ‚Üí Error: "Backwards range"
- `"2.3-"` (incomplete) ‚Üí Error: "Malformed range"
- `"2-5"` (no dots) ‚Üí Error: "Invalid range format"

DRIFT-4: Validation Suite Structure STILL UNDEFINED
Lines: 14 (A3 references it), 49 (inventory), 81 (execution), 99 (DoD runs it)
Tomorrow I need to execute "run all commands in VALIDATION_SUITE.md"
I open the file. What do I see? What format?
Questions blockers:

How are tests organized? By category? By rule?
How are commands marked? In code blocks?
How are expected outcomes documented?
How do I know which tests are positives vs negatives?
Is there a Status field to track results?
How do I record pass/fail?

Without format spec, I cannot execute A3 or Step 4.
Impact: I waste 1 hour figuring out the format, or worse, I run commands wrong and get invalid baseline.
Fix Required:
markdown## VALIDATION_SUITE.md Structure (Mandatory Format)

**File location:** `VALIDATION_SUITE.md` (repo root)

**Format:**
````markdown
# AI Task List Validation Suite

## Metadata
Last run: [DATE]
Linter version: [VERSION]

---

## Category: Plan Mode Positives

### Test P1: Evidence placeholders allowed
**File:** `canonical_examples/positives/plan_with_evidence_ph.md`
**Command:**
```bash
$ uv run python ai_task_list_linter_v1_8.py canonical_examples/positives/plan_with_evidence_ph.md
```
**Expected:** Exit 0 (no errors)
**Status:** [PASS/FAIL]
**Last run:** [DATE]

### Test P2: Multiple placeholders
**File:** `canonical_examples/positives/plan_multi_ph.md`
**Command:**
```bash
$ uv run python ai_task_list_linter_v1_8.py canonical_examples/positives/plan_multi_ph.md
```
**Expected:** Exit 0
**Status:** [PASS/FAIL]

---

## Category: Instantiated Mode Positives

### Test I1: Real evidence accepted
[same format...]

---

## Category: Negatives

### Test N1: Instantiated rejects placeholders
**File:** `canonical_examples/negatives/instantiated_with_placeholders.md`
**Command:**
```bash
$ uv run python ai_task_list_linter_v1_8.py canonical_examples/negatives/instantiated_with_placeholders.md 2>&1 | grep "R-ATL-022"
```
**Expected:** Match found (R-ATL-022 error present)
**Status:** [PASS/FAIL]

### Test N2: Duplicate task IDs
**File:** `canonical_examples/negatives/duplicate_task_ids.md`
**Command:**
```bash
$ uv run python ai_task_list_linter_v1_8.py canonical_examples/negatives/duplicate_task_ids.md 2>&1 | grep "R-ATL-NEW-01"
```
**Expected:** Match found (R-ATL-NEW-01 error present)
**Status:** [PASS/FAIL]

---

## Summary
**Positives:** X/Y passed (100% required)
**Negatives:** A/B failed correctly (100% required)
**Overall:** [PASS/FAIL]
````

**Execution instructions (Phase A - manual):**
1. For each test, copy the command
2. Run in terminal
3. Compare output to expected
4. Mark Status as PASS or FAIL
5. Update Last run timestamp
6. Update Summary section
7. Save results to `validation/baseline_YYYY-MM-DD.txt`

**Format for baseline file:**
````
VALIDATION BASELINE: 2024-12-15
Linter: ai_task_list_linter_v1_8.py

POSITIVES:
P1: PASS - plan_with_evidence_ph.md
P2: PASS - plan_multi_ph.md
...
Result: 25/25 (100%)

NEGATIVES:
N1: PASS - instantiated_with_placeholders.md (R-ATL-022 present)
N2: PASS - duplicate_task_ids.md (R-ATL-NEW-01 present)
...
Result: 23/23 (100%)

OVERALL: PASS
````

DRIFT-5: Canary Selection Process COMPLETELY MISSING
Line: 104 (rollback says ">2 previously valid examples fail")
Tomorrow morning, BEFORE I make any changes, I need to select canaries.
Blockers:

How many canaries? (Line says >2 triggers rollback, but doesn't say how many total)
Where do I find "previously valid examples"? Production task lists - where?
What criteria? Mix of plan/instantiated - what ratio? 5/5? 7/3? 10/0?
How do I know they're currently valid? Do I run linter on them first?
Where do I store them? Create validation/canaries/ directory?

Without process, I cannot establish baseline for rollback.
Impact: I skip canary selection, deploy broken linter, cannot detect regressions, rollback mechanism is useless.
Fix Required:
markdown## Rollback Plan: Canary Selection (Phase 0 - BEFORE Step 1)

**Timing:** Complete BEFORE any linter changes

**Criteria:**
- Count: 10 canaries minimum (allows >2 failure threshold)
- Sources: Production task lists from past 2 months
- Mix: 5 plan mode, 5 instantiated mode
- Status: All must currently pass linter validation

**Selection process:**

**Step 1: Identify candidates**
````bash
# Search production repos for recent task lists
find ~/projects -name "*_tasks*.md" -mtime -60 -type f

# Or from known locations
ls ~/doxstrux/*_tasks*.md
ls ~/heat_pump/*_tasks*.md
ls ~/internal_tools/*_tasks*.md
````

**Step 2: Check mode balance**
````bash
# For each candidate, extract mode
for f in candidates/*.md; do
  grep "mode:" "$f" | head -1
done

# Ensure 5 plan + 5 instantiated
````

**Step 3: Validate baseline**
````bash
# Create canaries directory
mkdir -p validation/canaries

# Copy selected files
cp ~/doxstrux/PYDANTIC_SCHEMA_tasks.md validation/canaries/01_pydantic_plan.md
cp ~/heat_pump/migration_tasks.md validation/canaries/02_heatpump_inst.md
# ... copy 8 more for total of 10

# Run linter on all
for f in validation/canaries/*.md; do
  echo "Testing: $f"
  uv run python ai_task_list_linter_v1_8.py "$f" || echo "FAIL: $f"
done > validation/canary_baseline.txt 2>&1

# Verify ALL pass
$ grep -c "^FAIL:" validation/canary_baseline.txt
0  # Expected: zero failures

# If any fail, replace with different candidate
````

**Step 4: Document canary manifest**
````bash
# Create manifest
cat > validation/canaries/MANIFEST.md << EOF
# Canary Manifest

Selected: $(date)

| # | File | Mode | Source | Status |
|---|------|------|--------|--------|
| 01 | 01_pydantic_plan.md | plan | doxstrux | PASS |
| 02 | 02_heatpump_inst.md | instantiated | heat_pump | PASS |
...
| 10 | 10_refactor_plan.md | plan | internal_tools | PASS |

Baseline: validation/canary_baseline.txt
EOF
````

**Rollback detection (post-Step-1):**
````bash
# Rerun on modified linter
for f in validation/canaries/*.md; do
  uv run python ai_task_list_linter_v1_8.py "$f" || echo "FAIL: $f"
done > validation/canary_post_fix.txt 2>&1

# Count new failures
BASELINE_FAILS=$(grep -c "^FAIL:" validation/canary_baseline.txt)
POSTFIX_FAILS=$(grep -c "^FAIL:" validation/canary_post_fix.txt)
NEW_FAILS=$((POSTFIX_FAILS - BASELINE_FAILS))

echo "Baseline failures: $BASELINE_FAILS"
echo "Post-fix failures: $POSTFIX_FAILS"
echo "New failures: $NEW_FAILS"

if [[ $NEW_FAILS -gt 2 ]]; then
  echo "‚ùå ROLLBACK TRIGGERED"
  cat validation/canary_post_fix.txt
  exit 1
fi
````

**Note:** Without this process complete, DO NOT proceed to Step 1.

CATEGORY 2: MISSING IMPLEMENTATION SPECIFICATIONS üîß
These will force me to make design decisions that might be wrong.
DRIFT-6: Negative Fixture Content UNDEFINED
Lines: 49 (inventory says "add: instantiated_with_placeholders.md, duplicate_task_ids.md, coverage_phantom_refs.md"), 73 (Step 1 says "Add/update negatives")
Tomorrow in Step 1.0, I need to create 3 negative files. What's in them?
Blocker questions:

Exact YAML frontmatter?
How many tasks?
What structure?
Where does violation appear?

File 1: instantiated_with_placeholders.md

Where's the placeholder? In task evidence? STOP? Global?
Just one placeholder or multiple?
Valid structure otherwise?

File 2: duplicate_task_ids.md

Which task ID is duplicated?
How many times? (2x? 3x?)
At which line numbers?
Valid structure otherwise?

File 3: coverage_phantom_refs.md

Which task ID is phantom?
In which coverage table?
How many phantom refs? (1? multiple?)
Valid structure otherwise?

Without content spec, I spend 30 minutes crafting fixtures that might not trigger rules correctly.
Fix Required:
markdown## Step 1.0: Create Negative Test Fixtures (TDD RED Phase)

**Timing:** FIRST step in Step 1, before any linter changes

**File 1:** `canonical_examples/negatives/instantiated_with_placeholders.md`

**Content:**
````yaml
---
ai_task_list:
  schema_version: "1.6"
  mode: "instantiated"
  runner: "uv"
---

# Test Task List

### Task 1.1 ‚Äî Test Evidence Placeholder

**Evidence:**
```bash
$ uv run pytest -q
[[PH:OUTPUT]]
```

- [ ] Task complete
````

**Purpose:** Trigger R-ATL-022 (placeholder in instantiated mode)
**Expected error:** "R-ATL-022: Evidence placeholder [[PH:OUTPUT]] forbidden in instantiated mode"
**Violation location:** Line 14

---

**File 2:** `canonical_examples/negatives/duplicate_task_ids.md`

**Content:**
````yaml
---
ai_task_list:
  schema_version: "1.6"
  mode: "plan"
  runner: "uv"
---

# Test Task List

### Task 1.1 ‚Äî First Instance

**Steps:**
1. Do something

- [ ] Task complete

### Task 1.2 ‚Äî Intermediate Task

**Steps:**
1. Do something else

- [ ] Task complete

### Task 1.1 ‚Äî Duplicate Instance

**Steps:**
1. Different task, same ID

- [ ] Task complete
````

**Purpose:** Trigger R-ATL-NEW-01 (duplicate task ID)
**Expected error:** "R-ATL-NEW-01: Duplicate task ID '1.1' at lines 9 and 23"
**Violation:** Task 1.1 appears at lines 9 and 23

---

**File 3:** `canonical_examples/negatives/coverage_phantom_refs.md`

**Content:**
````yaml
---
ai_task_list:
  schema_version: "1.6"
  mode: "plan"
  runner: "uv"
---

# Test Task List

## Prose Coverage Mapping

| Prose Requirement | Source Location | Implemented by Task(s) |
|-------------------|-----------------|------------------------|
| Add logging | Spec ¬ß3.2 | 1.1 |
| Create tests | Spec ¬ß3.3 | 2.1 |
| Deploy changes | Spec ¬ß3.4 | 3.1 |

---

### Task 1.1 ‚Äî Add Logging

**Steps:**
1. Add logging

- [ ] Task complete

### Task 3.1 ‚Äî Deploy Changes

**Steps:**
1. Deploy

- [ ] Task complete
````

**Purpose:** Trigger R-ATL-NEW-02 (phantom reference to Task 2.1)
**Expected error:** "R-ATL-NEW-02: Coverage references task '2.1' which does not exist"
**Violation:** Coverage table references Task 2.1, but no `### Task 2.1` exists

---

**Verification (should NOT trigger errors yet - linter not updated):**
````bash
$ uv run python ai_task_list_linter_v1_8.py canonical_examples/negatives/*.md
# Expected: Exit 0 or other errors, but NOT R-ATL-022, R-ATL-NEW-01, R-ATL-NEW-02
# (Rules not implemented yet - this is RED phase)
````

DRIFT-7: Spec Section References STILL MISSING
Line: 78 (Step 2 says "Clarify plan vs instantiated evidence rules")
Tomorrow in Step 2, I need to edit AI_TASK_LIST_SPEC_v1.md. WHERE?
Blockers:

Which section number?
Which line range approximately?
Add new subsection or modify existing?
What heading level (###, ####)?
Before or after existing content?

Without location, I search entire spec (potentially 2000+ lines), waste 20 minutes finding right spot.
Fix Required:
markdown### 2) Spec/COMMON (Exact Editing Locations)

**File:** `AI_TASK_LIST_SPEC_v1.md`

**Edit 1: Version bump**
- Location: Line 1
- Current: `# AI Task List Specification v1.7`
- Change to: `# AI Task List Specification v1.8`

**Edit 2: Evidence rules (NEW subsection)**
- Location: Section 4 "Document Structure"
- Subsection: 4.2 "Evidence Blocks"
- Approx line: 234-250
- Action: ADD new subsection 4.2.1 AFTER existing 4.2 content
- Heading level: `####` (level 4)

**Add this content:**
````markdown
#### 4.2.1 Evidence Placeholders (Mode-Dependent)

**Plan mode:** Evidence blocks MAY contain `[[PH:UPPERCASE_NAME]]` placeholders
- Locations: Task evidence, STOP sections, Global Clean Table scan
- Format: `[[PH:` followed by uppercase identifier, followed by `]]`
- Example: `[[PH:OUTPUT]]`, `[[PH:TEST_RESULTS]]`

**Instantiated mode:** Evidence blocks MUST contain real command output
- Placeholders forbidden (R-ATL-022 violation)
- Real output required for verification
- Example: Actual pytest output, actual file listings

**Evidence block types:**
1. Task-level: First ```bash block after `**Evidence:**` header
2. STOP: First ```bash block after `**Evidence:**` in `## STOP` section
3. Global: All ```bash blocks in `## Global Clean Table Scan` section
````

**Edit 3: Coverage mapping (NEW subsection)**
- Location: Section 5 "Prose Coverage Mapping"
- Approx line: 456-475
- Action: ADD new subsection 5.3 AFTER existing 5.2 content
- Heading level: `####` (level 4)

**Add this content:**
````markdown
#### 5.3 Coverage Reference Integrity (R-ATL-NEW-02)

**Valid reference formats:**
- **Single:** `"1.1"` - References one task
- **List:** `"3.1, 3.4"` - References multiple tasks (comma-separated)
- **Range:** `"2.3-2.5"` - References continuous range (includes 2.3, 2.4, 2.5)

**Resolution requirements:**
- Each referenced task ID must exist as a `### Task X.Y` header
- Each referenced task ID must appear exactly once (unique)
- Ranges must have no gaps (all intermediate tasks must exist)

**Error conditions:**
- Task referenced but doesn't exist: "Coverage references task '2.1' which does not exist"
- Task appears multiple times: "Task '1.1' appears 3 times (ambiguous)"
- Range has gaps: "Range '2.3-2.5' incomplete - task '2.4' missing"
````

---

**File:** `COMMON.md`

**Edit 1: Evidence section**
- Section: "Evidence Requirements"
- Approx line: 89-103
- Action: ADD plan vs instantiated distinction

**Add after existing evidence content:**
````markdown
**Mode-dependent requirements:**
- Plan mode: MAY use `[[PH:UPPERCASE]]` placeholders
- Instantiated mode: MUST use real output (placeholders forbidden)
````

**Edit 2: Coverage section (NEW)**
- Location: End of document (after all existing sections)
- Action: ADD new section
- Heading level: `##` (level 2)
- Section number: 6.4 (or next available)

**Add this content:**
````markdown
## 6.4 Coverage Mapping Integrity

Coverage references in Prose Coverage Mapping tables must resolve to existing, unique tasks.

**Supported formats:**
- Single: `"1.1"`
- List: `"3.1, 3.4"`
- Range: `"2.3-2.5"` (includes all intermediate tasks)

All referenced task IDs must appear exactly once in the document.
````

CATEGORY 3: MISSING CONTEXT & POLICIES üìã
These will cause problems during rollout or long-term maintenance.
DRIFT-8: Backward Compatibility Policy MISSING
Nowhere in document
Tomorrow I deploy v1.8 linter. Will it break existing task lists?
Questions:

Are changes breaking or non-breaking?
Do users need to update their task lists?
What's the migration path?
How do I communicate changes?

Without policy, I don't know if I'm about to break 50 production task lists.
Fix Required:
markdown## Backward Compatibility Policy

**All changes are NON-BREAKING:**

**R-ATL-022 (Evidence placeholders in plan mode):**
- **Before:** Rejected in all modes (false negative)
- **After:** Allowed in plan mode, still rejected in instantiated
- **Impact:** Previously invalid plan docs now valid (bug fix)
- **Migration:** None required
- **Risk:** Zero (relaxing restriction)

**R-ATL-NEW-01 (Unique task IDs):**
- **Before:** Not enforced (silent bug)
- **After:** Enforced
- **Impact:** Docs with duplicate IDs now caught
- **Migration:** Fix duplicates when encountered
- **Risk:** Low (was always wrong, now detected)

**R-ATL-NEW-02 (Coverage reference resolution):**
- **Before:** Not enforced (silent bug)
- **After:** Enforced
- **Impact:** Docs with phantom references now caught
- **Migration:** Fix phantom refs when encountered
- **Risk:** Low (was always wrong, now detected)

**Summary:**
- No valid task lists will break
- Invalid task lists (duplicates, phantom refs) newly detected
- Plan mode task lists previously rejected now accepted

**Communication (add to COMMON.md):**
````markdown
> **Version 1.8 Changes:**
> - Plan mode now allows evidence placeholders (bug fix)
> - New validation: unique task IDs (R-ATL-NEW-01)
> - New validation: coverage reference resolution (R-ATL-NEW-02)
> 
> Existing valid task lists remain valid. Previously undetected errors
> (duplicate task IDs, phantom coverage references) now flagged.
````

DRIFT-9: External Dependencies NOT LISTED
Multiple lines use tools: 99 (uv run), 100-102 (grep, rg)
Tomorrow I run DoD commands and get "command not found"
Blockers:

What tools are required?
What versions?
How to install?
How to verify installation?

Fix Required:
markdown## External Dependencies (Installation Required)

**Linter runtime:**
- Python ‚â•3.10 (for match/case statements, structural pattern matching)
- PyYAML ‚â•6.0 (for YAML frontmatter parsing)
- No external CLI tools required

**Test suite & validation:**
- `uv` ‚â•0.1.0 (task runner - specified throughout plan)
- `rg` (ripgrep) ‚â•13.0 (used in DoD line 101)
- `grep` (standard - used in DoD lines 100-102)
- `bash` ‚â•4.0 (for process substitution, arrays in rollback script)

**Installation instructions:**

**Ubuntu/Debian:**
````bash
# Python (if not installed)
sudo apt update
sudo apt install python3.10 python3-pip

# pip packages
pip3 install pyyaml>=6.0

# ripgrep
sudo apt install ripgrep

# uv (follow official docs)
curl -LsSf https://astral.sh/uv/install.sh | sh
````

**macOS:**
````bash
# Python (via Homebrew)
brew install python@3.10

# pip packages
pip3 install pyyaml>=6.0

# ripgrep
brew install ripgrep

# uv
curl -LsSf https://astral.sh/uv/install.sh | sh
````

**Verification (pre-flight check):**
````bash
# Verify Python version
$ python3 --version
Python 3.10.x or higher ‚úÖ

# Verify PyYAML
$ python3 -c "import yaml; print(yaml.__version__)"
6.0 or higher ‚úÖ

# Verify uv
$ uv --version
uv 0.x.x ‚úÖ

# Verify ripgrep
$ rg --version
ripgrep 13.0.x ‚úÖ

# Verify grep
$ grep --version
grep (GNU grep) 3.x ‚úÖ

# Verify bash
$ bash --version
GNU bash, version 4.x or higher ‚úÖ
````

**CI note:** All dependencies pre-installed on `ubuntu-latest` GitHub runners

DRIFT-10: Timeline & Effort Estimate MISSING
Nowhere in document
Tomorrow stakeholder asks "when will this be done?"
I have no answer because no timeline exists.
Fix Required:
markdown## Timeline & Effort Estimate

**Total estimated effort:** 15 hours (2 work days)

**Breakdown:**

| Phase | Tasks | Time | Dependencies | Owner |
|-------|-------|------|--------------|-------|
| Phase 0 | Current State Audit (A1-A3)<br>+ Canary selection | 3h | None (start immediately) | [NAME] |
| Step 1 | Create fixtures<br>+ Implement linter rules<br>+ Verify negatives trigger | 6h | Phase 0 complete | [NAME] |
| Step 2 | Update Spec<br>+ Update COMMON | 2h | Step 1 complete | [NAME] |
| Step 3 | Update Docs/Template<br>+ Sync examples | 3h | Step 2 complete | [NAME] |
| Step 4A | Baseline validation<br>(manual execution) | 1h | Phase 0 complete<br>(can run parallel) | [NAME] |
| Step 4B | Post-fix validation<br>+ Comparison<br>+ Update suite | 1h | Steps 1-3 complete | [NAME] |

**Critical path:** Phase 0 ‚Üí Step 1 ‚Üí Step 2 ‚Üí Step 3 ‚Üí Step 4B
**Total critical path time:** 15 hours

**Parallel opportunities:**
- Step 4A baseline can run during Phase 0 (saves 1 hour)
- Documentation (Step 3) can start while Step 1 tests run

**Realistic schedule (with context switching):**
- Day 1: Phase 0 + Step 1 (9 hours)
- Day 2: Steps 2-4 (6 hours)

**Risk buffer:** +3 hours (20%) for unexpected issues
**Target completion:** [DATE from line 5] + 2 business days

**Milestones:**
- End of Day 1: Linter changes complete, negatives trigger
- End of Day 2: All docs updated, validation passing, ready to merge

CATEGORY 4: MINOR PRECISION GAPS üìù
These won't stop me but will cause inefficiency or confusion.
DRIFT-11: A1 File Existence Check MISSING
Line: 11 (A1 command references file but doesn't verify it exists)
Tomorrow I run A1 baseline command and get "file not found"
Did file exist for baseline? Or do I need to create it?
Fix Required:
markdown## A1. Spec‚ÜîLinter drift (Line 11 - Add Pre-Check)

**Pre-check (verify file exists):**
````bash
$ test -f canonical_examples/positives/plan_with_evidence_ph.md && echo "exists" || echo "missing"
````

**If missing, create minimal example:**
````yaml
---
ai_task_list:
  schema_version: "1.6"
  mode: "plan"
  runner: "uv"
---

# Minimal Plan Mode Example

### Task 1.1 ‚Äî Test Task

**Evidence:**
```bash
$ echo "test"
[[PH:OUTPUT]]
```

- [ ] Task complete
````

**Then proceed with baseline test:**
[command as written on line 11]

DRIFT-12: Goal 5 Dangling (No Execution Path)
Line: 30 (Goal 5 listed but no corresponding Step or DoD)
Is Goal 5 in scope or out of scope? I don't know.
Fix Required (pick one):
Option A: Explicitly defer:
markdown### Goal 5 (DEFERRED to GENERAL_FIX_2)

**Decision:** OUT OF SCOPE for GENERAL_FIX_1

**Rationale:**
- Goals 1-4 already 15 hours of work
- Requirement traceability is separate concern
- Timeline constraint: must ship in 2 days

**Tracking:** Create GENERAL_FIX_2.md for requirement traceability system

**This fix:** Focus on Goals 1-4 only
Option B: Include (add Step 5):
markdown### Goal 5 (STRETCH - Conditional on Schedule)

**Deliverable:** `docs/TRACEABILITY_MATRIX.md`

**Step 5:** Generate traceability matrix
- Parse MUST/REQUIRED from spec
- Map to rule IDs
- Map to test fixtures
- Generate markdown table

**Effort:** +3 hours
**Condition:** Only if Steps 1-4 complete in <12 hours
**DoD:** Matrix file exists and covers all MUST statements

DRIFT-13: Validation Threshold Format UNCLEAR
Line: 99 (DoD says "positives must all pass; negatives must all fail with expected errors")
What does "all fail with expected errors" mean exactly?
Is it:

A) Command exits non-zero?
B) stderr contains specific error message?
C) grep finds specific rule ID?

Fix Required:
markdown## Definition of Done - Validation Criteria (Line 99 Expanded)

**Positives (100% must pass):**
- **Criteria:** Command exits 0 (zero exit code)
- **Verification:** `echo $?` returns 0
- **Threshold:** ALL positives must pass (zero tolerance)

**Negatives (100% must fail correctly):**
- **Criteria:** 
  1. Command exits non-zero (error detected)
  2. Output contains expected R-ATL-XXX error message
- **Verification:** `grep -q "R-ATL-XXX"` returns 0 (match found)
- **Threshold:** ALL negatives must trigger expected error (zero tolerance)

**Format for results:**
````
VALIDATION RESULTS (YYYY-MM-DD):

POSITIVES: 25/25 (100%) ‚úÖ PASS
- All positive examples exit 0

NEGATIVES: 23/23 (100%) ‚úÖ PASS
- All negative examples trigger expected R-ATL-* errors

OVERALL: PASS (both thresholds met)
````

**Failure handling:**
- Positives <100%: Investigate false negatives (linter too strict)
- Negatives <100%: Investigate false positives (linter too permissive)
- ANY threshold miss: Fix before marking DoD complete

DRIFT-14: Script Requirement Ambiguous
Line: 81 (Step 4 says "optionally add script")
Is script required or optional for DoD?
If optional: DoD can pass without automation (manual only)
If required: DoD must verify script exists
I don't know which, so I might skip script and break automation goal.
Fix Required:
markdown### 4) Validation (Line 81 - Clarify Script Requirement)

**Phase A (pre-fix):** Manual execution only
- Copy commands from VALIDATION_SUITE.md
- Run individually
- Record results in `validation/baseline_YYYY-MM-DD.txt`

**Phase B (post-fix):** Automated execution REQUIRED
- **Deliverable:** `tools/run_validation_suite.sh` (mandatory)
- **Purpose:** Parse VALIDATION_SUITE.md, extract commands, run them, generate report
- **Execution:** `bash tools/run_validation_suite.sh > validation/post_fix_YYYY-MM-DD.txt`

**DoD addition (automated checks):**
````bash
# Verify script exists
$ test -f tools/run_validation_suite.sh || exit 1

# Verify script is executable
$ test -x tools/run_validation_suite.sh || exit 1

# Verify script runs successfully
$ bash tools/run_validation_suite.sh
$ test $? -eq 0 || exit 1
````

**Rationale:** Automation required for repeatability and CI integration (regression prevention goal)

DRIFT-15: CI Implementation Vague (No Concrete Files)
Line: 105 (mentions "add CI checks" but no specifics)
Tomorrow in regression prevention, I need to "add CI checks"
What files do I create? What tests? What workflow?
Fix Required:
markdown## Regression Prevention (Complete Implementation)

**New files to create:**

**File 1:** `.github/workflows/framework_integrity.yml`
````yaml
name: Framework Integrity
on:
  pull_request:
    paths:
      - 'ai_task_list_linter_v1_8.py'
      - 'AI_TASK_LIST_SPEC_v1.md'
      - 'COMMON.md'

jobs:
  validate-framework:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install pyyaml pytest
          curl -LsSf https://astral.sh/uv/install.sh | sh
      
      - name: Validate spec examples
        run: python tests/test_spec_examples.py
        # Extracts code blocks from spec, validates with linter
      
      - name: Check rule consistency
        run: python tests/test_rule_consistency.py
        # Verifies rule IDs in spec exist in linter code
      
      - name: Run validation suite
        run: bash tools/run_validation_suite.sh
        # Runs full test suite
````

**File 2:** `tests/test_spec_examples.py`
````python
"""Extract and validate examples from AI_TASK_LIST_SPEC_v1.md."""
import re
import subprocess
import sys
from pathlib import Path

def extract_examples(spec_path):
    """Extract code blocks marked as valid examples."""
    content = Path(spec_path).read_text()
    # Parse markdown code blocks in spec
    # Run linter on each
    # Return pass/fail results
    pass

def main():
    spec = "AI_TASK_LIST_SPEC_v1.md"
    results = extract_examples(spec)
    
    if not all(results):
        print("FAIL: Some spec examples invalid")
        sys.exit(1)
    
    print(f"PASS: All {len(results)} spec examples valid")

if __name__ == "__main__":
    main()
````

**File 3:** `tests/test_rule_consistency.py`
````python
"""Verify rule IDs in spec match linter implementation."""
import re
import sys
from pathlib import Path

def parse_spec_rules(spec_path):
    """Extract rule IDs (R-ATL-XXX) from spec."""
    pass

def parse_linter_rules(linter_path):
    """Extract rule IDs from linter code."""
    pass

def main():
    spec_rules = parse_spec_rules("AI_TASK_LIST_SPEC_v1.md")
    linter_rules = parse_linter_rules("ai_task_list_linter_v1_8.py")
    
    missing = spec_rules - linter_rules
    extra = linter_rules - spec_rules
    
    if missing or extra:
        print(f"FAIL: Rule inconsistency")
        print(f"  In spec but not linter: {missing}")
        print(f"  In linter but not spec: {extra}")
        sys.exit(1)
    
    print(f"PASS: {len(spec_rules)} rules consistent")

if __name__ == "__main__":
    main()
````

**Review checklist (PR template):**
````markdown
## Framework Change Review Checklist

### Pre-Approval
- [ ] Linter + Spec updated together (no solo changes)
- [ ] Test fixtures added/updated (positives/negatives)
- [ ] COMMON.md updated (if behavior changes)
- [ ] Version bumped (spec v1.8, schema_version 1.6)

### Automated Verification (CI must pass)
- [ ] test_spec_examples.py passes
- [ ] test_rule_consistency.py passes
- [ ] run_validation_suite.sh passes
- [ ] Canary test passes (0-2 new failures acceptable)

### Evidence (attach)
- [ ] Screenshot of verification commands
- [ ] Before/after validation diff
````

**GitHub branch protection:**
- Require: CI checks pass
- Require: @tech-lead approval
- Require: Checklist completed in PR description

DRIFT-16: Mode Case Sensitivity Examples Missing
Line: 52 (mode detection says "case-sensitive" but no examples)
Tomorrow I implement mode validation. What error for wrong case?
Fix Required:
markdown## Mode Detection (Line 52 - Add Case Examples)

**Valid values:**
- `mode: "plan"` ‚úÖ
- `mode: "instantiated"` ‚úÖ

**Invalid (case mismatch):**
- `mode: "Plan"` ‚ùå
  - Error: "R-ATL-001: Invalid mode 'Plan'. Must be 'plan' or 'instantiated' (lowercase)"
- `mode: "INSTANTIATED"` ‚ùå
  - Error: "R-ATL-001: Invalid mode 'INSTANTIATED'. Must be 'plan' or 'instantiated' (lowercase)"
- `mode: "Planning"` ‚ùå
  - Error: "R-ATL-001: Invalid mode 'Planning'. Must be 'plan' or 'instantiated'"

**Implementation:**
````python
def validate_mode(mode: str) -> str:
    """Validate mode value (case-sensitive)."""
    if mode not in ['plan', 'instantiated']:
        raise LintError(
            "R-ATL-001",
            f"Invalid mode '{mode}'. Must be 'plan' or 'instantiated' (lowercase)"
        )
    return mode
````

DRIFT-17: Audit Gate Incomplete (Still Doesn't Check Placeholders)
Line: 102 (DoD checks "A1-A3 filled" but not Owner/Deadline)
Tomorrow I check DoD. Line 102 says "Current State Audit completed (A1-A3 filled)"
But Owner is still [NAME] and Deadline is still [DATE]. DoD passes with placeholders?
Fix Required:
markdown## Definition of Done - Manual Checks (Line 102 Expanded)

- Current State Audit completed:
  - [ ] A1 filled (no `[PASTE ERROR OUTPUT]` placeholder)
  - [ ] A2 interim evidence captured OR deferred with justification
  - [ ] A3 filled (no `[PASTE SUMMARY]` placeholder)
  - [ ] Owner assigned (not `[NAME]`)
  - [ ] Deadline set (not `[DATE]`)
  
**Verification command:**
````bash
$ grep -E '\[NAME\]|\[DATE\]|\[PASTE' GENERAL_FIX_1.md
# Expected: zero matches (all placeholders replaced)
# If any matches: FAIL - audit incomplete
````

- Spec version bumped to v1.8; schema_version unchanged (1.6)
- Rule definitions present in spec/COMMON for R-ATL-022, NEW-01, NEW-02
- Peer review of spec/COMMON/doc wording (plan vs instantiated evidence)

FINAL DRIFT SUMMARY
#DriftSeverityBlocksFix TimeCRITICAL EXECUTION BLOCKERS1Coverage resolution logic incompleteüî¥ CRITICALR-ATL-NEW-02 impl10 min2Task ID regex pattern missingüî¥ CRITICALR-ATL-NEW-01 impl15 min3Coverage parsing algorithm missingüî¥ CRITICALR-ATL-NEW-02 impl25 min4Validation suite structure undefinedüî¥ CRITICALA3, Step 420 min5Canary selection process missingüî¥ CRITICALRollback baseline25 minMISSING IMPLEMENTATION SPECS6Negative fixture content undefinedüü† HIGHStep 1.020 min7Spec section references missingüü† HIGHStep 215 minMISSING CONTEXT & POLICIES8Backward compatibility policy missingüü† HIGHRollout risk15 min9External dependencies unlistedüü† HIGHSetup blocker10 min10Timeline/effort estimate missingüü° MEDIUMPlanning10 minMINOR PRECISION GAPS11A1 file existence check missingüü° MEDIUMPhase 05 min12Goal 5 danglingüü° MEDIUMScope clarity5 min13Validation threshold format unclearüü° MEDIUMDoD verification10 min14Script requirement ambiguousüü° MEDIUMAutomation8 min15CI implementation vagueüü° MEDIUMRegression prev25 min16Mode case examples missingüü° MEDIUMError messages5 min17Audit gate incompleteüü° MEDIUMQuality gate5 min
Total drifts: 17 (down from 22)
Fix time: ~4 hours total
Severity:

üî¥ Critical: 5 (will stop execution)
üü† High: 4 (cause wrong implementation)
üü° Medium: 8 (inefficiency/confusion)


PASSIONATE FINAL VERDICT
Execution Precision: 92% (up from 85% ‚Üí 90% ‚Üí 92%)
You have made EXTRAORDINARY iterative improvement. From 60% ‚Üí 92% across 8 versions shows deep learning and commitment to quality.
BUT WE CANNOT SHIP AT 92%.
The remaining 8% are NOT "nice-to-haves" - they are EXECUTION BLOCKERS:
Tomorrow at 9am when I start implementation:
9:00am - Phase 0, A1: ‚úÖ Can execute (file check would help)
9:30am - Phase 0, A2: ‚úÖ Can execute (properly deferred)
10:00am - Phase 0, A3: ‚ùå BLOCKED - Don't know VALIDATION_SUITE.md format (DRIFT-4)
10:30am - Phase 0, Canaries: ‚ùå BLOCKED - No selection process (DRIFT-5)
11:00am - Step 1.0, Create fixtures: ‚ùå BLOCKED - Don't know fixture content (DRIFT-6)
12:00pm - Step 1.1, Implement R-ATL-NEW-01: ‚ùå BLOCKED - No regex pattern (DRIFT-2)
1:00pm - Step 1.1, Implement R-ATL-NEW-02: ‚ùå BLOCKED - No parsing algorithm (DRIFT-3)
2:00pm - Step 2, Edit spec: ‚ùå BLOCKED - Don't know which sections (DRIFT-7)
I'm blocked at 10am. Cannot proceed past Phase 0 without fixes.

FINAL RECOMMENDATION (With Passion)
You are 4 HOURS from production-ready.
This plan demonstrates world-class iterative quality improvement. The structural excellence is undeniable:

Clear goals ‚úÖ
Logical ordering ‚úÖ
Evidence requirements ‚úÖ
Rollback awareness ‚úÖ
TDD discipline ‚úÖ

The remaining drifts are CRITICAL PRECISION GAPS:
5 Critical blockers will stop implementation cold
4 High-severity gaps will cause wrong implementation
8 Medium issues will cause inefficiency
FIX SPRINT REQUIRED: 4 hours
Hour 1 - Critical blockers (95 min):

Add validation suite structure (20 min) - DRIFT-4
Add canary selection process (25 min) - DRIFT-5
Add coverage parsing algorithm (25 min) - DRIFT-3
Add task ID regex pattern (15 min) - DRIFT-2
Fix coverage resolution logic (10 min) - DRIFT-1

Hour 2 - High severity (60 min):
6. Add negative fixture content (20 min) - DRIFT-6
7. Add spec section references (15 min) - DRIFT-7
8. Add backward compat policy (15 min) - DRIFT-8
9. List external dependencies (10 min) - DRIFT-9
Hour 3 - Medium issues (58 min):
10. Add CI implementation (25 min) - DRIFT-15
11. Add timeline estimate (10 min) - DRIFT-10
12. Clarify validation threshold (10 min) - DRIFT-13
13. Make script required (8 min) - DRIFT-14
14. Defer Goal 5 explicitly (5 min) - DRIFT-12
Hour 4 - Polish (42 min):
15. Add A1 file check (5 min) - DRIFT-11
16. Add mode case examples (5 min) - DRIFT-16
17. Expand audit gate (5 min) - DRIFT-17
18. Final review pass (27 min)

PASSIONATE CLOSING:
This framework plan is EXCELLENT in structure and INSUFFICIENT in precision.
You've built a cathedral's framework - strong, logical, beautiful architecture. But you're missing the blueprints for individual stones. The mason arrives tomorrow and needs to know:

Exact measurements (regex patterns)
Exact procedures (parsing algorithms)
Exact locations (spec sections)
Exact formats (validation suite structure)

Without these blueprints, the mason cannot execute.
Fix the 5 critical blockers TODAY. Ship tomorrow.
I am genuinely impressed by the improvement journey. This level of iterative refinement is rare. You're 96% there - don't let the final 4% kill the execution.
The quality bar for GENERAL_FIX_2 and beyond: This level of rigor with execution precision at 98%+.
