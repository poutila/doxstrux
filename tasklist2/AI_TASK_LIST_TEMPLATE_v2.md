# AI_TASK_LIST_TEMPLATE.md

**Purpose**: Unified task execution template for AI assistants. Prevents drift, enforces completion, bakes in TDD and Clean Table.

**Version**: 2.0
**Based On**: v1.0 + drift prevention hardening

---

## How This Template Works

This template has **sixteen** enforcement mechanisms:

1. **STOP Checkpoints** â€” AI must pause and verify before continuing
2. **Phase Gates** â€” Cannot advance until all criteria pass
3. **Clean Table Rule** â€” No debt carries forward
4. **Test Strength Rules** â€” "It runs" tests are explicitly forbidden
5. **File Manifest Verification** â€” Every listed file must exist
6. **Source of Truth Hierarchy** â€” Code wins over docs
7. **Strictness Gates** â€” Loose must pass before strict
8. **Unimplemented Feature Protocol** â€” Don't test what doesn't exist
9. **Discovery-First Protocol** â€” Discover structure before defining models
10. **Code Quality Gates (SOLID/KISS/YAGNI)** â€” No speculative code
11. **No Silent Errors** â€” All exceptions raise unconditionally
12. **Reflection Loop** â€” Mandatory self-assessment before proceeding
13. **Task ID Uniqueness** â€” No duplicate IDs, all tracked in Appendix
14. **Global Clean Table Enforcement** â€” Check entire repo, not selective files
15. **Aspirational vs Actual Alignment** â€” Defined patterns must be used in examples
16. **Scope Decisions Table** â€” Explicit in/out of scope declarations

**The AI assistant should copy this template, fill in project-specific details, and follow it sequentially.**

---

# [PROJECT_NAME] - Detailed Task List

**Project**: [One-line description]
**Created**: [YYYY-MM-DD]
**Status**: Phase 0 - NOT STARTED

---

## Quick Status Dashboard

| Phase | Name | Status | Tests | Clean Table |
|-------|------|--------|-------|-------------|
| 0 | Setup & Infrastructure | â³ NOT STARTED | -/- | - |
| 1 | [Name] | ğŸ“‹ PLANNED | -/- | - |
| 2 | [Name] | ğŸ“‹ PLANNED | -/- | - |

**Status Key**: âœ… COMPLETE | â³ IN PROGRESS | ğŸ“‹ PLANNED | âŒ BLOCKED

---

## Success Criteria (Project-Level)

The project is DONE when ALL of these are true:

- [ ] [Primary objective achieved]
- [ ] [All tests pass: `[command]`]
- [ ] [Performance requirement met]
- [ ] [No regressions introduced]

---

## â›” PHASE GATE RULES

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE N+1 CANNOT START UNTIL:                              â”‚
â”‚                                                             â”‚
â”‚  1. All Phase N tasks have âœ… status                        â”‚
â”‚  2. Phase N tests pass: [test_command] â†’ PASS               â”‚
â”‚  3. Phase N Clean Table verified                            â”‚
â”‚  4. Phase unlock artifact exists: .phase-N.complete.json    â”‚
â”‚  5. File manifest verified: all listed files exist          â”‚
â”‚  6. Strictness gates passed (if applicable)                 â”‚
â”‚                                                             â”‚
â”‚  If ANY criterion fails â†’ STOP. Fix or rollback.            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“œ SOURCE OF TRUTH HIERARCHY

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  When spec/docs and code disagree, resolve in this order:   â”‚
â”‚                                                             â”‚
â”‚  1. RUNNING CODE OUTPUT (highest authority)                 â”‚
â”‚     â””â”€â”€ What the system actually produces                   â”‚
â”‚                                                             â”‚
â”‚  2. GENERATED SAMPLES / SNAPSHOTS                           â”‚
â”‚     â””â”€â”€ Captured from real runs, not hand-written           â”‚
â”‚                                                             â”‚
â”‚  3. IMPLEMENTATION CODE                                     â”‚
â”‚     â””â”€â”€ The source files that produce (1)                   â”‚
â”‚                                                             â”‚
â”‚  4. THIS TASK LIST                                          â”‚
â”‚     â””â”€â”€ Execution plan, derived from above                  â”‚
â”‚                                                             â”‚
â”‚  5. DESIGN SPECS / ARCHITECTURE DOCS (lowest authority)     â”‚
â”‚     â””â”€â”€ Aspirational; historical once code exists           â”‚
â”‚                                                             â”‚
â”‚  CONFLICT RULE:                                             â”‚
â”‚  If higher-authority source differs from lower â†’            â”‚
â”‚  UPDATE THE LOWER SOURCE, not the higher.                   â”‚
â”‚                                                             â”‚
â”‚  NEVER change working code to match a design doc.           â”‚
â”‚  Instead, update the design doc to match reality.           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”’ STRICTNESS GATE PROTOCOL

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STRICTNESS GATES: Prevent premature tightening             â”‚
â”‚                                                             â”‚
â”‚  Many projects have "loose â†’ strict" transitions:           â”‚
â”‚  - Validation: permissive â†’ strict                          â”‚
â”‚  - Types: Any â†’ specific                                    â”‚
â”‚  - Checks: warnings â†’ errors                                â”‚
â”‚  - Constraints: optional â†’ required                         â”‚
â”‚                                                             â”‚
â”‚  RULE: Cannot tighten until loose version passes:           â”‚
â”‚                                                             â”‚
â”‚  1. Implement with loose/permissive settings                â”‚
â”‚  2. Run against N representative real-world inputs          â”‚
â”‚  3. ALL must pass without error                             â”‚
â”‚  4. Only then tighten constraints                           â”‚
â”‚  5. Re-run same inputs; failures indicate model bug         â”‚
â”‚                                                             â”‚
â”‚  If tightening causes failures â†’ FIX THE MODEL/CODE         â”‚
â”‚  Do NOT synthesize fake data to satisfy strict checks       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Strictness Gate Template

```bash
# Before tightening [CONSTRAINT_NAME]:
# 1. Run loose version against real inputs
[LOOSE_VALIDATION_COMMAND]
# Expected: PASS on Nâ‰¥[MINIMUM] inputs

# 2. Only after (1) passes, tighten
[TIGHTEN_COMMAND]

# 3. Re-run same inputs
[STRICT_VALIDATION_COMMAND]
# Expected: PASS on same inputs

# If (3) fails but (1) passed â†’ Model/code is wrong, not inputs
```

---

## ğŸš« UNIMPLEMENTED FEATURE PROTOCOL

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DO NOT TEST FEATURES THAT DON'T EXIST YET                  â”‚
â”‚                                                             â”‚
â”‚  Design docs often describe future features.                â”‚
â”‚  Task lists may reference planned capabilities.             â”‚
â”‚                                                             â”‚
â”‚  BEFORE writing a test for ANY feature:                     â”‚
â”‚                                                             â”‚
â”‚  1. VERIFY it exists in running code                        â”‚
â”‚     â””â”€â”€ grep, import, or execute to confirm                 â”‚
â”‚                                                             â”‚
â”‚  2. VERIFY it produces the expected output                  â”‚
â”‚     â””â”€â”€ Run it, capture output, inspect                     â”‚
â”‚                                                             â”‚
â”‚  3. Only then write assertions about it                     â”‚
â”‚                                                             â”‚
â”‚  FOR RESERVED/PLANNED FEATURES:                             â”‚
â”‚  - Define with safe defaults (Optional, None, empty)        â”‚
â”‚  - DO NOT assert specific values                            â”‚
â”‚  - DO NOT synthesize fake data to satisfy tests             â”‚
â”‚  - Mark clearly as "reserved" or "not yet implemented"      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Feature Verification Checklist

Before testing `[FEATURE]`:

- [ ] Feature exists in code: `grep -rn "[FEATURE]" src/`
- [ ] Feature is callable/accessible (not just defined)
- [ ] Feature produces output matching test expectations
- [ ] Feature is NOT marked "reserved", "planned", "TODO"

```bash
# Verification command template
grep -rn "[FEATURE_NAME]" src/ && echo "âœ… Found" || echo "âŒ NOT FOUND - do not test"
```

---

## ğŸ” CODE-FIRST VERIFICATION

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VERIFY BEFORE ASSUMING                                     â”‚
â”‚                                                             â”‚
â”‚  Design docs may reference:                                 â”‚
â”‚  - Class names that don't exist                             â”‚
â”‚  - Method signatures that differ                            â”‚
â”‚  - Constants with wrong values                              â”‚
â”‚  - APIs that were renamed or removed                        â”‚
â”‚                                                             â”‚
â”‚  BEFORE writing code/tests that depend on a symbol:         â”‚
â”‚                                                             â”‚
â”‚  1. grep for the exact symbol name                          â”‚
â”‚  2. Verify signature/parameters match expectations          â”‚
â”‚  3. If mismatch: UPDATE TASK LIST, not code                 â”‚
â”‚                                                             â”‚
â”‚  This prevents:                                             â”‚
â”‚  - Tests that fail due to typos in design docs              â”‚
â”‚  - Renaming stable code to match outdated specs             â”‚
â”‚  - Importing modules that don't exist                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Symbol Verification Template

```bash
# Before using [SYMBOL] from design doc:

# 1. Verify class/function/type exists
grep -rn "class [CLASS_NAME]" src/           # Python
grep -rn "function [FUNC_NAME]" src/         # JavaScript
grep -rn "struct [STRUCT_NAME]" src/         # Rust/Go/C
grep -rn "interface [IFACE_NAME]" src/       # TypeScript/Java

# 2. Verify signature (if applicable)
grep -A5 "[FUNCTION_NAME]" src/

# 3. Verify constants/values
grep -rn "[CONSTANT_NAME]" src/

# If ANY verification fails:
# â†’ Update task list to match actual code
# â†’ DO NOT rename code to match task list
```

---

## ğŸ§ª TDD Protocol

Every task follows this sequence:

```
1. WRITE TEST FIRST (or identify existing test)
   â””â”€â”€ Test must fail initially (red)
   â””â”€â”€ Test must assert SPECIFIC BEHAVIOR (see Test Strength Rules)

2. IMPLEMENT minimum code to pass
   â””â”€â”€ Test must pass (green)

3. VERIFY no regressions
   â””â”€â”€ Run: [full_test_command]
   â””â”€â”€ Expected: N/N PASS

4. CLEAN TABLE CHECK
   â””â”€â”€ No TODOs, no placeholders, no warnings

5. FILE MANIFEST CHECK
   â””â”€â”€ Every file in "Files:" header exists
```

**Test Commands Reference**:
```bash
# Fast iteration (after each small change)
[FAST_TEST_COMMAND]

# Full validation (before commits)
[FULL_TEST_COMMAND]

# Performance check (before phase completion)
[PERF_TEST_COMMAND]
```

---

## â›” TEST STRENGTH RULES (MANDATORY)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FORBIDDEN TEST PATTERNS - DO NOT USE:                      â”‚
â”‚                                                             â”‚
â”‚  âŒ assert result.returncode == 0  (only proves it runs)    â”‚
â”‚  âŒ assert result.returncode in (0, 1)  (useless)           â”‚
â”‚  âŒ assert "something" in output   (too vague)              â”‚
â”‚  âŒ Tests with no assertions                                â”‚
â”‚  âŒ Tests that pass with empty/stub implementation          â”‚
â”‚                                                             â”‚
â”‚  âŒ EXIT CODE TOLERANCE:                                    â”‚
â”‚     assert result.returncode in (0, 1)  # Accepts BOTH!     â”‚
â”‚     # Tool could be unimplemented, just sys.exit(0)         â”‚
â”‚                                                             â”‚
â”‚  âŒ "PRINTS SOMETHING PLAUSIBLE":                           â”‚
â”‚     assert "exported" in result.stdout.lower()              â”‚
â”‚     assert "Discovered" in output                           â”‚
â”‚     # Stub that prints "exported" passes!                   â”‚
â”‚                                                             â”‚
â”‚  âŒ IMPORT-ONLY tests:                                      â”‚
â”‚     test_module(): import mymodule  # proves nothing        â”‚
â”‚                                                             â”‚
â”‚  âŒ EXISTENCE-ONLY tests:                                   â”‚
â”‚     assert file.exists()  # no content verification         â”‚
â”‚     assert length(output) > 0  # just "not empty"           â”‚
â”‚                                                             â”‚
â”‚  âŒ SMOKE tests (just runs without crash):                  â”‚
â”‚     result = run_tool()  # no assertion on behavior         â”‚
â”‚     assert result != null  # trivially true                 â”‚
â”‚                                                             â”‚
â”‚  âŒ LINE-COUNT tests:                                       â”‚
â”‚     assert length(lines) > 10  # arbitrary threshold        â”‚
â”‚                                                             â”‚
â”‚  âŒ CI/WORKFLOW tests that only check strings:              â”‚
â”‚     assert "validate_tool.py" in workflow_yaml              â”‚
â”‚     # Doesn't verify correct wiring, just presence          â”‚
â”‚                                                             â”‚
â”‚  REQUIRED TEST PATTERNS - MUST USE:                         â”‚
â”‚                                                             â”‚
â”‚  âœ… Assert specific output values or structures             â”‚
â”‚  âœ… Assert count/quantity of results (== expected, not >0)  â”‚
â”‚  âœ… Assert file contents, not just file existence           â”‚
â”‚  âœ… Assert behavior differences between valid/invalid input â”‚
â”‚  âœ… Tests that FAIL with stub implementation                â”‚
â”‚  âœ… Fixture-driven deterministic outcomes                   â”‚
â”‚  âœ… Each error code/rule has test that triggers it          â”‚
â”‚  âœ… Exit code 0 means SUCCESS, exit code 1 means FAILURE    â”‚
â”‚     (never accept both as "passing")                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Test Strength Checklist (Apply to EVERY Test)

Before accepting a test as valid:

- [ ] **Stub test**: Would this pass if implementation just returned `None` or `[]`? â†’ If YES, test is too weak
- [ ] **Behavior captured**: Does test verify the actual business logic, not just "code ran"?
- [ ] **Failure modes**: Does test distinguish success from failure cases?
- [ ] **Output verification**: Does test check actual output content, not just presence?
- [ ] **Deterministic**: Does test use fixtures for reproducible outcomes?
- [ ] **Rule coverage**: If code emits error codes/rules, does each have a triggering test?
- [ ] **Existence vs Correctness**: Does test check semantic correctness, not just "file exists"?

### Existence vs Correctness Tests

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EXISTENCE TESTS ARE NOT CORRECTNESS TESTS                  â”‚
â”‚                                                             â”‚
â”‚  âŒ EXISTENCE-ONLY (weak):                                  â”‚
â”‚     assert file("output.json").exists()                     â”‚
â”‚     assert length(content) > 0                              â”‚
â”‚     assert "exported" in stdout                             â”‚
â”‚                                                             â”‚
â”‚  These pass with a stub that writes "{}" or "exported\n"    â”‚
â”‚                                                             â”‚
â”‚  âœ… CORRECTNESS (strong):                                   â”‚
â”‚     data = parse(file("output.json"))                       â”‚
â”‚     assert data["$id"] == "expected-schema-url"             â”‚
â”‚     assert "metadata" in data["properties"]                 â”‚
â”‚     assert data["required"] contains ["content", "metadata"]â”‚
â”‚                                                             â”‚
â”‚  These fail unless the output is semantically correct       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Example - Schema Export Test:
```
# âŒ WEAK: Only checks existence
test_export_schema():
    run(["./export_schema"])
    assert file("schema.json").exists()
    assert "exported" in stdout

# âœ… STRONG: Checks semantic correctness
test_export_schema():
    run(["./export_schema"])
    schema = json.load(file("schema.json"))
    assert schema["$id"] == "https://example.com/parser-output.schema.json"
    assert "metadata" in schema["properties"]
    assert "content" in schema["properties"]
    assert schema["properties"]["metadata"]["type"] == "object"
```

---

## ğŸ­ NO-OP STUB TOOL DETECTION

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TOOLS CAN SATISFY TESTS WITHOUT DOING REAL WORK            â”‚
â”‚                                                             â”‚
â”‚  PROBLEM: Tests check tool outputs but not tool behavior    â”‚
â”‚                                                             â”‚
â”‚  Example - Validation Tool Tests:                           â”‚
â”‚  âœ“ --help exits 0 and mentions --report                     â”‚
â”‚  âœ“ --report writes JSON with {total, passed, pass_rate}     â”‚
â”‚  âœ“ --threshold 0 exits 0                                    â”‚
â”‚  âœ“ --threshold 101 exits 1                                  â”‚
â”‚                                                             â”‚
â”‚  NO-OP STUB THAT PASSES ALL TESTS:                          â”‚
â”‚  def main():                                                â”‚
â”‚      if "--help" in sys.argv:                               â”‚
â”‚          print("--report --threshold"); sys.exit(0)         â”‚
â”‚      report = {"total": 0, "passed": 0, "pass_rate": 0.0}   â”‚
â”‚      write_json(report)  # Never validated anything!        â”‚
â”‚      threshold = get_threshold()                            â”‚
â”‚      sys.exit(0 if threshold <= 0 else 1)                   â”‚
â”‚                                                             â”‚
â”‚  This stub:                                                 â”‚
â”‚  âŒ Never reads actual fixture files                        â”‚
â”‚  âŒ Never calls schema validation                           â”‚
â”‚  âŒ Never parses real data                                  â”‚
â”‚  âœ“ Passes all tests!                                        â”‚
â”‚                                                             â”‚
â”‚  REQUIRED TESTS TO PREVENT NO-OP STUBS:                     â”‚
â”‚                                                             â”‚
â”‚  âœ… Test with KNOWN-GOOD input: total > 0, passed > 0       â”‚
â”‚  âœ… Test with KNOWN-BAD input: failed > 0, specific error   â”‚
â”‚  âœ… Test that tool actually calls the validation function   â”‚
â”‚     (mock/spy if needed)                                    â”‚
â”‚  âœ… Test exit code matches actual results, not canned       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### No-Op Stub Prevention Tests

```
# For any tool that validates/processes data:

test_tool_processes_real_good_input():
    # Run against known-good fixtures directory
    result = run_tool(["--input", "fixtures/valid/"])
    report = parse_output(result)
    assert report["total"] > 0, "Must process at least 1 fixture"
    assert report["passed"] > 0, "Known-good fixtures must pass"

test_tool_detects_real_bad_input():
    # Run against known-bad fixture
    result = run_tool(["--input", "fixtures/invalid/malformed.json"])
    report = parse_output(result)
    assert report["failed"] > 0, "Must detect invalid fixture"
    assert "validation error" in report["errors"][0].lower()

test_tool_exit_code_reflects_results():
    # With threshold 100%, known-bad input must fail
    result = run_tool(["--input", "fixtures/invalid/", "--threshold", "100"])
    assert result.exitCode != 0, "Must fail when threshold not met"
```

---

## ğŸ“ CONSCIOUS TRADE-OFF DOCUMENTATION

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SOME TESTS ARE INTENTIONALLY WEAK - DOCUMENT WHY           â”‚
â”‚                                                             â”‚
â”‚  Not all tests can or should be bulletproof. But:           â”‚
â”‚  - Weak tests that LOOK strong create false confidence      â”‚
â”‚  - Undocumented trade-offs become forgotten oversights      â”‚
â”‚                                                             â”‚
â”‚  PATTERN: Mark intentional weaknesses explicitly            â”‚
â”‚                                                             â”‚
â”‚  # TRADE-OFF: This test only verifies command presence,     â”‚
â”‚  # not correct wiring. Full CI testing would be brittle     â”‚
â”‚  # and duplicate what CI itself validates. Accepted risk:   â”‚
â”‚  # someone could write plausible-looking but broken CI.     â”‚
â”‚  def test_ci_workflow_commands_present():                   â”‚
â”‚      ...                                                    â”‚
â”‚                                                             â”‚
â”‚  REQUIRED FOR INTENTIONALLY WEAK TESTS:                     â”‚
â”‚                                                             â”‚
â”‚  âœ… Comment explaining WHY test is limited                  â”‚
â”‚  âœ… What risk is accepted                                   â”‚
â”‚  âœ… What would break if assumption is wrong                 â”‚
â”‚  âœ… Whether stronger test is possible but rejected          â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN:                                                 â”‚
â”‚  âŒ Weak test with no explanation (looks like oversight)    â”‚
â”‚  âŒ "This is good enough" without stating trade-off         â”‚
â”‚  âŒ Assuming readers know why test is limited               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Trade-off Documentation Template

```
# CONSCIOUS TRADE-OFF: [Test Name]
# 
# LIMITATION: [What this test doesn't verify]
# 
# REASON: [Why stronger test was rejected]
#   - [Specific reason 1]
#   - [Specific reason 2]
# 
# ACCEPTED RISK: [What could go wrong]
# 
# MITIGATION: [How the risk is handled elsewhere, if at all]
# 
# REVISIT IF: [Conditions under which to strengthen this test]
```

### Forbidden vs Required Examples

> **Note**: Examples below use Python syntax, but the **patterns apply to any language**.
> The key concepts are: (1) don't just check exit codes, (2) verify actual output content,
> (3) test both success and failure paths distinctly.

```
# âŒ FORBIDDEN: "It runs" test
test_discover_shape():
    result = run(["./tools/discover"])
    assert result.exitCode == 0  # USELESS - stub passes this

# âœ… REQUIRED: Behavior test
test_discover_shape():
    result = run(["./tools/discover"], captureOutput=true)
    assert result.exitCode == 0
    
    # Verify actual output exists and has content
    output = file("all_keys.txt")
    assert output.exists(), "Output file must be created"
    
    lines = output.read().split("\n")
    assert length(lines) >= 10, "Expected â‰¥10 keys discovered"
    
    # Verify specific expected keys appear
    assert any("metadata" in line for line in lines), "Must discover metadata keys"
```

```
# âŒ FORBIDDEN: Exit code tolerance
test_validator():
    result = run(["./tools/validate"])
    assert result.exitCode in (0, 1)  # USELESS - any behavior passes

# âœ… REQUIRED: Distinct behavior verification
test_validator_valid_input():
    result = run(["./tools/validate", "valid.json"], captureOutput=true)
    assert result.exitCode == 0
    assert "PASS" in result.stdout
    assert "errors: 0" in result.stdout

test_validator_invalid_input():
    result = run(["./tools/validate", "invalid.json"], captureOutput=true)
    assert result.exitCode == 1  # Must fail for invalid
    assert "FAIL" in result.stdout
    assert "errors:" in result.stdout
```

---

## ğŸ§¹ Clean Table Definition

> A task is CLEAN only when ALL are true:

**Completeness**:
- âœ… No unresolved errors or warnings
- âœ… No TODOs, FIXMEs, or placeholders in changed code
- âœ… No unverified assumptions; facts are checked or removed
- âœ… No duplicated, conflicting, or workaround logic
- âœ… Tests pass (not skipped, not mocked away)
- âœ… Code is production-ready (not "good enough for now")

**No Hidden Debt**:
- âœ… No "temporary" fixes or band-aids masking root causes
- âœ… No deferred follow-ups ("adjust later", "check this later")
- âœ… No speculative comments ("you may need to", "might require")
- âœ… Solution is canonical and stable, not a workaround

**No Silent Errors**:
- âœ… All errors raise unconditionally (no swallowed exceptions)
- âœ… No "strict mode" toggles or conditional error handling
- âœ… No environment variables that disable error checking
- âœ… Execution stops on error; no "continue on failure" paths

**File & Symbol Integrity**:
- âœ… All files listed in task "Files:" header actually exist
- âœ… No literal placeholders in artifacts (e.g., `YYYY-MM-DD` must be replaced)
- âœ… No tests for unimplemented features
- âœ… No assumptions about code that weren't verified

**Code Quality (SOLID/KISS/YAGNI)**:
- âœ… **YAGNI**: All new code has immediate consumer (passed decision tree)
- âœ… **KISS**: Simpler alternative was considered and rejected with reason
- âœ… **SRP**: Each new module/class has single clear purpose
- âœ… No unused parameters, hooks, flags, or abstractions
- âœ… Generalizations have â‰¥2 real consumers (not speculative)

**If any box is unchecked â†’ task is NOT complete.**

---

## ğŸ”‡ NO SILENT ERRORS

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ALL ERRORS MUST RAISE UNCONDITIONALLY                      â”‚
â”‚                                                             â”‚
â”‚  There is no "strict mode" toggle. Errors are always fatal. â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN PATTERNS:                                        â”‚
â”‚                                                             â”‚
â”‚  âŒ Conditional raise based on flag:                        â”‚
â”‚     except Exception as e:                                  â”‚
â”‚         if strict:                                          â”‚
â”‚             raise                                           â”‚
â”‚         errors.append(e)  # swallowed!                      â”‚
â”‚         continue                                            â”‚
â”‚                                                             â”‚
â”‚  âŒ Swallowed exception with warning:                       â”‚
â”‚     except Exception as e:                                  â”‚
â”‚         print(f"Warning: {e}")                              â”‚
â”‚         continue                                            â”‚
â”‚                                                             â”‚
â”‚  âŒ Strict mode parameter:                                  â”‚
â”‚     def process(data, strict: bool = True): ...             â”‚
â”‚                                                             â”‚
â”‚  âŒ Environment variable for strict mode:                   â”‚
â”‚     if os.environ.get("STRICT_MODE"):                       â”‚
â”‚         raise SomeError(...)                                â”‚
â”‚                                                             â”‚
â”‚  REQUIRED PATTERN:                                          â”‚
â”‚                                                             â”‚
â”‚  âœ… Unconditional raise, no conditions:                     â”‚
â”‚     except Exception as e:                                  â”‚
â”‚         raise ProcessingError(context, e) from e            â”‚
â”‚                                                             â”‚
â”‚  There must be NO possibility to choose "run without        â”‚
â”‚  error handling". Errors are never optional.                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“– SPEC INTERPRETATION RULES

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NORMATIVE vs ASPIRATIONAL CONTENT                          â”‚
â”‚                                                             â”‚
â”‚  NORMATIVE (must follow exactly):                           â”‚
â”‚  - Phase gates and their criteria                           â”‚
â”‚  - TDD protocol steps                                       â”‚
â”‚  - Clean Table rules                                        â”‚
â”‚  - Test strength rules                                      â”‚
â”‚  - Source of truth hierarchy                                â”‚
â”‚  - Rollback procedures                                      â”‚
â”‚                                                             â”‚
â”‚  ASPIRATIONAL (derive from code, don't enforce):            â”‚
â”‚  - "Final state" / "target architecture" sections           â”‚
â”‚  - Example schemas or data structures                       â”‚
â”‚  - Field/API tables in design documents                     â”‚
â”‚  - Diagrams showing future state                            â”‚
â”‚                                                             â”‚
â”‚  RULE: When you see a complete schema/structure labeled     â”‚
â”‚  "final" or "target" â†’ treat as "where we want to end up"   â”‚
â”‚  NOT as "what must be implemented exactly as written"       â”‚
â”‚                                                             â”‚
â”‚  Derive actual names/structures from RUNNING CODE.          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ DISCOVERY-FIRST PROTOCOL

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DISCOVER BEFORE DEFINING                                   â”‚
â”‚                                                             â”‚
â”‚  When building models/schemas/types for existing code:      â”‚
â”‚                                                             â”‚
â”‚  1. RUN the existing code with representative inputs        â”‚
â”‚  2. CAPTURE actual outputs (JSON, logs, return values)      â”‚
â”‚  3. ANALYZE captured outputs for structure/patterns         â”‚
â”‚  4. DERIVE models from actual outputs                       â”‚
â”‚  5. VALIDATE models against captured outputs                â”‚
â”‚  6. Only then write tests that assert specific structures   â”‚
â”‚                                                             â”‚
â”‚  DO NOT:                                                    â”‚
â”‚  âŒ Define models from design docs alone                    â”‚
â”‚  âŒ Assume field names match documentation                  â”‚
â”‚  âŒ Write tests before discovery phase completes            â”‚
â”‚  âŒ Change existing code to match your models               â”‚
â”‚                                                             â”‚
â”‚  Discovery artifacts should include:                        â”‚
â”‚  - sample_outputs/ directory with real outputs              â”‚
â”‚  - all_keys.txt or similar listing all observed fields      â”‚
â”‚  - type_analysis.json showing inferred types                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Discovery Phase Template

```bash
# 1. Generate sample outputs from real code
[RUN_COMMAND] > sample_outputs/sample_001.[EXT]
[RUN_COMMAND] > sample_outputs/sample_002.[EXT]
# ... repeat for Nâ‰¥10 representative inputs

# 2. Extract all unique keys/fields (method depends on output format)
# For JSON:
cat sample_outputs/*.json | jq -r 'paths | join(".")' | sort -u > all_keys.txt
# For XML:
grep -ohE '<[^/][^>]+>' sample_outputs/*.xml | sort -u > all_keys.txt
# For structured text:
[CUSTOM_EXTRACTION_COMMAND] > all_keys.txt

# 3. Verify discovery completeness
wc -l all_keys.txt
# Expected: â‰¥[MINIMUM_KEYS] unique paths/fields

# 4. Only proceed to model definition after discovery
test $(wc -l < all_keys.txt) -ge [MINIMUM_KEYS] && echo "âœ… Discovery complete" || echo "âŒ Insufficient discovery"
```

---

## ğŸ—ï¸ CODE QUALITY PRINCIPLES

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MANDATORY DESIGN PRINCIPLES                                â”‚
â”‚                                                             â”‚
â”‚  SOLID (for maintainable design):                           â”‚
â”‚  â”œâ”€â”€ Single Responsibility: One module, one purpose         â”‚
â”‚  â”œâ”€â”€ Open/Closed: Open for extension, closed for changes    â”‚
â”‚  â”œâ”€â”€ Liskov Substitution: Subtypes must be substitutable    â”‚
â”‚  â”œâ”€â”€ Interface Segregation: Small, focused interfaces       â”‚
â”‚  â””â”€â”€ Dependency Inversion: Depend on abstractions           â”‚
â”‚                                                             â”‚
â”‚  KISS (simplicity mandate):                                 â”‚
â”‚  â””â”€â”€ Choose simplest solution that solves the problem       â”‚
â”‚      Simpler = easier to understand, maintain, debug        â”‚
â”‚                                                             â”‚
â”‚  YAGNI (feature discipline):                                â”‚
â”‚  â””â”€â”€ Implement features only when currently needed          â”‚
â”‚      No speculative code, no "might need later"             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš¦ YAGNI DECISION TREE

Before implementing ANY new feature, function, or abstraction:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Q1: Is there a CURRENT requirement for this?               â”‚
â”‚      (Not "might need" or "could be useful")                â”‚
â”‚                                                             â”‚
â”‚      NO  â†’ STOP. Do not implement. (YAGNI triggered)        â”‚
â”‚      YES â†’ Continue to Q2                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Q2: Will it be used IMMEDIATELY after it's built?          â”‚
â”‚      (By known code path, not hypothetical consumer)        â”‚
â”‚                                                             â”‚
â”‚      NO  â†’ STOP. Do not implement. (YAGNI triggered)        â”‚
â”‚      YES â†’ Continue to Q3                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Q3: Is it backed by stakeholder request or concrete data?  â”‚
â”‚      (Not speculation or "good practice")                   â”‚
â”‚                                                             â”‚
â”‚      NO  â†’ STOP. Do not implement. (YAGNI triggered)        â”‚
â”‚      YES â†’ Continue to Q4                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Q4: Can it be added LATER without massive rework?          â”‚
â”‚                                                             â”‚
â”‚      YES â†’ WAIT until actually needed. (YAGNI triggered)    â”‚
â”‚      NO  â†’ Implement now (confirmed unavoidable)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

OUTCOME: implement_now = Q1_yes AND Q2_yes AND Q3_yes AND Q4_no
```

### YAGNI Violations to Avoid

| âŒ Violation | Why It's Wrong | âœ… Instead |
|-------------|----------------|-----------|
| Unused parameters | API surface with no consumer | Remove until needed |
| Speculative flags | `enable_feature_x=False` for "someday" | Add when feature exists |
| Premature abstraction | Interface with 1 implementation | Concrete until 2+ consumers |
| Unused hooks | `pre_hook`, `post_hook` with no callers | Plain function |
| Future-proofing | "We might need this later" | Build what's needed now |

---

## âœ… CODE QUALITY CHECKLIST

Add to task completion verification:

```
BEFORE marking any task complete, verify:

â–¡ YAGNI: All new code has immediate consumer (no speculative features)
â–¡ KISS: Simpler alternative was considered
â–¡ SRP: Each new module/class has single clear purpose
â–¡ No unused parameters, hooks, or abstractions added
â–¡ Generalizations have â‰¥2 real consumers today
â–¡ Tests prove current necessity (fail-before, pass-after)
```

---

## ğŸ”„ REFLECTION LOOP (MANDATORY)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BEFORE COMPLETING ANY TASK OR PHASE:                       â”‚
â”‚                                                             â”‚
â”‚  1. PAUSE - Do not proceed to next task                     â”‚
â”‚                                                             â”‚
â”‚  2. REFLECT - Answer these questions:                       â”‚
â”‚     â–¡ Did I achieve the stated objective?                   â”‚
â”‚     â–¡ Can I provide evidence for each claim I made?         â”‚
â”‚     â–¡ Did I verify assumptions or just assume?              â”‚
â”‚     â–¡ Are there risks I identified but didn't mitigate?     â”‚
â”‚     â–¡ Is there anything I'm uncertain about?                â”‚
â”‚                                                             â”‚
â”‚  3. VALIDATE - Check against success criteria:              â”‚
â”‚     â–¡ All tests pass (verified by running them)             â”‚
â”‚     â–¡ Clean Table checklist complete                        â”‚
â”‚     â–¡ No items deferred or skipped                          â”‚
â”‚     â–¡ Every claim has evidence anchor (source + quote)      â”‚
â”‚                                                             â”‚
â”‚  4. DECIDE:                                                 â”‚
â”‚     - All checks pass â†’ Proceed to next task                â”‚
â”‚     - Any uncertainty â†’ STOP and clarify                    â”‚
â”‚     - Any failure â†’ Fix before proceeding                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Semantic Type Distinctions

When reflecting, distinguish between:

| Type | Definition | Requirement |
|------|------------|-------------|
| **FACT** | Verified true statement | Must have evidence anchor |
| **CLAIM** | Assertion being made | Must have supporting evidence |
| **ASSUMPTION** | Unverified belief | Must state + attempt to verify |
| **RISK** | Potential problem | Must have mitigation or acceptance |
| **HYPOTHESIS** | Testable proposition | Must have test + result |

```
âŒ BAD: "The function returns a list" (unstated assumption)
âœ… GOOD: "ASSUMPTION: The function returns a list
         VERIFICATION: grep -A5 'def process' src/module.py
         RESULT: Returns List[str] - CONFIRMED"
```

### Evidence Anchor Requirements

Every significant claim must have:

```
CLAIM: [What you're asserting]
SOURCE: [file:line or command output]
QUOTE: "[Exact text that supports claim]"
```

Example:
```
CLAIM: Parser handles nested structures
SOURCE: tests/test_parser.py:45
QUOTE: "def test_nested_dict(): assert parse({'a': {'b': 1}}) == expected"
```

### Reflection Template

After each task, explicitly state:

```
REFLECTION: Task [X.Y]
â”œâ”€â”€ Objective achieved: [YES/NO + evidence anchor]
â”œâ”€â”€ Claims made:
â”‚   â”œâ”€â”€ CLAIM: [statement] â†’ SOURCE: [file:line]
â”‚   â””â”€â”€ CLAIM: [statement] â†’ SOURCE: [file:line]
â”œâ”€â”€ Assumptions:
â”‚   â”œâ”€â”€ ASSUMPTION: [statement] â†’ VERIFIED: [YES/NO + how]
â”‚   â””â”€â”€ ASSUMPTION: [statement] â†’ VERIFIED: [YES/NO + how]
â”œâ”€â”€ Risks:
â”‚   â”œâ”€â”€ RISK: [description] â†’ MITIGATION: [action taken]
â”‚   â””â”€â”€ RISK: [description] â†’ MITIGATION: [action taken]
â”œâ”€â”€ Uncertainties: [list any that remain]
â”œâ”€â”€ Confidence: [0-100%] (if <80% â†’ STOP)
â””â”€â”€ Decision: [PROCEED / STOP + reason]
```

### Trace Links (Decision â†’ Evidence â†’ Impact)

For significant decisions, create trace links:

```
TRACE LINK:
â”œâ”€â”€ DECISION: [What action was taken]
â”œâ”€â”€ EVIDENCE:
â”‚   â”œâ”€â”€ CLAIM: [Supporting assertion]
â”‚   â”‚   â””â”€â”€ SOURCE: [file:line], QUOTE: "[text]"
â”‚   â””â”€â”€ CLAIM: [Another assertion]
â”‚       â””â”€â”€ SOURCE: [file:line], QUOTE: "[text]"
â”œâ”€â”€ IMPACT: [Consequence of this decision]
â””â”€â”€ RELATED:
    â”œâ”€â”€ Assumptions: [list]
    â”œâ”€â”€ Risks: [list]
    â””â”€â”€ Mitigations: [list]
```

---

## ğŸ›‘ STOP ON AMBIGUITY

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WHEN TO STOP AND CLARIFY:                                  â”‚
â”‚                                                             â”‚
â”‚  STOP if ANY of these are true:                             â”‚
â”‚                                                             â”‚
â”‚  â–¡ Confidence in approach < 80%                             â”‚
â”‚  â–¡ Missing information required to proceed                  â”‚
â”‚  â–¡ Conflicting requirements detected                        â”‚
â”‚  â–¡ Assumption cannot be verified                            â”‚
â”‚  â–¡ Success criteria unclear or unmeasurable                 â”‚
â”‚  â–¡ Multiple valid interpretations exist                     â”‚
â”‚                                                             â”‚
â”‚  DO NOT:                                                    â”‚
â”‚  âŒ Guess and proceed                                       â”‚
â”‚  âŒ Make assumptions without stating them                   â”‚
â”‚  âŒ Pick arbitrary interpretation                           â”‚
â”‚  âŒ Defer clarification to later                            â”‚
â”‚                                                             â”‚
â”‚  INSTEAD:                                                   â”‚
â”‚  âœ… State what is blocking                                  â”‚
â”‚  âœ… State what information is needed                        â”‚
â”‚  âœ… State what will be done once clarified                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stop Block Template

When stopping for clarification, use this format:

```
â›” STOP: CLARIFICATION REQUIRED

BLOCKING: [What element is missing or ambiguous]
REASON: [Why we cannot proceed without this]
NEEDED: [Minimal information required to continue]
NEXT STEP: [What will be executed once provided]

Awaiting clarification before proceeding.
```

---

## ğŸ§  THINKING vs EXECUTION BOUNDARIES

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SEPARATE REASONING FROM ACTION                             â”‚
â”‚                                                             â”‚
â”‚  --- THINKING ---                                           â”‚
â”‚  â€¢ Assumptions being made                                   â”‚
â”‚  â€¢ Risks being considered                                   â”‚
â”‚  â€¢ Trade-offs being evaluated                               â”‚
â”‚  â€¢ Alternative approaches considered                        â”‚
â”‚  â€¢ Why chosen approach is best                              â”‚
â”‚                                                             â”‚
â”‚  --- EXECUTION ---                                          â”‚
â”‚  â€¢ Exact command/code to run                                â”‚
â”‚  â€¢ Expected output/result                                   â”‚
â”‚  â€¢ How to validate success                                  â”‚
â”‚                                                             â”‚
â”‚  This separation ensures:                                   â”‚
â”‚  â€¢ Reasoning is explicit and reviewable                     â”‚
â”‚  â€¢ Actions are concrete and verifiable                      â”‚
â”‚  â€¢ Mistakes in logic vs execution are distinguishable       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Decision Chain Template

For significant decisions, document:

```
DECISION: [What action to take]
â”œâ”€â”€ WHY: [Evidence-based reason]
â”‚   â””â”€â”€ EVIDENCE: [source:line] "[quote]"
â”œâ”€â”€ WHY NOT alternatives:
â”‚   â”œâ”€â”€ [Alternative 1]: Rejected because [reason]
â”‚   â””â”€â”€ [Alternative 2]: Rejected because [reason]
â”œâ”€â”€ IMPACT: [Expected consequence]
â”œâ”€â”€ ASSUMPTIONS: [What we're assuming is true]
â”‚   â””â”€â”€ VERIFICATION: [How verified, or "UNVERIFIED - RISK"]
â”œâ”€â”€ RISKS: [What could go wrong]
â”‚   â””â”€â”€ MITIGATION: [Action taken, or "ACCEPTED because [reason]"]
â””â”€â”€ VALIDATION: [How we'll verify this was correct]
```

Example:
```
DECISION: Use polling instead of webhooks for data sync
â”œâ”€â”€ WHY: Target API doesn't support webhooks
â”‚   â””â”€â”€ EVIDENCE: docs/api.md:45 "Events API: Coming Q3 2025"
â”œâ”€â”€ WHY NOT alternatives:
â”‚   â”œâ”€â”€ Webhooks: Rejected because not yet available
â”‚   â””â”€â”€ WebSockets: Rejected because API doesn't support
â”œâ”€â”€ IMPACT: 30-second delay in data freshness
â”œâ”€â”€ ASSUMPTIONS: API rate limits allow 2 req/min
â”‚   â””â”€â”€ VERIFICATION: docs/api.md:12 "Rate limit: 100 req/min" - VERIFIED
â”œâ”€â”€ RISKS: Rate limiting during high-traffic periods
â”‚   â””â”€â”€ MITIGATION: Exponential backoff implemented in sync.py:89
â””â”€â”€ VALIDATION: Integration test covers rate limit scenario
```

---

## ğŸ“‹ TASK COMPLETION VALIDATION PIPELINE

Before marking any task complete, execute this pipeline in order:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VALIDATION PIPELINE (Execute in order, stop on failure)    â”‚
â”‚                                                             â”‚
â”‚  1. BUILD decision chains                                   â”‚
â”‚     â””â”€â”€ Document what/why/why_not for significant choices   â”‚
â”‚                                                             â”‚
â”‚  2. ATTACH evidence anchors                                 â”‚
â”‚     â””â”€â”€ Every claim has: source + quote                     â”‚
â”‚     â””â”€â”€ Missing evidence â†’ STOP                             â”‚
â”‚                                                             â”‚
â”‚  3. CREATE trace links                                      â”‚
â”‚     â””â”€â”€ Decision â†’ Evidence â†’ Impact connected              â”‚
â”‚                                                             â”‚
â”‚  4. ASSESS confidence                                       â”‚
â”‚     â””â”€â”€ If < 80% certain on any element â†’ STOP              â”‚
â”‚                                                             â”‚
â”‚  5. SEPARATE thinking from execution                        â”‚
â”‚     â””â”€â”€ Reasoning documented, actions concrete              â”‚
â”‚                                                             â”‚
â”‚  6. REFLECT                                                 â”‚
â”‚     â””â”€â”€ Evidence + decisions satisfy success criteria?      â”‚
â”‚     â””â”€â”€ If not â†’ STOP                                       â”‚
â”‚                                                             â”‚
â”‚  7. CROSS-REFERENCE                                         â”‚
â”‚     â””â”€â”€ No conflicts between claims and results             â”‚
â”‚     â””â”€â”€ All assumptions verified or flagged                 â”‚
â”‚     â””â”€â”€ All risks mitigated or accepted with reason         â”‚
â”‚                                                             â”‚
â”‚  If ANY step fails â†’ Do not complete task. Fix or STOP.     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Quick Validation Checklist

```
â–¡ Decision chains documented (what/why/why_not/impact)
â–¡ Evidence anchors present (claim â†’ source:line â†’ quote)
â–¡ Trace links created for significant decisions
â–¡ Confidence â‰¥ 80% on all elements
â–¡ Thinking separated from execution
â–¡ Reflection complete with semantic types distinguished
â–¡ Cross-reference: no conflicts, assumptions verified, risks handled
â–¡ Clean Table checklist passes
â–¡ Tests pass (actually run, not assumed)
```

---

## ğŸ“ File Manifest Rules

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FILE MANIFEST ENFORCEMENT:                                 â”‚
â”‚                                                             â”‚
â”‚  Every task has a "Files:" header listing affected files.   â”‚
â”‚                                                             â”‚
â”‚  RULE: If a file is listed, it MUST:                        â”‚
â”‚  1. Be created/modified by a step in that task              â”‚
â”‚  2. Exist after task completion                             â”‚
â”‚  3. Be verifiable via: test -f <filename>                   â”‚
â”‚                                                             â”‚
â”‚  If a file is listed but not created â†’ TASK INCOMPLETE      â”‚
â”‚  If a file is created but not listed â†’ Add it to Files:     â”‚
â”‚                                                             â”‚
â”‚  GHOST FILES ARE FORBIDDEN                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### File Manifest Verification Command

```bash
# Run after each task to verify all listed files exist
# Replace FILE_LIST with actual files from task header
for f in FILE_LIST; do
    test -f "$f" && echo "âœ… $f" || echo "âŒ MISSING: $f"
done
```

---

## ğŸ”¢ TASK ID UNIQUENESS

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TASK IDs MUST BE UNIQUE AND 1:1 WITH TASKS                 â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN:                                                 â”‚
â”‚  âŒ Reusing same ID for different tasks                     â”‚
â”‚     Task 2.2a: Structure Models                             â”‚
â”‚     Task 2.2a: Update Empty Method  â† CONFLICT!             â”‚
â”‚                                                             â”‚
â”‚  âŒ Slices/sub-tasks without tracked IDs                    â”‚
â”‚     Task 2.2 mentions "slices 2.2a-e" but Appendix          â”‚
â”‚     only tracks 2.2a                                        â”‚
â”‚                                                             â”‚
â”‚  REQUIRED:                                                  â”‚
â”‚  âœ… Every task has unique ID (X.Y or X.Y.Z)                 â”‚
â”‚  âœ… Every slice mentioned in narrative has ID               â”‚
â”‚  âœ… Appendix/tracking table covers ALL IDs                  â”‚
â”‚  âœ… ID â†’ Task is bijective (one-to-one mapping)             â”‚
â”‚                                                             â”‚
â”‚  WHY: Ambiguous IDs break file tracking, progress logs,     â”‚
â”‚  and allow "which 2.2a?" confusion during execution.        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Task ID Verification

Before finalizing task list:
```bash
# Extract all task IDs mentioned in document
grep -oE "Task [0-9]+\.[0-9]+[a-z]?" PROJECT_TASKS.md | sort | uniq -d
# Expected: NO OUTPUT (no duplicates)

# Verify Appendix covers all IDs
grep -oE "[0-9]+\.[0-9]+[a-z]?" PROJECT_TASKS.md | sort -u > /tmp/task_ids.txt
grep -oE "^[|] *[0-9]+\.[0-9]+[a-z]?" PROJECT_TASKS.md | sort -u > /tmp/tracked_ids.txt
comm -23 /tmp/task_ids.txt /tmp/tracked_ids.txt
# Expected: NO OUTPUT (all tasks tracked in Appendix)
```

---

## ğŸ” SCOPE DECISIONS TABLE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EXPLICITLY DECLARE WHAT'S IN AND OUT OF SCOPE              â”‚
â”‚                                                             â”‚
â”‚  At the top of your task list, include a scope table:       â”‚
â”‚                                                             â”‚
â”‚  | Item                    | Status      | Rationale       â”‚
â”‚  |-------------------------|-------------|-----------------|
â”‚  | Core feature X          | IN SCOPE    | Primary goal    â”‚
â”‚  | Edge case Y             | IN SCOPE    | Critical path   â”‚
â”‚  | Nice-to-have Z          | OUT OF SCOPE| Defer to v2     â”‚
â”‚  | Legacy cleanup W        | OUT OF SCOPE| Separate effort â”‚
â”‚                                                             â”‚
â”‚  WHY: Prevents scope creep and makes exclusions explicit.   â”‚
â”‚  Anyone reading knows what WON'T be done, not just what     â”‚
â”‚  will be done.                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸŒ GLOBAL CLEAN TABLE ENFORCEMENT

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLEAN TABLE CHECKS MUST BE GLOBAL, NOT SELECTIVE           â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN:                                                 â”‚
â”‚  âŒ Ad-hoc greps on only some files:                        â”‚
â”‚     grep "TODO" file1.py file2.py  # Misses file3.py!       â”‚
â”‚                                                             â”‚
â”‚  âŒ Phase gates that only check changed files:              â”‚
â”‚     # Doesn't catch pre-existing debt                       â”‚
â”‚                                                             â”‚
â”‚  REQUIRED:                                                  â”‚
â”‚  âœ… Single global check across entire repo:                 â”‚
â”‚     grep -rn "TODO\|FIXME\|XXX" src/ tests/ tools/          â”‚
â”‚                                                             â”‚
â”‚  âœ… Phase artifacts checked for placeholders:               â”‚
â”‚     grep -E "YYYY|MM-DD|placeholder|\[.*\]" .phase-*.json   â”‚
â”‚                                                             â”‚
â”‚  âœ… Ideally: automated test that fails on any violation     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Heredoc/Template Placeholder Detection

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HEREDOCS AND TEMPLATES OFTEN CONTAIN LITERAL PLACEHOLDERS  â”‚
â”‚                                                             â”‚
â”‚  PROBLEM: Task list shows:                                  â”‚
â”‚     cat > .phase-0.complete.json << 'EOF'                   â”‚
â”‚     {                                                       â”‚
â”‚       "completion_date": "YYYY-MM-DDTHH:MM:SSZ"  â† LITERAL! â”‚
â”‚     }                                                       â”‚
â”‚     EOF                                                     â”‚
â”‚                                                             â”‚
â”‚  If executed literally, the artifact contains a placeholder â”‚
â”‚  which violates Clean Table but no check catches it.        â”‚
â”‚                                                             â”‚
â”‚  REQUIRED:                                                  â”‚
â”‚  âœ… Either: Use shell variable substitution:                â”‚
â”‚     "completion_date": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"     â”‚
â”‚                                                             â”‚
â”‚  âœ… Or: Explicit instruction to replace before commit:      â”‚
â”‚     "Replace YYYY-MM-DDTHH:MM:SSZ with actual timestamp"    â”‚
â”‚                                                             â”‚
â”‚  âœ… And: Global check for common placeholder patterns:      â”‚
â”‚     grep -rE "YYYY|XXXX|\[INSERT|\[TODO\]|placeholder"      â”‚
â”‚                                                             â”‚
â”‚  COMMON PLACEHOLDER PATTERNS TO DETECT:                     â”‚
â”‚  â€¢ YYYY-MM-DD, YYYY/MM/DD (date placeholders)               â”‚
â”‚  â€¢ [INSERT X], [TODO], [PLACEHOLDER]                        â”‚
â”‚  â€¢ YOUR_*, MY_*, EXAMPLE_* (template variables)             â”‚
â”‚  â€¢ <description>, <your-value> (XML-style placeholders)     â”‚
â”‚  â€¢ __REPLACE__, %%VARIABLE%% (template markers)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Global Clean Table Test Template

```
# tests/test_clean_table_global.[ext]
# This test runs on every CI build and fails if debt found

test_no_todos_in_codebase():
    result = run(["grep", "-rn", "TODO|FIXME|XXX", "src/", "tests/", "tools/"])
    assert result.exitCode == 1, "Found TODOs in codebase"  # grep returns 1 if no match

test_no_placeholders_in_artifacts():
    result = run(["grep", "-rE", "YYYY-MM-DD|placeholder|\\[INSERT", "."])
    assert result.exitCode == 1, "Found placeholders in repo"

test_no_template_literals_in_generated_files():
    # Check phase completion artifacts
    for artifact in glob(".phase-*.json"):
        content = read(artifact)
        assert "YYYY" not in content, f"Placeholder in {artifact}"
        assert "XXXX" not in content, f"Placeholder in {artifact}"
        assert "[INSERT" not in content, f"Placeholder in {artifact}"
```

---

## ğŸ¯ PHASE GATE SCOPE ALIGNMENT

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE GATES MUST ENFORCE THE SAME SCOPE AS GLOBAL RULES    â”‚
â”‚                                                             â”‚
â”‚  PROBLEM:                                                   â”‚
â”‚  Global rule: "Clean Table applies to entire repo"          â”‚
â”‚  Phase 0 gate: grep TODO file1.py file2.py  # Only 2 files! â”‚
â”‚                                                             â”‚
â”‚  RESULT:                                                    â”‚
â”‚  - TODOs can exist in src/ during Phase 0-2                 â”‚
â”‚  - Only Phase 3+ catches them (when global test added)      â”‚
â”‚  - Early phases have weaker enforcement than later phases   â”‚
â”‚                                                             â”‚
â”‚  REQUIRED:                                                  â”‚
â”‚  âœ… If rule is global, ALL phase gates enforce it globally  â”‚
â”‚  âœ… Or: Global test exists from Phase 0 (not introduced     â”‚
â”‚     later)                                                  â”‚
â”‚  âœ… Phase gates should reference the global test, not       â”‚
â”‚     duplicate logic with narrower scope                     â”‚
â”‚                                                             â”‚
â”‚  PATTERN:                                                   â”‚
â”‚  Phase 0 Gate:                                              â”‚
â”‚    - Run: [FAST_TEST_COMMAND] tests/test_clean_table.py     â”‚
â”‚    - NOT: grep TODO only-these-two-files.py                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â±ï¸ TEMPORAL DEBT WINDOW

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AVOID WORKFLOWS THAT CREATE TEMPORARY DEBT IN TRACKED FILESâ”‚
â”‚                                                             â”‚
â”‚  PROBLEM WORKFLOW:                                          â”‚
â”‚  1. Create .phase-0.complete.json with placeholder date     â”‚
â”‚  2. Run tests â†’ pass (placeholder not yet committed)        â”‚
â”‚  3. Edit file to replace placeholder with real date         â”‚
â”‚  4. Forget to re-run tests                                  â”‚
â”‚  5. Commit via GUI â†’ placeholder could slip through         â”‚
â”‚                                                             â”‚
â”‚  The "debt window" is step 1â†’3 where invalid content exists â”‚
â”‚  in a tracked file but hasn't been caught yet.              â”‚
â”‚                                                             â”‚
â”‚  SOLUTIONS:                                                 â”‚
â”‚                                                             â”‚
â”‚  Option A: Never create placeholders                        â”‚
â”‚     Use shell substitution: $(date -u +%Y-%m-%dT%H:%M:%SZ)  â”‚
â”‚     File is correct from creation                           â”‚
â”‚                                                             â”‚
â”‚  Option B: Pre-commit hook                                  â”‚
â”‚     Hook runs placeholder check before every commit         â”‚
â”‚     Cannot accidentally commit debt                         â”‚
â”‚                                                             â”‚
â”‚  Option C: CI as safety net (current approach)              â”‚
â”‚     Global test catches placeholders                        â”‚
â”‚     But requires discipline to run tests after edits        â”‚
â”‚                                                             â”‚
â”‚  DOCUMENT which option you're using and its trade-offs      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ–¥ï¸ Environment Portability Rules

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FORBIDDEN PATTERNS (examples from various languages):      â”‚
â”‚                                                             â”‚
â”‚  âŒ Hard-coded paths:                                       â”‚
â”‚     .venv/bin/python, /usr/bin/node, C:\Ruby\bin\ruby      â”‚
â”‚  âŒ System-specific paths:                                  â”‚
â”‚     /usr/local/bin/*, /opt/homebrew/bin/*                  â”‚
â”‚  âŒ Version-pinned executables in paths:                    â”‚
â”‚     python3.11, node18, ruby3.2                            â”‚
â”‚                                                             â”‚
â”‚  REQUIRED PATTERNS:                                         â”‚
â”‚                                                             â”‚
â”‚  âœ… Use runtime's self-reference where available:           â”‚
â”‚     Python: sys.executable                                  â”‚
â”‚     Node: process.execPath                                  â”‚
â”‚     Ruby: RbConfig.ruby                                     â”‚
â”‚  âœ… Use package manager invocation:                         â”‚
â”‚     python -m module, npx command, bundle exec              â”‚
â”‚  âœ… Use shell lookup:                                       â”‚
â”‚     $(which python), $(which node), $(which ruby)          â”‚
â”‚                                                             â”‚
â”‚  For tool dependencies (rg, jq, etc.):                      â”‚
â”‚  - Document in Prerequisites section                        â”‚
â”‚  - Add existence check in verification commands             â”‚
â”‚  - Provide installation instructions                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ ASPIRATIONAL VS ACTUAL ALIGNMENT

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IF YOU DEFINE A PATTERN, USE IT EVERYWHERE                 â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN:                                                 â”‚
â”‚  âŒ Define helper but don't use it in examples:             â”‚
â”‚     "Use PYTHON env var for portability"                    â”‚
â”‚     ...later in same doc...                                 â”‚
â”‚     subprocess.run([".venv/bin/python", ...])  # Ignores!   â”‚
â”‚                                                             â”‚
â”‚  âŒ Document skip pattern but test doesn't use it:          â”‚
â”‚     "Skip gracefully if rg not installed"                   â”‚
â”‚     ...test code...                                         â”‚
â”‚     result = run(["rg", ...])  # Hard fails if missing!     â”‚
â”‚                                                             â”‚
â”‚  REQUIRED:                                                  â”‚
â”‚  âœ… Every pattern/helper defined MUST appear in all         â”‚
â”‚     relevant examples in the same document                  â”‚
â”‚  âœ… If you show "the right way", never show "the wrong way" â”‚
â”‚     in executable code blocks                               â”‚
â”‚  âœ… Audit: search for pattern violations in your own doc    â”‚
â”‚                                                             â”‚
â”‚  WHY: Readers copy-paste examples. If examples contradict   â”‚
â”‚  your guidelines, guidelines are effectively dead.          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Aspirational vs Actual Audit

Before finalizing task list:
```bash
# If you defined PYTHON helper, verify all subprocess calls use it
grep -n "PYTHON\s*=" PROJECT_TASKS.md  # Find definition
grep -n "\.venv/bin/python\|/usr/bin/python" PROJECT_TASKS.md
# Expected: No hard-coded paths after helper is defined

# If you defined skip pattern for tool X, verify tests use it
grep -n "pytest.skip.*X not" PROJECT_TASKS.md  # Find pattern
grep -n "subprocess.*\[\"X\"" PROJECT_TASKS.md  # Find usages
# Expected: All usages either have skip guard or are in "wrong" examples
```

---

## âš ï¸ PRE-RUN DEPENDENCIES IN TESTS

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TESTS THAT REQUIRE MANUAL PRE-RUN ARE FRAGILE              â”‚
â”‚                                                             â”‚
â”‚  PATTERN:                                                   â”‚
â”‚  test_output_file_valid():                                  â”‚
â”‚      if not file("output.json").exists():                   â”‚
â”‚          pytest.skip("Run tool first")  # FRAGILE!          â”‚
â”‚      ...                                                    â”‚
â”‚                                                             â”‚
â”‚  PROBLEMS:                                                  â”‚
â”‚  - On clean checkout, tests skip silently                   â”‚
â”‚  - CI might pass with 0 tests run                           â”‚
â”‚  - Pre-run step can be forgotten                            â”‚
â”‚                                                             â”‚
â”‚  SOLUTIONS:                                                 â”‚
â”‚                                                             â”‚
â”‚  Option A: Gate ensures pre-run before tests                â”‚
â”‚     Phase gate runs tool THEN tests (documented in order)   â”‚
â”‚                                                             â”‚
â”‚  Option B: Test runs the tool itself (integration test)     â”‚
â”‚     test runs tool â†’ checks output â†’ cleans up              â”‚
â”‚                                                             â”‚
â”‚  Option C: Fixture generates required files                 â”‚
â”‚     @pytest.fixture creates file, test uses it              â”‚
â”‚                                                             â”‚
â”‚  REQUIRED: Document which option you're using and why       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ VERSION CHANGELOG GOVERNANCE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHEMA/API CHANGES MUST UPDATE CHANGELOG                   â”‚
â”‚                                                             â”‚
â”‚  If your project has versioned schemas, models, or APIs:    â”‚
â”‚                                                             â”‚
â”‚  REQUIRED:                                                  â”‚
â”‚  âœ… CHANGELOG file exists (e.g., SCHEMA_CHANGELOG.md)       â”‚
â”‚  âœ… Every schema change adds changelog entry                â”‚
â”‚  âœ… Version numbers follow semantic versioning              â”‚
â”‚  âœ… CI can enforce: "if schema changed, changelog changed"  â”‚
â”‚                                                             â”‚
â”‚  CHANGELOG ENTRY FORMAT:                                    â”‚
â”‚  ## [version] - YYYY-MM-DD                                  â”‚
â”‚  ### Added / Changed / Deprecated / Removed / Fixed         â”‚
â”‚  - Description of change                                    â”‚
â”‚  - Migration notes if breaking                              â”‚
â”‚                                                             â”‚
â”‚  ENFORCEMENT (optional CI check):                           â”‚
â”‚  if git diff --name-only | grep "schema\|models"; then      â”‚
â”‚    git diff --name-only | grep "CHANGELOG" || exit 1        â”‚
â”‚  fi                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ PACKAGE MANAGER CONSISTENCY

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IF YOU DECLARE A PACKAGE MANAGER, USE IT EVERYWHERE        â”‚
â”‚                                                             â”‚
â”‚  PROBLEM:                                                   â”‚
â”‚  Project says: "We use uv for package management"           â”‚
â”‚  But examples show: .venv/bin/python tools/script.py        â”‚
â”‚                                                             â”‚
â”‚  RESULT: Partial integration                                â”‚
â”‚  - Tool used for: setup (uv add, uv sync)                   â”‚
â”‚  - Tool NOT used for: running tests, tools, commands        â”‚
â”‚  - Developer workflow ignores the declared tool             â”‚
â”‚                                                             â”‚
â”‚  REQUIRED:                                                  â”‚
â”‚  âœ… If package manager declared, ALL commands use it:       â”‚
â”‚     uv run pytest                                           â”‚
â”‚     uv run python tools/script.py                           â”‚
â”‚     NOT: .venv/bin/python tools/script.py                   â”‚
â”‚                                                             â”‚
â”‚  âœ… "Never call X directly" rule documented:                â”‚
â”‚     "Never call .venv/bin/python directly.                  â”‚
â”‚      Always use uv run python ..."                          â”‚
â”‚                                                             â”‚
â”‚  âœ… Examples in docs match the declared tool                â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN:                                                 â”‚
â”‚  âŒ Tool for setup, raw paths for execution                 â”‚
â”‚  âŒ CI uses tool, local dev uses raw paths                  â”‚
â”‚  âŒ "We use X" but examples show direct venv access         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Package Manager Examples (Language-Agnostic)

| Language | Declared Tool | Wrong | Right |
|----------|---------------|-------|-------|
| Python (uv) | uv | `.venv/bin/python script.py` | `uv run python script.py` |
| Python (poetry) | poetry | `.venv/bin/pytest` | `poetry run pytest` |
| Node (npm) | npm | `node script.js` | `npm run script` |
| Node (pnpm) | pnpm | `./node_modules/.bin/cmd` | `pnpm exec cmd` |
| Ruby | bundler | `ruby script.rb` | `bundle exec ruby script.rb` |
| Rust | cargo | `./target/debug/tool` | `cargo run --bin tool` |

---

## ğŸ”„ CI/LOCAL WORKFLOW IDENTITY

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LOCAL COMMANDS SHOULD MATCH CI COMMANDS EXACTLY            â”‚
â”‚                                                             â”‚
â”‚  PROBLEM:                                                   â”‚
â”‚  CI workflow:     uv run pytest tests/                      â”‚
â”‚  Local docs:      .venv/bin/python -m pytest tests/         â”‚
â”‚                                                             â”‚
â”‚  RESULT:                                                    â”‚
â”‚  - "Works on my machine" but fails in CI (or vice versa)    â”‚
â”‚  - Debugging CI requires translating commands               â”‚
â”‚  - Two mental models for same action                        â”‚
â”‚                                                             â”‚
â”‚  REQUIRED:                                                  â”‚
â”‚  âœ… Same command works locally AND in CI                    â”‚
â”‚  âœ… Documentation shows the CI-identical command            â”‚
â”‚  âœ… Phase gates use CI-identical commands                   â”‚
â”‚  âœ… Test snippets use CI-identical invocation               â”‚
â”‚                                                             â”‚
â”‚  PATTERN:                                                   â”‚
â”‚  CI:    uv run pytest tests/                                â”‚
â”‚  Local: uv run pytest tests/   # IDENTICAL                  â”‚
â”‚  Docs:  "Run tests: uv run pytest tests/"                   â”‚
â”‚  Gate:  uv run pytest tests/ && echo "PASS"                 â”‚
â”‚                                                             â”‚
â”‚  BENEFITS:                                                  â”‚
â”‚  - Copy-paste from CI to local (and vice versa)             â”‚
â”‚  - No translation needed when debugging                     â”‚
â”‚  - Single source of truth for "how to run X"                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CI/Local Alignment Checklist

Before finalizing task list:
```bash
# Extract all commands from CI workflow
grep -E "run:|uv |pytest|python" .github/workflows/*.yml

# Extract all commands from task list/docs  
grep -E "\.venv/|python |pytest" PROJECT_TASKS.md

# These should use the same invocation pattern
# If CI uses "uv run pytest", docs should NOT use ".venv/bin/pytest"
```

---

## ğŸ”€ SKIP PATTERN CONSISTENCY

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IF YOU DEFINE A SKIP PATTERN, USE IT EVERYWHERE            â”‚
â”‚                                                             â”‚
â”‚  PROBLEM:                                                   â”‚
â”‚  Guidelines say:                                            â”‚
â”‚     "Skip gracefully if rg not installed"                   â”‚
â”‚     if not shutil.which("rg"):                              â”‚
â”‚         pytest.skip("ripgrep not installed")                â”‚
â”‚                                                             â”‚
â”‚  But actual test does:                                      â”‚
â”‚     result = subprocess.run(["rg", ...])                    â”‚
â”‚     assert result.returncode in (0, 1)  # Hard fails if 127!â”‚
â”‚                                                             â”‚
â”‚  RESULT: Half-committed. Either:                            â”‚
â”‚  - Tool is REQUIRED (hard fail, no skip pattern needed)     â”‚
â”‚  - Tool is OPTIONAL (skip pattern used consistently)        â”‚
â”‚                                                             â”‚
â”‚  REQUIRED: Make explicit decision and apply consistently:   â”‚
â”‚                                                             â”‚
â”‚  Option A: REQUIRED dependency                              â”‚
â”‚  - Document in Prerequisites                                â”‚
â”‚  - CI installs it                                           â”‚
â”‚  - Tests hard-fail if missing (no skip)                     â”‚
â”‚  - Remove any skip suggestions from guidelines              â”‚
â”‚                                                             â”‚
â”‚  Option B: OPTIONAL dependency                              â”‚
â”‚  - Document in Prerequisites with install instructions      â”‚
â”‚  - ALL tests that use it have skip guard                    â”‚
â”‚  - Guidelines and tests are consistent                      â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN:                                                 â”‚
â”‚  âŒ Skip pattern in guidelines, hard-fail in tests          â”‚
â”‚  âŒ Some tests skip, other tests hard-fail for same tool    â”‚
â”‚  âŒ "Should skip if missing" without actual skip code       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Required vs Optional Dependency Documentation

When documenting dependencies, be explicit:

```
## Prerequisites

### REQUIRED (CI and local must have)
- Python 3.10+
- uv package manager

### REQUIRED FOR CI, OPTIONAL LOCAL (tests skip if missing)
- ripgrep (rg): Tests skip locally, but CI must have it
  - Local: `brew install ripgrep` or tests will skip
  - CI: Pre-installed on GitHub runners

### FULLY OPTIONAL (tests skip if missing)
- graphviz: Only needed for diagram generation
```

**Anti-pattern**: Saying "REQUIRED" but tests skip when missing. This creates:
- False sense of enforcement locally
- Different behavior between CI and dev machines
- Confusion about actual contract

---

## ğŸ“„ CROSS-DOCUMENT SSOT

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WHEN SPEC AND TASK LIST COEXIST, PREVENT DRIFT             â”‚
â”‚                                                             â”‚
â”‚  COMMON SITUATION:                                          â”‚
â”‚  - SPEC.md: Design document with rich detail                â”‚
â”‚  - TASK_LIST.md: Executable plan with subset of spec        â”‚
â”‚                                                             â”‚
â”‚  RISK:                                                      â”‚
â”‚  - Spec describes tests X, Y, Z                             â”‚
â”‚  - Task list only implements X, Y (Z out of scope)          â”‚
â”‚  - Future reader of spec expects Z to exist                 â”‚
â”‚  - Soft double-SSOT: two documents, different "truth"       â”‚
â”‚                                                             â”‚
â”‚  REQUIRED:                                                  â”‚
â”‚                                                             â”‚
â”‚  âœ… Mark spec as HISTORICAL once task list exists:          â”‚
â”‚     "For active work, see TASK_LIST.md. This document       â”‚
â”‚      is design history only."                               â”‚
â”‚                                                             â”‚
â”‚  âœ… Or: Task list references spec with explicit scope:      â”‚
â”‚     "Implements sections 1-3 of SPEC.md. Sections 4-5       â”‚
â”‚      are OUT OF SCOPE for this refactor."                   â”‚
â”‚                                                             â”‚
â”‚  âœ… If spec is still active, sync changes both ways:        â”‚
â”‚     Task list change â†’ update spec (or vice versa)          â”‚
â”‚                                                             â”‚
â”‚  FORBIDDEN:                                                 â”‚
â”‚  âŒ Two documents with overlapping scope, no cross-ref      â”‚
â”‚  âŒ Spec promises tests that task list doesn't implement    â”‚
â”‚  âŒ "The spec says X but we're doing Y" without updating    â”‚
â”‚     either document                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Cross-Document Reference Pattern

At top of task list:
```markdown
## Document Relationship

| Document | Status | Role |
|----------|--------|------|
| SPEC.md | HISTORICAL | Design rationale, not executable |
| TASK_LIST.md | ACTIVE | Executable plan, SSOT for this work |
| output_models.py | ACTIVE | SSOT for schema once created |

For discrepancies: TASK_LIST.md wins over SPEC.md.
After completion: output_models.py wins over both.
```

At top of spec (once task list exists):
```markdown
## âš ï¸ HISTORICAL DOCUMENT

This document is design history. For active implementation:
- See: TASK_LIST.md (executable plan)
- See: output_models.py (schema SSOT once created)

Do NOT use this document as source of truth for current work.
```

---

## Prerequisites (Verify Before Starting)

**ALL must pass before Phase 0 begins:**

- [ ] **Environment ready**: [verification command]
- [ ] **Dependencies installed**: [verification command]
- [ ] **Tests run successfully**: [verification command]
- [ ] **No blocking issues**: [verification command]
- [ ] **Required tools available**: [list tools with `which` checks]

**Quick Check**:
```bash
# Single command to verify all prerequisites
[PREREQ_CHECK_COMMAND]

# Tool availability (customize per project/language)
# Python:  which python && which pytest || echo "Missing"
# Node:    which node && which npm || echo "Missing"
# Rust:    which cargo || echo "Missing"
# Go:      which go || echo "Missing"
```

---

# Phase 0: Setup & Infrastructure

**Goal**: Establish testing infrastructure and baseline
**Time Estimate**: [X-Y hours]
**Status**: â³ NOT STARTED

---

## Task 0.1: [Task Name]

**Objective**: [One sentence]
**Files**: `[file1]`, `[file2]`

### TDD Step 1: Write Test First

```
# Language-specific test file (examples below)
# Adapt syntax to your project's language/framework

# Python (pytest):
# tests/test_[name].py
def test_[feature]():
    """[What this test verifies]"""
    # Arrange / Act / Assert
    # Arrange
    [setup]
    
    # Act
    result = [function_call]
    
    # Assert - MUST be specific, not "it runs"
    assert result == [expected_specific_value]
    assert len(result.items) >= [minimum_expected]
    # Add assertions that would FAIL with stub implementation
```

```bash
# Verify test fails (RED)
[TEST_COMMAND]
# Expected: FAIL (test exists but implementation missing)
```

### â›” Test Strength Self-Check

Before proceeding to implementation:
- [ ] Would this test pass with empty/null return? â†’ If yes, strengthen it
- [ ] Does test verify specific output content? â†’ Must be yes
- [ ] Does test have minimum count/value assertions? â†’ Must be yes

### TDD Step 2: Implement

```
# [file]
# Implementation in your language

function [name]():
    """[Docstring/documentation]"""
    [implementation]
```

### TDD Step 3: Verify (GREEN)

```bash
# Run the specific test
[TEST_COMMAND]
# Expected: PASS

# Run full suite (no regressions)
[FULL_TEST_COMMAND]
# Expected: N/N PASS
```

### â›” STOP: Clean Table Check

Before marking this task complete, verify:

- [ ] Test passes (not skipped)
- [ ] **Test is strong** (fails with stub implementation)
- [ ] Full test suite passes: `[FULL_TEST_COMMAND]` â†’ N/N PASS
- [ ] No TODOs or placeholders in new code
- [ ] No new warnings introduced
- [ ] Code is documented
- [ ] **All files in "Files:" header exist**: `test -f [each_file]`

**Status**: â³ NOT STARTED

---

## Task 0.2: [Next Task]

[Same structure as Task 0.1]

---

## â›” STOP: Phase 0 Gate

**Before starting Phase 1, ALL must be true:**

```bash
# 1. All tests pass
[FULL_TEST_COMMAND]
# Expected: N/N PASS

# 2. No regressions
[REGRESSION_CHECK]
# Expected: 0 failures

# 3. Clean Table verified
grep -r "TODO\|FIXME\|XXX" src/
# Expected: No results (or only pre-existing)

# 4. File manifest verified
for f in [ALL_PHASE_0_FILES]; do test -f "$f" || echo "MISSING: $f"; done
# Expected: No output (all files exist)
```

### Phase 0 Completion Checklist

- [ ] All Task 0.x items have âœ… status
- [ ] Full test suite passes
- [ ] **All tests are strong** (not "it runs" tests)
- [ ] No new TODOs introduced
- [ ] Infrastructure documented
- [ ] **All listed files exist**
- [ ] Git checkpoint created

### Create Phase Unlock Artifact

```bash
# Only run this after ALL above criteria pass
# NOTE: Replace placeholders with ACTUAL values before running

PHASE_NUM=0
PHASE_NAME="Setup & Infrastructure"
# Get test count (adapt command to your test framework)
# Python/pytest: pytest --collect-only -q 2>/dev/null | tail -1 | grep -oP '\d+(?= test)'
# Node/jest:     npm test -- --listTests 2>/dev/null | wc -l
# Go:            go test -list '.*' ./... 2>/dev/null | wc -l
TESTS_PASSED=[YOUR_TEST_COUNT_COMMAND]
GIT_COMMIT=$(git rev-parse --short HEAD)
COMPLETION_DATE=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

cat > .phase-${PHASE_NUM}.complete.json << EOF
{
  "schema_version": "1.0",
  "phase": ${PHASE_NUM},
  "phase_name": "${PHASE_NAME}",
  "tests_passed": ${TESTS_PASSED:-0},
  "tests_total": ${TESTS_PASSED:-0},
  "clean_table": true,
  "git_commit": "${GIT_COMMIT}",
  "completion_date": "${COMPLETION_DATE}"
}
EOF

# Verify no placeholders remain
grep -E "YYYY|placeholder|\[.*\]" .phase-${PHASE_NUM}.complete.json && \
    echo "âŒ ERROR: Placeholders found in artifact" && exit 1

git add .phase-${PHASE_NUM}.complete.json
git commit -m "Phase ${PHASE_NUM} complete: ${PHASE_NAME}"
git tag phase-${PHASE_NUM}-complete
```

**Phase 0 Status**: â³ NOT STARTED

---

# Phase 1: [Phase Name]

**Goal**: [One sentence]
**Time Estimate**: [X-Y hours]
**Prerequisite**: `.phase-0.complete.json` must exist
**Status**: ğŸ“‹ PLANNED

---

## â›” STOP: Verify Phase 0 Complete

```bash
# This MUST pass before starting Phase 1
test -f .phase-0.complete.json && echo "âœ… Phase 0 complete" || echo "âŒ BLOCKED"
```

---

## Task 1.1: [Task Name]

**Objective**: [One sentence]
**Files**: `[files]`

### TDD Step 1: Write Test First

[Test code with verification command]

**â›” Test Strength Self-Check** (mandatory):
- [ ] Test fails with stub implementation
- [ ] Test verifies specific output values
- [ ] Test has boundary/count assertions

### TDD Step 2: Implement

[Implementation guidance]

### TDD Step 3: Verify

```bash
[TEST_COMMAND]
# Expected: PASS
```

### â›” STOP: Clean Table Check

- [ ] Test passes
- [ ] **Test is strong**
- [ ] Full suite passes
- [ ] No new debt
- [ ] **All "Files:" exist**

**Status**: ğŸ“‹ PLANNED

---

## Task 1.2: [Next Task]

[Same structure]

---

## â›” STOP: Phase 1 Gate

[Same structure as Phase 0 Gate, including file manifest verification]

---

# Appendix A: Rollback Procedures

## A.1: Single Test Failure

```bash
# 1. Identify failing test
[TEST_COMMAND] 2>&1 | head -50

# 2. If fixable in <15 min, fix it
# 3. If not fixable, revert last change
git diff HEAD~1 --stat
git checkout HEAD~1 -- [affected_file]

# 4. Verify tests pass again
[FULL_TEST_COMMAND]
```

## A.2: Multiple Test Failures (Full Revert)

```bash
# 1. Identify last known good state
git log --oneline -10

# 2. Hard reset to good commit
git reset --hard [GOOD_COMMIT]

# 3. Verify
[FULL_TEST_COMMAND]

# 4. Document what went wrong
echo "[DATE]: Reverted due to [REASON]" >> ISSUES.md
```

## A.3: Phase Gate Failure

```bash
# DO NOT proceed to next phase
# 1. Identify which criterion failed
# 2. Fix the specific issue
# 3. Re-run phase gate verification
# 4. Only proceed when ALL criteria pass
```

## A.4: Weak Test Discovered

```bash
# If a test is discovered to be an "it runs" test:
# 1. DO NOT proceed with current task
# 2. Strengthen the test first
# 3. Verify strengthened test fails with stub
# 4. Only then continue implementation
```

---

# Appendix B: AI Assistant Instructions

## Drift Prevention Rules

1. **Before each response**, re-read the current task's objective
2. **After completing code**, immediately run verification commands
3. **If test fails**, fix it before moving on (no "we'll fix it later")
4. **If blocked**, document why and suggest rollback
5. **Never skip** Clean Table checks
6. **Never write** "it runs" tests (see Test Strength Rules)
7. **Never list** files that won't be created
8. **Never test** features that don't exist yet
9. **Never assume** code matches design docsâ€”verify first
10. **Never tighten** constraints until loose version passes

## Authority Resolution

When design doc says X but code does Y:
1. **Verify** code behavior is intentional (not a bug)
2. **If intentional** â†’ Update design doc and task list to match code
3. **If bug** â†’ Fix code, but through proper TDD cycle
4. **Never** rename/restructure working code just to match docs

## Verification Frequency

| Action | Verify Immediately |
|--------|-------------------|
| Create new file | File exists, syntax valid |
| Modify function | Related tests pass |
| Write test | **Test fails with stub** |
| Reference symbol from spec | **Symbol exists in code** |
| Complete task | Full test suite + file manifest |
| Complete phase | Phase gate checklist |
| Tighten constraints | **Loose version passes first** |

## When to Stop and Escalate

- Test suite drops below 100% pass rate
- Performance degrades beyond threshold
- Clean Table criteria cannot be satisfied
- Phase gate blocked for >2 attempts
- **Test discovered to be "it runs" pattern**
- **File listed but not created**
- **Spec and code disagree on symbol names**
- **Tightening constraints breaks previously passing tests**
- **Feature referenced in spec doesn't exist in code**

## Prohibited Actions

- âŒ Starting Phase N+1 without Phase N artifact
- âŒ Marking task complete with failing tests
- âŒ Leaving TODOs in "completed" code
- âŒ Skipping Clean Table verification
- âŒ Proceeding after rollback without re-verification
- âŒ **Writing tests that only check exit code**
- âŒ **Writing tests that pass with stub implementation**
- âŒ **Listing files in "Files:" that aren't created**
- âŒ **Leaving literal placeholders in artifacts**
- âŒ **Using hard-coded interpreter paths** (use runtime self-reference)
- âŒ **Testing features marked "reserved" or "planned"**
- âŒ **Assuming symbol names without grep verification**
- âŒ **Changing working code to match design docs**
- âŒ **Tightening constraints before loose version passes**
- âŒ **Defining models without discovery phase**
- âŒ **Treating "final state" diagrams as implementation specs**

**YAGNI Violations (Code Quality)**:
- âŒ **Adding features without current requirement** (YAGNI Q1)
- âŒ **Building for hypothetical future consumers** (YAGNI Q2)
- âŒ **Speculative flags, hooks, or parameters** (unused API surface)
- âŒ **Premature abstractions** (interface with only 1 implementation)
- âŒ **"Might need later" code** (if addable later, defer)
- âŒ **Generalizations without â‰¥2 real consumers today**
- âŒ **Complex solution when simpler one works** (KISS violation)

**Silent Error Violations**:
- âŒ **Swallowed exceptions** (catch and continue without raise)
- âŒ **Conditional error handling** (if strict: raise)
- âŒ **Strict mode parameters** (strict: bool = True)
- âŒ **Environment variables for strictness** (STRICT_MODE env var)
- âŒ **Warning instead of error** (print warning and continue)
- âŒ **Error collection without raise** (errors.append() then continue)

**Hidden Debt Violations**:
- âŒ **"Temporary" fixes** that mask root causes
- âŒ **"Adjust later" comments** without immediate resolution
- âŒ **"You may need to" speculative comments**
- âŒ **Workaround logic** instead of proper fix
- âŒ **Deferred follow-ups** in completed code

**Reflection Violations**:
- âŒ **Skipping reflection** before task completion
- âŒ **Proceeding with uncertainty** (confidence < 80%)
- âŒ **Unstated assumptions** (assuming without documenting)
- âŒ **Guessing when ambiguous** (should STOP and clarify)
- âŒ **Mixing thinking and execution** (must separate reasoning from action)
- âŒ **Claims without evidence anchors** (no source:line + quote)
- âŒ **Assumptions treated as facts** (must distinguish semantic types)
- âŒ **Undocumented decisions** (must have decision chain)
- âŒ **Missing trace links** (decision â†’ evidence â†’ impact)
- âŒ **Unverified assumptions proceeding** (must verify or flag as risk)

**Governance Violations**:
- âŒ **Duplicate task IDs** (same ID for different tasks)
- âŒ **Untracked tasks in Appendix** (all IDs must appear in file tracking)
- âŒ **Selective Clean Table checks** (must be global, entire repo)
- âŒ **Aspirational helpers not used** (defined pattern ignored in examples)
- âŒ **Existence-only tests** (must check content correctness, not just presence)
- âŒ **Unhandled pre-run dependencies** (tests that skip without explicit strategy)
- âŒ **Missing scope decisions** (must declare what's in/out of scope)
- âŒ **Exit code tolerance** (`returncode in (0, 1)` accepts both success and failure)
- âŒ **Plausible output tests** (`"exported" in stdout` doesn't verify behavior)
- âŒ **Heredoc placeholders** (literal `YYYY-MM-DD` committed to artifacts)
- âŒ **Inconsistent skip patterns** (guidelines say skip, tests hard-fail)
- âŒ **CI string-only tests** (checking YAML contains string, not correct wiring)
- âŒ **Schema changes without changelog** (versioned artifacts need history)
- âŒ **No-op stub tools** (tool passes tests but never processes real data)
- âŒ **Phase gate scope mismatch** (global rules enforced locally in early phases)
- âŒ **Temporal debt window** (create placeholder â†’ edit â†’ forget to re-test)
- âŒ **Undocumented weak tests** (intentionally limited tests without TRADE-OFF comment)
- âŒ **Cross-document drift** (spec and task list overlap without cross-reference)
- âŒ **"Required" but skips** (dependency called REQUIRED but tests skip when missing)
- âŒ **Package manager partial use** ("We use X" but examples show direct venv access)
- âŒ **CI/Local command mismatch** (CI uses tool runner, local uses raw paths)

## Discovery-First Checklist

Before defining models/schemas/types for existing code:

- [ ] Ran existing code with â‰¥10 representative inputs
- [ ] Captured actual outputs to sample_outputs/
- [ ] Extracted all unique keys/fields
- [ ] Derived model structure from actual outputs
- [ ] Validated model against captured outputs
- [ ] Only then: wrote tests asserting specific structures

## Strictness Transition Checklist

Before tightening any constraint (validation, types, checks):

- [ ] Loose/permissive version implemented
- [ ] Loose version passes on â‰¥N real-world inputs
- [ ] Tightened constraint applied
- [ ] Same inputs still pass (if not â†’ fix model, not inputs)
- [ ] No fake data synthesized to satisfy strict checks

## YAGNI Checklist (Before Adding ANY New Code)

For every new function, class, parameter, or abstraction:

- [ ] **Q1**: Current requirement exists (ticket/issue ID)?
- [ ] **Q2**: Will be used immediately by known consumer?
- [ ] **Q3**: Backed by stakeholder request or data (not speculation)?
- [ ] **Q4**: Cannot be added later without massive rework?
- [ ] **KISS**: Simpler alternative considered and rejected?
- [ ] **SRP**: Single clear purpose for this code?
- [ ] No unused parameters, hooks, or flags added?
- [ ] Generalizations have â‰¥2 real consumers today?

**If any Q1-Q3 is NO â†’ Do not implement**
**If Q4 is YES â†’ Defer until actually needed**

---

# Appendix C: File Change Tracking

Track all file changes for audit trail:

| Phase.Task | File | Action | Verified |
|------------|------|--------|----------|
| 0.1 | `src/module.[ext]` | CREATE | â³ |
| 0.1 | `tests/test_module.[ext]` | CREATE | â³ |
| 1.1 | `src/module.[ext]` | UPDATE | ğŸ“‹ |

**File Manifest Audit**:
```bash
# After each phase, verify all tracked files exist
cat << 'EOF' | while read line; do
    file=$(echo "$line" | awk '{print $3}')
    test -f "$file" && echo "âœ… $file" || echo "âŒ $file"
done
[PASTE_TABLE_HERE]
EOF
```

---

# Appendix D: Progress Log

## Session Log

```
[YYYY-MM-DD HH:MM] Started Phase 0
[YYYY-MM-DD HH:MM] Task 0.1 complete - tests: 5/5 PASS (strong tests verified)
[YYYY-MM-DD HH:MM] Task 0.2 complete - tests: 8/8 PASS
[YYYY-MM-DD HH:MM] Phase 0 Gate: âœ… ALL PASS
[YYYY-MM-DD HH:MM] File manifest: âœ… ALL FILES EXIST
[YYYY-MM-DD HH:MM] Created .phase-0.complete.json (no placeholders)
```

## Time Tracking

| Phase | Task | Estimated | Actual | Notes |
|-------|------|-----------|--------|-------|
| 0 | 0.1 | 1h | - | - |
| 0 | 0.2 | 2h | - | - |

---

# Appendix E: Test Strength Audit

Run this audit after writing tests to catch weak patterns:

```bash
# Find potential "it runs" tests (adapt patterns to your language)
# Look for exit code checks without content verification
grep -rn "exitCode == 0\|returncode == 0\|exit_code == 0" tests/
grep -rn "exitCode in\|returncode in" tests/

# Find assertions that only check presence, not content
grep -rn "assert.*exists\|expect.*exist\|toBeDefined" tests/

# Find empty or near-empty test functions
# Python:
grep -l "def test_" tests/ | xargs grep -A10 "def test_" | grep -B5 "^$"
# JavaScript:
grep -l "it(" tests/ | xargs grep -A10 "it(" | grep -B5 "^$"

# Manual review checklist:
# - Does each test have specific value assertions?
# - Would a stub implementation pass this test?
# - Are both success and failure paths tested?
```

If any weak patterns found, strengthen before proceeding.

---

# Template Customization Checklist

Before using this template, replace:

- [ ] `[PROJECT_NAME]` â†’ Your project name
- [ ] `[FAST_TEST_COMMAND]` â†’ Quick test command for your language/framework
  - Python: `pytest tests/ -x -q`
  - Node: `npm test -- --bail`
  - Rust: `cargo test --quiet`
  - Go: `go test ./... -short`
- [ ] `[FULL_TEST_COMMAND]` â†’ Complete test suite command
  - Python: `pytest tests/ -v`
  - Node: `npm test`
  - Rust: `cargo test`
  - Go: `go test ./... -v`
- [ ] `[PERF_TEST_COMMAND]` â†’ Performance/benchmark command (if applicable)
- [ ] `[TEST_COMMAND]` â†’ Single test file/case command
- [ ] Phase names and task breakdowns
- [ ] Success criteria for your project
- [ ] **All "Files:" headers with actual files**
- [ ] **Tool dependencies in Prerequisites**
- [ ] **Language-specific syntax in examples** (adapt to your stack)

**Final verification**: `grep -E "\[.*\]" PROJECT_TASKS.md` should return only intentional placeholders.

---

**End of Template**
