#!/usr/bin/env python3
"""
Comprehensive test suite for MarkdownParserCore using JSON ground truth files.
Tests 290+ real-world markdown documents with expected feature counts.
"""

import json
import time
from collections import defaultdict
from pathlib import Path

from src.docpipe.loaders.markdown_parser_core import MarkdownParserCore


class MarkdownTestFramework:
    """Framework for testing markdown parser against JSON ground truth."""

    def __init__(self, test_dir: Path):
        self.test_dir = test_dir
        self.results = {
            "total": 0,
            "passed": 0,
            "failed": 0,
            "skipped": 0,
            "errors": defaultdict(list),
            "mismatches": defaultdict(list),
            "performance": [],
            "category_results": defaultdict(lambda: {"passed": 0, "failed": 0}),
        }

    def discover_test_pairs(self) -> list[tuple[Path, Path]]:
        """Discover all .md/.json test file pairs."""
        pairs = []
        md_files = sorted(
            [f for f in self.test_dir.glob("*.md") if not f.name.endswith("__INDEX.md")]
        )

        for md_file in md_files:
            json_file = md_file.with_suffix(".json")
            if json_file.exists():
                pairs.append((md_file, json_file))

        return pairs

    def load_ground_truth(self, json_file: Path) -> dict:
        """Load expected results from JSON file."""
        try:
            with open(json_file, encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            print(f"  âš ï¸  Failed to load {json_file.name}: {e}")
            return None

    def extract_feature_counts(self, result: dict) -> dict[str, int]:
        """Extract feature counts from parser result."""
        counts = {}
        structure = result.get("structure", {})

        # Filter out YAML frontmatter headings
        headings = structure.get("headings", [])
        # YAML frontmatter creates bogus headings with multi-line text containing colons
        real_headings = [
            h for h in headings if not (": " in h.get("text", "") and "\n" in h.get("text", ""))
        ]

        # Basic structures
        counts["headings"] = len(real_headings)
        counts["blockquotes"] = len(structure.get("blockquotes", []))
        counts["tables"] = len(structure.get("tables", []))
        counts["links"] = len([l for l in structure.get("links", []) if l.get("type") != "image"])
        counts["images"] = len(structure.get("images", []))

        # Count hrules from tokens - need access to parser
        counts["hrules"] = self._count_hrules_from_tokens(result)

        # Lists
        lists = structure.get("lists", [])
        counts["unordered_list_items"] = sum(
            len(lst.get("items", [])) for lst in lists if lst.get("type") == "bullet"
        )
        counts["ordered_list_items"] = sum(
            len(lst.get("items", [])) for lst in lists if lst.get("type") == "ordered"
        )
        counts["task_items"] = sum(
            len([item for item in lst.get("items", []) if item.get("checked") is not None])
            for lst in lists
            if lst.get("type") == "task"
        )

        # Code blocks
        code_blocks = structure.get("code_blocks", [])
        counts["code_fences"] = len([cb for cb in code_blocks if cb.get("type") == "fenced"])
        counts["indented_code"] = len([cb for cb in code_blocks if cb.get("type") == "indented"])

        # Inline code - count from paragraphs and other text content
        counts["inline_code"] = 0
        for para in structure.get("paragraphs", []):
            if para.get("has_code"):
                # Rough estimate - would need token analysis for exact count
                counts["inline_code"] += 1

        # Reference links
        counts["ref_links"] = 0  # Need token analysis

        # HTML blocks
        html = structure.get("html_blocks", {})
        if isinstance(html, dict):
            counts["html_blocks"] = len(html.get("blocks", []))
        else:
            counts["html_blocks"] = len(html) if html else 0

        # Footnotes
        footnotes = structure.get("footnotes", {})
        if isinstance(footnotes, dict):
            counts["footnotes"] = len(footnotes.get("definitions", []))
        else:
            counts["footnotes"] = 0

        # Autolinks
        counts["autolinks"] = len(
            [
                l
                for l in structure.get("links", [])
                if l.get("url", "").startswith(("http://", "https://"))
                and l.get("text") == l.get("url")
            ]
        )

        return counts

    def _count_hrules_from_tokens(self, result: dict) -> int:
        """Count horizontal rules from parser tokens, excluding frontmatter."""
        # Look for hr tokens that are not part of YAML frontmatter
        hrule_count = 0
        tokens = result.get("_debug_tokens", [])

        if not tokens:
            return 0

        # Skip YAML frontmatter hr tokens (usually at start/end of frontmatter)
        for i, token in enumerate(tokens):
            if hasattr(token, "type") and token.type == "hr":
                # Check if this might be YAML frontmatter delimiter
                if i < 2:  # First two tokens might be frontmatter start
                    continue
                if i >= len(tokens) - 2:  # Last two tokens might be frontmatter end
                    continue

                # Check surrounding context for YAML patterns
                is_frontmatter = False
                if i > 0 and i < len(tokens) - 1:
                    prev_token = tokens[i - 1] if i > 0 else None
                    next_token = tokens[i + 1] if i < len(tokens) - 1 else None

                    # Look for YAML content patterns around hr
                    if prev_token and hasattr(prev_token, "content"):
                        if ": " in str(prev_token.content):
                            is_frontmatter = True
                    if next_token and hasattr(next_token, "content"):
                        if ": " in str(next_token.content):
                            is_frontmatter = True

                if not is_frontmatter:
                    hrule_count += 1

        return hrule_count

    def compare_counts(
        self, expected: dict[str, int], actual: dict[str, int]
    ) -> tuple[bool, list[str]]:
        """Compare expected vs actual feature counts."""
        mismatches = []

        for feature, expected_count in expected.items():
            actual_count = actual.get(feature, 0)
            if expected_count != actual_count:
                mismatches.append(f"{feature}: expected {expected_count}, got {actual_count}")

        # Allow some tolerance for features that are hard to count exactly
        tolerance_features = ["inline_code", "ref_links", "hrules"]
        critical_mismatches = [
            m for m in mismatches if not any(feat in m for feat in tolerance_features)
        ]

        passed = len(critical_mismatches) == 0
        return passed, mismatches

    def test_file_pair(self, md_file: Path, json_file: Path, config: dict = None) -> bool:
        """Test a single markdown/JSON file pair."""
        # Load ground truth
        ground_truth = self.load_ground_truth(json_file)
        if not ground_truth:
            self.results["skipped"] += 1
            return False

        # Get test category from filename
        category = md_file.name.split("__")[0]

        try:
            # Read markdown content
            content = md_file.read_text(encoding="utf-8")

            # Configure parser based on ground truth settings
            parser_config = config or {}
            if "allows_html" in ground_truth:
                parser_config["allows_html"] = ground_truth["allows_html"]

            # Parse with timing
            start_time = time.perf_counter()
            parser = MarkdownParserCore(content, parser_config)
            result = parser.parse()
            parse_time = (time.perf_counter() - start_time) * 1000

            self.results["performance"].append(parse_time)

            # Extract and compare feature counts
            actual_counts = self.extract_feature_counts(result)
            expected_counts = ground_truth.get("feature_counts", {})

            passed, mismatches = self.compare_counts(expected_counts, actual_counts)

            if passed:
                self.results["passed"] += 1
                self.results["category_results"][category]["passed"] += 1
                return True
            self.results["failed"] += 1
            self.results["category_results"][category]["failed"] += 1
            self.results["mismatches"][md_file.name] = mismatches
            return False

        except Exception as e:
            self.results["failed"] += 1
            self.results["category_results"][category]["failed"] += 1
            self.results["errors"][category].append((md_file.name, str(e)))
            return False

    def run_tests(self, limit: int = None) -> bool:
        """Run all tests and return success status."""
        print("ğŸ” Discovering test file pairs...")
        pairs = self.discover_test_pairs()

        if limit:
            pairs = pairs[:limit]

        self.results["total"] = len(pairs)
        print(f"ğŸ“ Found {len(pairs)} test file pairs")
        print("=" * 70)

        # Test configurations
        configs = [
            ("default", {}),
            ("with_html", {"allows_html": True}),
            ("no_plugins", {"external_plugins": []}),
        ]

        print("ğŸ§ª Running tests...")
        print("-" * 70)

        for i, (md_file, json_file) in enumerate(pairs, 1):
            # Progress indicator
            if i % 50 == 0:
                print(f"  Progress: {i}/{len(pairs)} files tested...")

            # Test with default config (uses allows_html from JSON)
            success = self.test_file_pair(md_file, json_file)

            # Show failures immediately for debugging
            if not success and len(self.results["mismatches"]) <= 5:
                mismatches = self.results["mismatches"].get(md_file.name, [])
                if mismatches:
                    print(f"  âŒ {md_file.name}:")
                    for mismatch in mismatches[:3]:
                        print(f"      {mismatch}")

        return self.results["passed"] == self.results["total"]

    def print_results(self):
        """Print detailed test results."""
        print("\n" + "=" * 70)
        print("ğŸ“Š TEST RESULTS")
        print("=" * 70)

        # Overall results
        total = self.results["total"]
        passed = self.results["passed"]
        failed = self.results["failed"]
        skipped = self.results["skipped"]

        if total > 0:
            success_rate = (passed / total) * 100
            print(f"\nâœ… Passed: {passed}/{total} ({success_rate:.1f}%)")
        else:
            print("\nâŒ No tests found!")
            return

        if failed > 0:
            print(f"âŒ Failed: {failed}")
        if skipped > 0:
            print(f"â­ï¸  Skipped: {skipped}")

        # Category breakdown
        print("\nğŸ“‚ Results by Category:")
        for category, stats in sorted(self.results["category_results"].items()):
            cat_total = stats["passed"] + stats["failed"]
            if cat_total > 0:
                cat_rate = (stats["passed"] / cat_total) * 100
                status = "âœ…" if cat_rate == 100 else "âš ï¸" if cat_rate >= 50 else "âŒ"
                print(f"  {status} {category:20s}: {stats['passed']}/{cat_total} ({cat_rate:.0f}%)")

        # Feature mismatch summary
        if self.results["mismatches"]:
            print("\nğŸ” Feature Mismatches (first 10):")
            items = list(self.results["mismatches"].items())[:10]
            for filename, mismatches in items:
                print(f"  {filename}:")
                for mismatch in mismatches[:3]:
                    print(f"    - {mismatch}")

        # Error summary
        if self.results["errors"]:
            print("\nâš ï¸  Errors by Category:")
            for category, errors in self.results["errors"].items():
                print(f"  {category}: {len(errors)} errors")
                for filename, error in errors[:2]:
                    print(f"    - {filename}: {error[:100]}")

        # Performance statistics
        if self.results["performance"]:
            perf = self.results["performance"]
            avg_time = sum(perf) / len(perf)
            max_time = max(perf)
            min_time = min(perf)

            print("\nâš¡ Performance Statistics:")
            print(f"  Files tested:       {len(perf)}")
            print(f"  Average parse time: {avg_time:.2f}ms")
            print(f"  Fastest parse:      {min_time:.2f}ms")
            print(f"  Slowest parse:      {max_time:.2f}ms")

        # Final summary
        print("\n" + "=" * 70)
        if passed == total:
            print("ğŸ‰ ALL TESTS PASSED! Parser handles all test cases correctly.")
        elif passed >= total * 0.9:
            print(f"âœ… MOSTLY PASSED: {success_rate:.1f}% success rate")
        elif passed >= total * 0.7:
            print(f"âš ï¸  PARTIAL SUCCESS: {success_rate:.1f}% success rate - needs improvement")
        else:
            print(f"âŒ FAILED: Only {success_rate:.1f}% success rate - significant issues")


def run_quick_validation():
    """Run a quick validation on a subset of files."""
    print("\nğŸš€ Running quick validation...")
    print("-" * 70)

    test_dir = Path("src/docpipe/loaders/test_mds/md_stress_mega")
    if not test_dir.exists():
        print(f"âŒ Test directory not found: {test_dir}")
        return False

    # Test just a few files from each category
    categories = ["base_everything", "bad_indentation", "html_chaos"]

    for category in categories:
        pattern = f"{category}__case_01"
        md_file = test_dir / f"{pattern}.md"
        json_file = test_dir / f"{pattern}.json"

        if md_file.exists() and json_file.exists():
            try:
                with open(json_file) as f:
                    ground_truth = json.load(f)

                content = md_file.read_text(encoding="utf-8")
                parser = MarkdownParserCore(content)
                result = parser.parse()

                print(f"  âœ… {category}: Parsed successfully")

            except Exception as e:
                print(f"  âŒ {category}: {e}")
                return False
        else:
            print(f"  âš ï¸  {category}: Test files not found")

    print("\nâœ… Quick validation passed!")
    return True


def main():
    """Main test execution."""
    import argparse
    import sys

    parser = argparse.ArgumentParser(description="Test MarkdownParserCore against ground truth")
    parser.add_argument("--quick", action="store_true", help="Run quick validation only")
    parser.add_argument("--limit", type=int, help="Limit number of test files")
    parser.add_argument("--category", help="Test only specific category")
    parser.add_argument("--verbose", action="store_true", help="Show detailed output")

    args = parser.parse_args()

    print("ğŸ”¬ MarkdownParserCore Test Framework")
    print("=" * 70)

    # Quick validation
    if args.quick:
        success = run_quick_validation()
        sys.exit(0 if success else 1)

    # Setup test directory
    test_dir = Path("src/docpipe/loaders/test_mds/md_stress_mega")
    if not test_dir.exists():
        print(f"âŒ Test directory not found: {test_dir}")
        print("   Expected location: src/docpipe/loaders/test_mds/md_stress_mega/")
        sys.exit(1)

    # Run quick validation first
    if not run_quick_validation():
        print("\nâŒ Quick validation failed. Fix basic issues before running full test.")
        sys.exit(1)

    # Run comprehensive tests
    print("\n" + "=" * 70)
    print("ğŸ”„ Starting comprehensive test suite...")

    framework = MarkdownTestFramework(test_dir)

    # Apply filters if specified
    if args.category:
        print(f"ğŸ“ Testing category: {args.category}")
        # Filter test pairs by category
        # Implementation would go here

    success = framework.run_tests(limit=args.limit)
    framework.print_results()

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
